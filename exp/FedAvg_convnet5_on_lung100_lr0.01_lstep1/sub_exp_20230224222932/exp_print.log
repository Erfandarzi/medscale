2023-02-24 22:29:32,643 (logging:124) INFO: the current machine is at 192.168.0.1
2023-02-24 22:29:32,643 (logging:126) INFO: the current dir is /home/ubuntu/medscale
2023-02-24 22:29:32,643 (logging:127) INFO: the output dir is exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
2023-02-24 22:29:42,596 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:42,596 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:42,620 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:42,792 (utils:144) INFO: The device information file is not provided
2023-02-24 22:29:43,118 (fed_runner:169) INFO: Server has been set up ... 
2023-02-24 22:29:43,317 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:43,341 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:43,352 (fed_runner:221) INFO: Client 1 has been set up ... 
2023-02-24 22:29:43,461 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:43,487 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:43,496 (fed_runner:221) INFO: Client 2 has been set up ... 
2023-02-24 22:29:43,656 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:43,680 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:43,689 (fed_runner:221) INFO: Client 3 has been set up ... 
2023-02-24 22:29:43,844 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:43,869 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:43,877 (fed_runner:221) INFO: Client 4 has been set up ... 
2023-02-24 22:29:44,018 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:44,042 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:44,051 (fed_runner:221) INFO: Client 5 has been set up ... 
2023-02-24 22:29:44,114 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:44,139 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:44,155 (fed_runner:221) INFO: Client 6 has been set up ... 
2023-02-24 22:29:44,294 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:44,318 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:44,326 (fed_runner:221) INFO: Client 7 has been set up ... 
2023-02-24 22:29:44,392 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:44,423 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:44,434 (fed_runner:221) INFO: Client 8 has been set up ... 
2023-02-24 22:29:44,583 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:44,606 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:44,621 (fed_runner:221) INFO: Client 9 has been set up ... 
2023-02-24 22:29:44,731 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 22:29:44,756 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.7}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224222932
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 22:29:44,768 (fed_runner:221) INFO: Client 10 has been set up ... 
<<<<<<< HEAD
2023-02-24 22:29:44,768 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.
=======
<<<<<<< HEAD
2023-02-24 22:29:44,768 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.
=======
2023-02-24 22:29:44,768 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.
>>>>>>> fe4962455354c9c11afd9c9806ceda28eb280737
>>>>>>> 64b283ee525ef53c32509882719e74890329b83f
2023-02-24 22:29:44,769 (trainer:349) INFO: Num of original para names: 39.
2023-02-24 22:29:44,769 (trainer:350) INFO: Num of original trainable para names: 24.
2023-02-24 22:29:44,769 (trainer:352) INFO: Num of preserved para names in local update: 39. 
Preserved para names in local update: {'conv4.bias', 'bn5.num_batches_tracked', 'conv4.weight', 'conv2.weight', 'bn2.bias', 'conv3.weight', 'conv3.bias', 'bn3.weight', 'bn2.running_var', 'bn2.num_batches_tracked', 'bn4.running_var', 'bn4.running_mean', 'bn2.weight', 'conv5.bias', 'fc2.bias', 'bn1.running_mean', 'bn4.num_batches_tracked', 'conv5.weight', 'fc2.weight', 'fc1.weight', 'bn3.running_mean', 'bn5.running_mean', 'bn4.weight', 'bn4.bias', 'bn5.running_var', 'bn1.bias', 'bn1.num_batches_tracked', 'bn5.weight', 'conv1.weight', 'bn3.running_var', 'bn1.running_var', 'bn5.bias', 'bn3.bias', 'conv1.bias', 'conv2.bias', 'bn3.num_batches_tracked', 'bn2.running_mean', 'fc1.bias', 'bn1.weight'}.
2023-02-24 22:29:44,769 (trainer:356) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2023-02-24 22:29:44,770 (trainer:361) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	}
2023-02-24 22:29:44,785 (server:804) INFO: ----------- Starting training (Round #0) -------------
2023-02-24 22:29:51,294 (client:306) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_total': 200, 'train_correct': 152.0, 'train_acc': 0.76, 'train_loss': 292.62583, 'train_f1': 0.145594, 'train_avg_loss': 1.463129}}
2023-02-24 22:29:51,860 (client:306) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_total': 36, 'train_correct': 0.0, 'train_acc': 0.0, 'train_loss': 153.190335, 'train_f1': 0.0, 'train_avg_loss': 4.255287}}
2023-02-24 22:29:52,922 (client:306) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_total': 112, 'train_correct': 76.0, 'train_acc': 0.678571, 'train_loss': 263.050762, 'train_f1': 0.205405, 'train_avg_loss': 2.348668}}
2023-02-24 22:29:53,040 (client:306) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_total': 7, 'train_correct': 0.0, 'train_acc': 0.0, 'train_loss': 32.92785, 'train_f1': 0.0, 'train_avg_loss': 4.703979}}
2023-02-24 22:29:53,867 (client:306) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_total': 67, 'train_correct': 20.0, 'train_acc': 0.298507, 'train_loss': 226.436638, 'train_f1': 0.08547, 'train_avg_loss': 3.379651}}
2023-02-24 22:29:55,556 (client:306) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_total': 169, 'train_correct': 142.0, 'train_acc': 0.840237, 'train_loss': 239.476806, 'train_f1': 0.153182, 'train_avg_loss': 1.417023}}
2023-02-24 22:29:57,444 (client:306) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_total': 142, 'train_correct': 78.0, 'train_acc': 0.549296, 'train_loss': 293.400484, 'train_f1': 0.117314, 'train_avg_loss': 2.066201}}
2023-02-24 22:29:57,918 (client:306) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_total': 45, 'train_correct': 20.0, 'train_acc': 0.444444, 'train_loss': 171.672997, 'train_f1': 0.090703, 'train_avg_loss': 3.814955}}
2023-02-24 22:29:58,103 (client:306) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_total': 15, 'train_correct': 0.0, 'train_acc': 0.0, 'train_loss': 68.696337, 'train_f1': 0.0, 'train_avg_loss': 4.579756}}
2023-02-24 22:29:59,010 (client:306) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_total': 84, 'train_correct': 21.0, 'train_acc': 0.25, 'train_loss': 256.783366, 'train_f1': 0.086967, 'train_avg_loss': 3.056945}}
2023-02-24 22:29:59,035 (monitor:541) INFO: {'Role': 'Server #', 'Round': 0, 'Results_model_metric': {}}
2023-02-24 22:29:59,037 (server:330) INFO: Server: Starting evaluation at the end of round 0.
2023-02-24 22:29:59,046 (server:336) INFO: ----------- Starting a new training round (Round #1) -------------
2023-02-24 22:29:59,297 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:29:59,406 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:29:59,525 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:29:59,768 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:00,145 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:00,215 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:00,717 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:00,779 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:01,042 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:01,149 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:01,151 (server:590) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 7.5, 'test_acc': 0.340909, 'test_loss': 134.607466, 'test_f1': 0.192785, 'test_avg_loss': 3.770012}, 'Results_avg': {'test_total': 22.0, 'test_correct': 7.5, 'test_acc': 0.33841, 'test_loss': 82.940257, 'test_f1': 0.187831, 'test_avg_loss': 3.788821}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 7.5, 'test_acc_std': 0.244493, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.65, 'test_acc_min': 0.0, 'test_acc_max': 0.65, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.65, 'test_acc_cos1': 0.810581, 'test_acc_entropy': 1.953909, 'test_loss_std': 65.156333, 'test_loss_bottom_decile': 26.362926, 'test_loss_top_decile': 236.599852, 'test_loss_min': 22.727636, 'test_loss_max': 236.599852, 'test_loss_bottom10%': 22.727636, 'test_loss_top10%': 236.599852, 'test_loss_cos1': 0.786369, 'test_loss_entropy': 2.037627, 'test_f1_std': 0.141788, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.391304, 'test_f1_min': 0.0, 'test_f1_max': 0.391304, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.391304, 'test_f1_cos1': 0.79813, 'test_f1_entropy': 1.939322, 'test_avg_loss_std': 0.069574, 'test_avg_loss_bottom_decile': 3.727447, 'test_avg_loss_top_decile': 3.970145, 'test_avg_loss_min': 3.721034, 'test_avg_loss_max': 3.970145, 'test_avg_loss_bottom10%': 3.721034, 'test_avg_loss_top10%': 3.970145, 'test_avg_loss_cos1': 0.999831, 'test_avg_loss_entropy': 2.302418}}
2023-02-24 22:30:01,589 (client:306) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'train_total': 36, 'train_correct': 20.0, 'train_acc': 0.555556, 'train_loss': 58.504915, 'train_f1': 0.419048, 'train_avg_loss': 1.625137}}
2023-02-24 22:30:02,733 (client:306) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'train_total': 112, 'train_correct': 86.0, 'train_acc': 0.767857, 'train_loss': 72.566079, 'train_f1': 0.469388, 'train_avg_loss': 0.647911}}
2023-02-24 22:30:02,993 (client:306) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 48.401259, 'train_f1': 0.088889, 'train_avg_loss': 3.226751}}
2023-02-24 22:30:04,091 (client:306) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'train_total': 84, 'train_correct': 40.0, 'train_acc': 0.47619, 'train_loss': 119.952506, 'train_f1': 0.326965, 'train_avg_loss': 1.428006}}
2023-02-24 22:30:05,721 (client:306) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'train_total': 169, 'train_correct': 158.0, 'train_acc': 0.934911, 'train_loss': 80.482504, 'train_f1': 0.405385, 'train_avg_loss': 0.476228}}
2023-02-24 22:30:05,807 (client:306) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'train_total': 7, 'train_correct': 3.0, 'train_acc': 0.428571, 'train_loss': 13.695051, 'train_f1': 0.32381, 'train_avg_loss': 1.956436}}
2023-02-24 22:30:06,195 (client:306) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'train_total': 45, 'train_correct': 38.0, 'train_acc': 0.844444, 'train_loss': 46.433344, 'train_f1': 0.474359, 'train_avg_loss': 1.031852}}
2023-02-24 22:30:06,788 (client:306) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'train_total': 67, 'train_correct': 45.0, 'train_acc': 0.671642, 'train_loss': 68.992528, 'train_f1': 0.391837, 'train_avg_loss': 1.029739}}
2023-02-24 22:30:08,069 (client:306) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'train_total': 142, 'train_correct': 99.0, 'train_acc': 0.697183, 'train_loss': 106.028618, 'train_f1': 0.610463, 'train_avg_loss': 0.74668}}
2023-02-24 22:30:10,076 (client:306) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'train_total': 200, 'train_correct': 172.0, 'train_acc': 0.86, 'train_loss': 129.807053, 'train_f1': 0.30991, 'train_avg_loss': 0.649035}}
2023-02-24 22:30:10,097 (monitor:541) INFO: {'Role': 'Server #', 'Round': 1, 'Results_model_metric': {}}
2023-02-24 22:30:10,099 (server:330) INFO: Server: Starting evaluation at the end of round 1.
2023-02-24 22:30:10,102 (server:336) INFO: ----------- Starting a new training round (Round #2) -------------
2023-02-24 22:30:10,265 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:10,466 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:10,581 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:10,828 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:11,287 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:11,382 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:11,940 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:12,028 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:12,277 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:12,405 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:12,419 (server:590) INFO: {'Role': 'Server #', 'Round': 1, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 7.7, 'test_acc': 0.35, 'test_loss': 102.492893, 'test_f1': 0.204469, 'test_avg_loss': 2.895078}, 'Results_avg': {'test_total': 22.0, 'test_correct': 7.7, 'test_acc': 0.34769, 'test_loss': 63.691713, 'test_f1': 0.201507, 'test_avg_loss': 2.943067}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 7.7, 'test_acc_std': 0.243961, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.65, 'test_acc_min': 0.0, 'test_acc_max': 0.65, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.65, 'test_acc_cos1': 0.818592, 'test_acc_entropy': 1.964579, 'test_loss_std': 48.983392, 'test_loss_bottom_decile': 20.237422, 'test_loss_top_decile': 180.111565, 'test_loss_min': 17.684246, 'test_loss_max': 180.111565, 'test_loss_bottom10%': 17.684246, 'test_loss_top10%': 180.111565, 'test_loss_cos1': 0.792686, 'test_loss_entropy': 2.047929, 'test_f1_std': 0.14306, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.391304, 'test_f1_min': 0.0, 'test_f1_max': 0.391304, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.391304, 'test_f1_cos1': 0.815403, 'test_f1_entropy': 1.960968, 'test_avg_loss_std': 0.181953, 'test_avg_loss_bottom_decile': 2.784172, 'test_avg_loss_top_decile': 3.423538, 'test_avg_loss_min': 2.756915, 'test_avg_loss_max': 3.423538, 'test_avg_loss_bottom10%': 2.756915, 'test_avg_loss_top10%': 3.423538, 'test_avg_loss_cos1': 0.998094, 'test_avg_loss_entropy': 2.300733}}
2023-02-24 22:30:14,243 (client:306) INFO: {'Role': 'Client #3', 'Round': 2, 'Results_raw': {'train_total': 169, 'train_correct': 152.0, 'train_acc': 0.899408, 'train_loss': 68.661284, 'train_f1': 0.316667, 'train_avg_loss': 0.40628}}
2023-02-24 22:30:14,670 (client:306) INFO: {'Role': 'Client #7', 'Round': 2, 'Results_raw': {'train_total': 45, 'train_correct': 33.0, 'train_acc': 0.733333, 'train_loss': 36.234373, 'train_f1': 0.420635, 'train_avg_loss': 0.805208}}
2023-02-24 22:30:15,900 (client:306) INFO: {'Role': 'Client #9', 'Round': 2, 'Results_raw': {'train_total': 142, 'train_correct': 99.0, 'train_acc': 0.697183, 'train_loss': 91.557356, 'train_f1': 0.603584, 'train_avg_loss': 0.64477}}
2023-02-24 22:30:16,925 (client:306) INFO: {'Role': 'Client #5', 'Round': 2, 'Results_raw': {'train_total': 112, 'train_correct': 85.0, 'train_acc': 0.758929, 'train_loss': 64.80589, 'train_f1': 0.562943, 'train_avg_loss': 0.578624}}
2023-02-24 22:30:17,274 (client:306) INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'train_total': 36, 'train_correct': 22.0, 'train_acc': 0.611111, 'train_loss': 47.427284, 'train_f1': 0.452951, 'train_avg_loss': 1.317425}}
2023-02-24 22:30:17,401 (client:306) INFO: {'Role': 'Client #4', 'Round': 2, 'Results_raw': {'train_total': 7, 'train_correct': 4.0, 'train_acc': 0.571429, 'train_loss': 10.155817, 'train_f1': 0.444444, 'train_avg_loss': 1.450831}}
2023-02-24 22:30:18,099 (client:306) INFO: {'Role': 'Client #10', 'Round': 2, 'Results_raw': {'train_total': 67, 'train_correct': 33.0, 'train_acc': 0.492537, 'train_loss': 61.080895, 'train_f1': 0.301144, 'train_avg_loss': 0.911655}}
2023-02-24 22:30:18,989 (client:306) INFO: {'Role': 'Client #6', 'Round': 2, 'Results_raw': {'train_total': 84, 'train_correct': 40.0, 'train_acc': 0.47619, 'train_loss': 104.383827, 'train_f1': 0.300975, 'train_avg_loss': 1.242665}}
2023-02-24 22:30:19,232 (client:306) INFO: {'Role': 'Client #8', 'Round': 2, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 45.61085, 'train_f1': 0.333333, 'train_avg_loss': 3.040723}}
2023-02-24 22:30:21,617 (client:306) INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'train_total': 200, 'train_correct': 174.0, 'train_acc': 0.87, 'train_loss': 96.616329, 'train_f1': 0.310992, 'train_avg_loss': 0.483082}}
2023-02-24 22:30:21,627 (monitor:541) INFO: {'Role': 'Server #', 'Round': 2, 'Results_model_metric': {}}
2023-02-24 22:30:21,629 (server:330) INFO: Server: Starting evaluation at the end of round 2.
2023-02-24 22:30:21,639 (server:336) INFO: ----------- Starting a new training round (Round #3) -------------
2023-02-24 22:30:21,771 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:21,902 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:22,074 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:22,369 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:22,739 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:22,797 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:23,300 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:23,363 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:23,532 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:23,662 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:23,664 (server:590) INFO: {'Role': 'Server #', 'Round': 2, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 9.9, 'test_acc': 0.45, 'test_loss': 63.079574, 'test_f1': 0.303104, 'test_avg_loss': 1.803408}, 'Results_avg': {'test_total': 22.0, 'test_correct': 9.9, 'test_acc': 0.434469, 'test_loss': 39.674976, 'test_f1': 0.282366, 'test_avg_loss': 1.867123}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 9.9, 'test_acc_std': 0.188295, 'test_acc_bottom_decile': 0.275862, 'test_acc_top_decile': 0.7, 'test_acc_min': 0.083333, 'test_acc_max': 0.7, 'test_acc_bottom10%': 0.083333, 'test_acc_top10%': 0.7, 'test_acc_cos1': 0.917536, 'test_acc_entropy': 2.196049, 'test_loss_std': 29.747928, 'test_loss_bottom_decile': 12.943606, 'test_loss_top_decile': 111.722826, 'test_loss_min': 11.374427, 'test_loss_max': 111.722826, 'test_loss_bottom10%': 11.374427, 'test_loss_top10%': 111.722826, 'test_loss_cos1': 0.80008, 'test_loss_entropy': 2.059663, 'test_f1_std': 0.130749, 'test_f1_bottom_decile': 0.148148, 'test_f1_top_decile': 0.499666, 'test_f1_min': 0.051282, 'test_f1_max': 0.499666, 'test_f1_bottom10%': 0.051282, 'test_f1_top10%': 0.499666, 'test_f1_cos1': 0.907437, 'test_f1_entropy': 2.181495, 'test_avg_loss_std': 0.264973, 'test_avg_loss_bottom_decile': 1.620904, 'test_avg_loss_top_decile': 2.56402, 'test_avg_loss_min': 1.552563, 'test_avg_loss_max': 2.56402, 'test_avg_loss_bottom10%': 1.552563, 'test_avg_loss_top10%': 2.56402, 'test_avg_loss_cos1': 0.99008, 'test_avg_loss_entropy': 2.293101}}
2023-02-24 22:30:24,276 (client:306) INFO: {'Role': 'Client #10', 'Round': 3, 'Results_raw': {'train_total': 67, 'train_correct': 47.0, 'train_acc': 0.701493, 'train_loss': 45.198649, 'train_f1': 0.453268, 'train_avg_loss': 0.674607}}
2023-02-24 22:30:25,959 (client:306) INFO: {'Role': 'Client #3', 'Round': 3, 'Results_raw': {'train_total': 169, 'train_correct': 150.0, 'train_acc': 0.887574, 'train_loss': 59.827062, 'train_f1': 0.314465, 'train_avg_loss': 0.354006}}
2023-02-24 22:30:27,228 (client:306) INFO: {'Role': 'Client #9', 'Round': 3, 'Results_raw': {'train_total': 142, 'train_correct': 90.0, 'train_acc': 0.633803, 'train_loss': 94.752472, 'train_f1': 0.554214, 'train_avg_loss': 0.667271}}
2023-02-24 22:30:27,375 (client:306) INFO: {'Role': 'Client #8', 'Round': 3, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 35.885049, 'train_f1': 0.333333, 'train_avg_loss': 2.392337}}
2023-02-24 22:30:27,917 (client:306) INFO: {'Role': 'Client #7', 'Round': 3, 'Results_raw': {'train_total': 45, 'train_correct': 32.0, 'train_acc': 0.711111, 'train_loss': 37.270505, 'train_f1': 0.405634, 'train_avg_loss': 0.828233}}
2023-02-24 22:30:28,922 (client:306) INFO: {'Role': 'Client #6', 'Round': 3, 'Results_raw': {'train_total': 84, 'train_correct': 40.0, 'train_acc': 0.47619, 'train_loss': 88.289749, 'train_f1': 0.316198, 'train_avg_loss': 1.051068}}
2023-02-24 22:30:31,132 (client:306) INFO: {'Role': 'Client #1', 'Round': 3, 'Results_raw': {'train_total': 200, 'train_correct': 174.0, 'train_acc': 0.87, 'train_loss': 99.417394, 'train_f1': 0.336703, 'train_avg_loss': 0.497087}}
2023-02-24 22:30:31,603 (client:306) INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'train_total': 36, 'train_correct': 20.0, 'train_acc': 0.555556, 'train_loss': 37.517509, 'train_f1': 0.415152, 'train_avg_loss': 1.042153}}
2023-02-24 22:30:31,773 (client:306) INFO: {'Role': 'Client #4', 'Round': 3, 'Results_raw': {'train_total': 7, 'train_correct': 5.0, 'train_acc': 0.714286, 'train_loss': 8.07218, 'train_f1': 0.6, 'train_avg_loss': 1.153169}}
2023-02-24 22:30:33,096 (client:306) INFO: {'Role': 'Client #5', 'Round': 3, 'Results_raw': {'train_total': 112, 'train_correct': 80.0, 'train_acc': 0.714286, 'train_loss': 62.118926, 'train_f1': 0.531136, 'train_avg_loss': 0.554633}}
2023-02-24 22:30:33,109 (monitor:541) INFO: {'Role': 'Server #', 'Round': 3, 'Results_model_metric': {}}
2023-02-24 22:30:33,111 (server:330) INFO: Server: Starting evaluation at the end of round 3.
2023-02-24 22:30:33,114 (server:336) INFO: ----------- Starting a new training round (Round #4) -------------
2023-02-24 22:30:33,266 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:33,394 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:33,634 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:33,938 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:34,477 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:34,548 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:35,152 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:35,213 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:35,393 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:35,565 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:35,567 (server:590) INFO: {'Role': 'Server #', 'Round': 3, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 10.5, 'test_acc': 0.477273, 'test_loss': 40.686594, 'test_f1': 0.323475, 'test_avg_loss': 1.185658}, 'Results_avg': {'test_total': 22.0, 'test_correct': 10.5, 'test_acc': 0.463826, 'test_loss': 26.084466, 'test_f1': 0.303338, 'test_avg_loss': 1.267449}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 10.5, 'test_acc_std': 0.182311, 'test_acc_bottom_decile': 0.333333, 'test_acc_top_decile': 0.75, 'test_acc_min': 0.083333, 'test_acc_max': 0.75, 'test_acc_bottom10%': 0.083333, 'test_acc_top10%': 0.75, 'test_acc_cos1': 0.930687, 'test_acc_entropy': 2.211545, 'test_loss_std': 19.30535, 'test_loss_bottom_decile': 8.398623, 'test_loss_top_decile': 74.634229, 'test_loss_min': 7.789528, 'test_loss_max': 74.634229, 'test_loss_bottom10%': 7.789528, 'test_loss_top10%': 74.634229, 'test_loss_cos1': 0.8038, 'test_loss_entropy': 2.065691, 'test_f1_std': 0.132333, 'test_f1_bottom_decile': 0.166667, 'test_f1_top_decile': 0.499666, 'test_f1_min': 0.051282, 'test_f1_max': 0.499666, 'test_f1_bottom10%': 0.051282, 'test_f1_top10%': 0.499666, 'test_f1_cos1': 0.916576, 'test_f1_entropy': 2.190793, 'test_avg_loss_std': 0.384378, 'test_avg_loss_bottom_decile': 0.899807, 'test_avg_loss_top_decile': 2.307374, 'test_avg_loss_min': 0.890151, 'test_avg_loss_max': 2.307374, 'test_avg_loss_bottom10%': 0.890151, 'test_avg_loss_top10%': 2.307374, 'test_avg_loss_cos1': 0.956961, 'test_avg_loss_entropy': 2.261955}}
2023-02-24 22:30:35,696 (client:306) INFO: {'Role': 'Client #4', 'Round': 4, 'Results_raw': {'train_total': 7, 'train_correct': 5.0, 'train_acc': 0.714286, 'train_loss': 7.608303, 'train_f1': 0.6, 'train_avg_loss': 1.0869}}
2023-02-24 22:30:36,485 (client:306) INFO: {'Role': 'Client #10', 'Round': 4, 'Results_raw': {'train_total': 67, 'train_correct': 47.0, 'train_acc': 0.701493, 'train_loss': 44.793169, 'train_f1': 0.466045, 'train_avg_loss': 0.668555}}
2023-02-24 22:30:36,691 (client:306) INFO: {'Role': 'Client #8', 'Round': 4, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 36.842136, 'train_f1': 0.333333, 'train_avg_loss': 2.456142}}
2023-02-24 22:30:37,141 (client:306) INFO: {'Role': 'Client #2', 'Round': 4, 'Results_raw': {'train_total': 36, 'train_correct': 20.0, 'train_acc': 0.555556, 'train_loss': 38.806326, 'train_f1': 0.40946, 'train_avg_loss': 1.077953}}
2023-02-24 22:30:38,353 (client:306) INFO: {'Role': 'Client #5', 'Round': 4, 'Results_raw': {'train_total': 112, 'train_correct': 90.0, 'train_acc': 0.803571, 'train_loss': 56.922966, 'train_f1': 0.635934, 'train_avg_loss': 0.508241}}
2023-02-24 22:30:40,675 (client:306) INFO: {'Role': 'Client #1', 'Round': 4, 'Results_raw': {'train_total': 200, 'train_correct': 175.0, 'train_acc': 0.875, 'train_loss': 95.789441, 'train_f1': 0.311111, 'train_avg_loss': 0.478947}}
2023-02-24 22:30:41,074 (client:306) INFO: {'Role': 'Client #7', 'Round': 4, 'Results_raw': {'train_total': 45, 'train_correct': 26.0, 'train_acc': 0.577778, 'train_loss': 41.992103, 'train_f1': 0.309091, 'train_avg_loss': 0.933158}}
2023-02-24 22:30:43,006 (client:306) INFO: {'Role': 'Client #3', 'Round': 4, 'Results_raw': {'train_total': 169, 'train_correct': 148.0, 'train_acc': 0.87574, 'train_loss': 65.117325, 'train_f1': 0.348148, 'train_avg_loss': 0.38531}}
2023-02-24 22:30:44,425 (client:306) INFO: {'Role': 'Client #6', 'Round': 4, 'Results_raw': {'train_total': 84, 'train_correct': 38.0, 'train_acc': 0.452381, 'train_loss': 85.923894, 'train_f1': 0.298067, 'train_avg_loss': 1.022903}}
2023-02-24 22:30:46,488 (client:306) INFO: {'Role': 'Client #9', 'Round': 4, 'Results_raw': {'train_total': 142, 'train_correct': 103.0, 'train_acc': 0.725352, 'train_loss': 80.105354, 'train_f1': 0.680899, 'train_avg_loss': 0.564122}}
2023-02-24 22:30:46,506 (monitor:541) INFO: {'Role': 'Server #', 'Round': 4, 'Results_model_metric': {}}
2023-02-24 22:30:46,508 (server:330) INFO: Server: Starting evaluation at the end of round 4.
2023-02-24 22:30:46,523 (server:336) INFO: ----------- Starting a new training round (Round #5) -------------
2023-02-24 22:30:46,760 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:46,954 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:47,067 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:47,317 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:47,762 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:47,821 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:48,485 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:48,547 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:48,719 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:48,878 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:48,880 (server:590) INFO: {'Role': 'Server #', 'Round': 4, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 15.9, 'test_acc': 0.722727, 'test_loss': 26.968356, 'test_f1': 0.50759, 'test_avg_loss': 0.839091}, 'Results_avg': {'test_total': 22.0, 'test_correct': 15.9, 'test_acc': 0.67066, 'test_loss': 18.460007, 'test_f1': 0.469686, 'test_avg_loss': 0.942535}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 15.9, 'test_acc_std': 0.264544, 'test_acc_bottom_decile': 0.428571, 'test_acc_top_decile': 0.931034, 'test_acc_min': 0.0, 'test_acc_max': 0.931034, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.931034, 'test_acc_cos1': 0.930245, 'test_acc_entropy': 2.17584, 'test_loss_std': 11.902197, 'test_loss_bottom_decile': 5.62084, 'test_loss_top_decile': 45.608872, 'test_loss_min': 3.78498, 'test_loss_max': 45.608872, 'test_loss_bottom10%': 3.78498, 'test_loss_top10%': 45.608872, 'test_loss_cos1': 0.840452, 'test_loss_entropy': 2.099742, 'test_f1_std': 0.250355, 'test_f1_bottom_decile': 0.222222, 'test_f1_top_decile': 0.911111, 'test_f1_min': 0.0, 'test_f1_max': 0.911111, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.911111, 'test_f1_cos1': 0.882465, 'test_f1_entropy': 2.121312, 'test_avg_loss_std': 0.429501, 'test_avg_loss_bottom_decile': 0.641707, 'test_avg_loss_top_decile': 2.030761, 'test_avg_loss_min': 0.540711, 'test_avg_loss_max': 2.030761, 'test_avg_loss_bottom10%': 0.540711, 'test_avg_loss_top10%': 2.030761, 'test_avg_loss_cos1': 0.909975, 'test_avg_loss_entropy': 2.2131}}
2023-02-24 22:30:50,045 (client:306) INFO: {'Role': 'Client #5', 'Round': 5, 'Results_raw': {'train_total': 112, 'train_correct': 92.0, 'train_acc': 0.821429, 'train_loss': 45.388835, 'train_f1': 0.669031, 'train_avg_loss': 0.405257}}
2023-02-24 22:30:50,692 (client:306) INFO: {'Role': 'Client #10', 'Round': 5, 'Results_raw': {'train_total': 67, 'train_correct': 47.0, 'train_acc': 0.701493, 'train_loss': 46.801424, 'train_f1': 0.439731, 'train_avg_loss': 0.698529}}
2023-02-24 22:30:50,840 (client:306) INFO: {'Role': 'Client #8', 'Round': 5, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 28.739746, 'train_f1': 0.190476, 'train_avg_loss': 1.915983}}
2023-02-24 22:30:52,436 (client:306) INFO: {'Role': 'Client #9', 'Round': 5, 'Results_raw': {'train_total': 142, 'train_correct': 109.0, 'train_acc': 0.767606, 'train_loss': 69.407926, 'train_f1': 0.701053, 'train_avg_loss': 0.488788}}
2023-02-24 22:30:52,837 (client:306) INFO: {'Role': 'Client #2', 'Round': 5, 'Results_raw': {'train_total': 36, 'train_correct': 22.0, 'train_acc': 0.611111, 'train_loss': 32.307577, 'train_f1': 0.459309, 'train_avg_loss': 0.897433}}
2023-02-24 22:30:52,938 (client:306) INFO: {'Role': 'Client #4', 'Round': 5, 'Results_raw': {'train_total': 7, 'train_correct': 4.0, 'train_acc': 0.571429, 'train_loss': 7.815123, 'train_f1': 0.444444, 'train_avg_loss': 1.116446}}
2023-02-24 22:30:55,305 (client:306) INFO: {'Role': 'Client #1', 'Round': 5, 'Results_raw': {'train_total': 200, 'train_correct': 172.0, 'train_acc': 0.86, 'train_loss': 97.920452, 'train_f1': 0.391441, 'train_avg_loss': 0.489602}}
2023-02-24 22:30:57,487 (client:306) INFO: {'Role': 'Client #3', 'Round': 5, 'Results_raw': {'train_total': 169, 'train_correct': 154.0, 'train_acc': 0.911243, 'train_loss': 53.013139, 'train_f1': 0.317853, 'train_avg_loss': 0.313687}}
2023-02-24 22:30:58,038 (client:306) INFO: {'Role': 'Client #7', 'Round': 5, 'Results_raw': {'train_total': 45, 'train_correct': 33.0, 'train_acc': 0.733333, 'train_loss': 34.595225, 'train_f1': 0.343844, 'train_avg_loss': 0.768783}}
2023-02-24 22:30:59,310 (client:306) INFO: {'Role': 'Client #6', 'Round': 5, 'Results_raw': {'train_total': 84, 'train_correct': 39.0, 'train_acc': 0.464286, 'train_loss': 82.004431, 'train_f1': 0.379536, 'train_avg_loss': 0.976243}}
2023-02-24 22:30:59,330 (monitor:541) INFO: {'Role': 'Server #', 'Round': 5, 'Results_model_metric': {}}
2023-02-24 22:30:59,333 (server:330) INFO: Server: Starting evaluation at the end of round 5.
2023-02-24 22:30:59,335 (server:336) INFO: ----------- Starting a new training round (Round #6) -------------
2023-02-24 22:30:59,588 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:59,691 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:30:59,803 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:00,097 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:00,561 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:00,619 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:01,151 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:01,213 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:01,384 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:01,516 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:01,518 (server:590) INFO: {'Role': 'Server #', 'Round': 5, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 14.4, 'test_acc': 0.654545, 'test_loss': 25.004644, 'test_f1': 0.482896, 'test_avg_loss': 0.792954}, 'Results_avg': {'test_total': 22.0, 'test_correct': 14.4, 'test_acc': 0.622, 'test_loss': 17.444977, 'test_f1': 0.446799, 'test_avg_loss': 0.918113}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 14.4, 'test_acc_std': 0.245302, 'test_acc_bottom_decile': 0.428571, 'test_acc_top_decile': 0.886364, 'test_acc_min': 0.0, 'test_acc_max': 0.886364, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.886364, 'test_acc_cos1': 0.93027, 'test_acc_entropy': 2.176633, 'test_loss_std': 11.417649, 'test_loss_bottom_decile': 5.72502, 'test_loss_top_decile': 43.884521, 'test_loss_min': 3.661028, 'test_loss_max': 43.884521, 'test_loss_bottom10%': 3.661028, 'test_loss_top10%': 43.884521, 'test_loss_cos1': 0.836721, 'test_loss_entropy': 2.095801, 'test_f1_std': 0.251812, 'test_f1_bottom_decile': 0.210526, 'test_f1_top_decile': 0.884877, 'test_f1_min': 0.0, 'test_f1_max': 0.884877, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.884877, 'test_f1_cos1': 0.871169, 'test_f1_entropy': 2.106319, 'test_avg_loss_std': 0.508537, 'test_avg_loss_bottom_decile': 0.529439, 'test_avg_loss_top_decile': 2.259868, 'test_avg_loss_min': 0.523004, 'test_avg_loss_max': 2.259868, 'test_avg_loss_bottom10%': 0.523004, 'test_avg_loss_top10%': 2.259868, 'test_avg_loss_cos1': 0.874774, 'test_avg_loss_entropy': 2.175681}}
2023-02-24 22:31:01,921 (client:306) INFO: {'Role': 'Client #7', 'Round': 6, 'Results_raw': {'train_total': 45, 'train_correct': 35.0, 'train_acc': 0.777778, 'train_loss': 27.761035, 'train_f1': 0.414545, 'train_avg_loss': 0.616912}}
2023-02-24 22:31:03,326 (client:306) INFO: {'Role': 'Client #9', 'Round': 6, 'Results_raw': {'train_total': 142, 'train_correct': 115.0, 'train_acc': 0.809859, 'train_loss': 61.881441, 'train_f1': 0.770185, 'train_avg_loss': 0.435785}}
2023-02-24 22:31:05,381 (client:306) INFO: {'Role': 'Client #3', 'Round': 6, 'Results_raw': {'train_total': 169, 'train_correct': 154.0, 'train_acc': 0.911243, 'train_loss': 46.98736, 'train_f1': 0.317853, 'train_avg_loss': 0.278032}}
2023-02-24 22:31:05,783 (client:306) INFO: {'Role': 'Client #2', 'Round': 6, 'Results_raw': {'train_total': 36, 'train_correct': 17.0, 'train_acc': 0.472222, 'train_loss': 39.853693, 'train_f1': 0.357576, 'train_avg_loss': 1.107047}}
2023-02-24 22:31:07,290 (client:306) INFO: {'Role': 'Client #5', 'Round': 6, 'Results_raw': {'train_total': 112, 'train_correct': 93.0, 'train_acc': 0.830357, 'train_loss': 44.313487, 'train_f1': 0.71638, 'train_avg_loss': 0.395656}}
2023-02-24 22:31:08,234 (client:306) INFO: {'Role': 'Client #6', 'Round': 6, 'Results_raw': {'train_total': 84, 'train_correct': 38.0, 'train_acc': 0.452381, 'train_loss': 84.041181, 'train_f1': 0.315281, 'train_avg_loss': 1.00049}}
2023-02-24 22:31:10,517 (client:306) INFO: {'Role': 'Client #1', 'Round': 6, 'Results_raw': {'train_total': 200, 'train_correct': 173.0, 'train_acc': 0.865, 'train_loss': 86.269266, 'train_f1': 0.33806, 'train_avg_loss': 0.431346}}
2023-02-24 22:31:10,652 (client:306) INFO: {'Role': 'Client #4', 'Round': 6, 'Results_raw': {'train_total': 7, 'train_correct': 4.0, 'train_acc': 0.571429, 'train_loss': 7.357636, 'train_f1': 0.444444, 'train_avg_loss': 1.051091}}
2023-02-24 22:31:11,366 (client:306) INFO: {'Role': 'Client #10', 'Round': 6, 'Results_raw': {'train_total': 67, 'train_correct': 46.0, 'train_acc': 0.686567, 'train_loss': 45.375731, 'train_f1': 0.448996, 'train_avg_loss': 0.67725}}
2023-02-24 22:31:11,601 (client:306) INFO: {'Role': 'Client #8', 'Round': 6, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 34.545075, 'train_f1': 0.222222, 'train_avg_loss': 2.303005}}
2023-02-24 22:31:11,623 (monitor:541) INFO: {'Role': 'Server #', 'Round': 6, 'Results_model_metric': {}}
2023-02-24 22:31:11,625 (server:330) INFO: Server: Starting evaluation at the end of round 6.
2023-02-24 22:31:11,628 (server:336) INFO: ----------- Starting a new training round (Round #7) -------------
2023-02-24 22:31:11,757 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:11,863 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:11,991 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:12,241 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:12,683 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:12,779 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:13,403 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:13,470 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:13,717 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:13,864 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:13,865 (server:590) INFO: {'Role': 'Server #', 'Round': 6, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 16.6, 'test_acc': 0.754545, 'test_loss': 22.233383, 'test_f1': 0.534437, 'test_avg_loss': 0.712855}, 'Results_avg': {'test_total': 22.0, 'test_correct': 16.6, 'test_acc': 0.702038, 'test_loss': 15.682802, 'test_f1': 0.497804, 'test_avg_loss': 0.820505}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 16.6, 'test_acc_std': 0.28435, 'test_acc_bottom_decile': 0.357143, 'test_acc_top_decile': 0.965517, 'test_acc_min': 0.0, 'test_acc_max': 0.965517, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.965517, 'test_acc_cos1': 0.926859, 'test_acc_entropy': 2.169585, 'test_loss_std': 10.043789, 'test_loss_bottom_decile': 4.696352, 'test_loss_top_decile': 37.502016, 'test_loss_min': 2.445535, 'test_loss_max': 37.502016, 'test_loss_bottom10%': 2.445535, 'test_loss_top10%': 37.502016, 'test_loss_cos1': 0.842106, 'test_loss_entropy': 2.093103, 'test_f1_std': 0.267037, 'test_f1_bottom_decile': 0.196078, 'test_f1_top_decile': 0.916084, 'test_f1_min': 0.0, 'test_f1_max': 0.916084, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.916084, 'test_f1_cos1': 0.881218, 'test_f1_entropy': 2.115556, 'test_avg_loss_std': 0.468366, 'test_avg_loss_bottom_decile': 0.477781, 'test_avg_loss_top_decile': 1.988044, 'test_avg_loss_min': 0.349362, 'test_avg_loss_max': 1.988044, 'test_avg_loss_bottom10%': 0.349362, 'test_avg_loss_top10%': 1.988044, 'test_avg_loss_cos1': 0.868468, 'test_avg_loss_entropy': 2.163171}}
2023-02-24 22:31:15,107 (client:306) INFO: {'Role': 'Client #5', 'Round': 7, 'Results_raw': {'train_total': 112, 'train_correct': 96.0, 'train_acc': 0.857143, 'train_loss': 39.121421, 'train_f1': 0.746463, 'train_avg_loss': 0.349298}}
2023-02-24 22:31:15,253 (client:306) INFO: {'Role': 'Client #4', 'Round': 7, 'Results_raw': {'train_total': 7, 'train_correct': 4.0, 'train_acc': 0.571429, 'train_loss': 6.624981, 'train_f1': 0.416667, 'train_avg_loss': 0.946426}}
2023-02-24 22:31:16,293 (client:306) INFO: {'Role': 'Client #6', 'Round': 7, 'Results_raw': {'train_total': 84, 'train_correct': 42.0, 'train_acc': 0.5, 'train_loss': 79.329803, 'train_f1': 0.40903, 'train_avg_loss': 0.944402}}
2023-02-24 22:31:16,989 (client:306) INFO: {'Role': 'Client #10', 'Round': 7, 'Results_raw': {'train_total': 67, 'train_correct': 46.0, 'train_acc': 0.686567, 'train_loss': 46.662918, 'train_f1': 0.452042, 'train_avg_loss': 0.696461}}
2023-02-24 22:31:17,367 (client:306) INFO: {'Role': 'Client #2', 'Round': 7, 'Results_raw': {'train_total': 36, 'train_correct': 16.0, 'train_acc': 0.444444, 'train_loss': 34.003366, 'train_f1': 0.374167, 'train_avg_loss': 0.944538}}
2023-02-24 22:31:18,996 (client:306) INFO: {'Role': 'Client #9', 'Round': 7, 'Results_raw': {'train_total': 142, 'train_correct': 113.0, 'train_acc': 0.795775, 'train_loss': 71.803065, 'train_f1': 0.756547, 'train_avg_loss': 0.505655}}
2023-02-24 22:31:19,194 (client:306) INFO: {'Role': 'Client #8', 'Round': 7, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 27.374475, 'train_f1': 0.222222, 'train_avg_loss': 1.824965}}
2023-02-24 22:31:21,490 (client:306) INFO: {'Role': 'Client #1', 'Round': 7, 'Results_raw': {'train_total': 200, 'train_correct': 172.0, 'train_acc': 0.86, 'train_loss': 83.435917, 'train_f1': 0.404098, 'train_avg_loss': 0.41718}}
2023-02-24 22:31:23,861 (client:306) INFO: {'Role': 'Client #3', 'Round': 7, 'Results_raw': {'train_total': 169, 'train_correct': 156.0, 'train_acc': 0.923077, 'train_loss': 43.127809, 'train_f1': 0.321981, 'train_avg_loss': 0.255194}}
2023-02-24 22:31:24,514 (client:306) INFO: {'Role': 'Client #7', 'Round': 7, 'Results_raw': {'train_total': 45, 'train_correct': 36.0, 'train_acc': 0.8, 'train_loss': 30.979446, 'train_f1': 0.423434, 'train_avg_loss': 0.688432}}
2023-02-24 22:31:24,530 (monitor:541) INFO: {'Role': 'Server #', 'Round': 7, 'Results_model_metric': {}}
2023-02-24 22:31:24,533 (server:330) INFO: Server: Starting evaluation at the end of round 7.
2023-02-24 22:31:24,535 (server:336) INFO: ----------- Starting a new training round (Round #8) -------------
2023-02-24 22:31:24,775 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:24,936 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:25,059 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:25,363 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:25,781 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:25,848 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:26,410 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:26,484 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:26,714 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:26,816 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:26,817 (server:590) INFO: {'Role': 'Server #', 'Round': 7, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 16.5, 'test_acc': 0.75, 'test_loss': 20.536139, 'test_f1': 0.529749, 'test_avg_loss': 0.694973}, 'Results_avg': {'test_total': 22.0, 'test_correct': 16.5, 'test_acc': 0.70338, 'test_loss': 15.289403, 'test_f1': 0.495959, 'test_avg_loss': 0.838359}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 16.5, 'test_acc_std': 0.251399, 'test_acc_bottom_decile': 0.428571, 'test_acc_top_decile': 0.931034, 'test_acc_min': 0.083333, 'test_acc_max': 0.931034, 'test_acc_bottom10%': 0.083333, 'test_acc_top10%': 0.931034, 'test_acc_cos1': 0.941661, 'test_acc_entropy': 2.21477, 'test_loss_std': 9.732155, 'test_loss_bottom_decile': 4.579329, 'test_loss_top_decile': 33.39457, 'test_loss_min': 2.175029, 'test_loss_max': 33.39457, 'test_loss_bottom10%': 2.175029, 'test_loss_top10%': 33.39457, 'test_loss_cos1': 0.843598, 'test_loss_entropy': 2.086799, 'test_f1_std': 0.248569, 'test_f1_bottom_decile': 0.222222, 'test_f1_top_decile': 0.911111, 'test_f1_min': 0.066667, 'test_f1_max': 0.911111, 'test_f1_bottom10%': 0.066667, 'test_f1_top10%': 0.911111, 'test_f1_cos1': 0.894001, 'test_f1_entropy': 2.162333, 'test_avg_loss_std': 0.59057, 'test_avg_loss_bottom_decile': 0.393366, 'test_avg_loss_top_decile': 2.333348, 'test_avg_loss_min': 0.310718, 'test_avg_loss_max': 2.333348, 'test_avg_loss_bottom10%': 0.310718, 'test_avg_loss_top10%': 2.333348, 'test_avg_loss_cos1': 0.817525, 'test_avg_loss_entropy': 2.095526}}
2023-02-24 22:31:26,941 (client:306) INFO: {'Role': 'Client #4', 'Round': 8, 'Results_raw': {'train_total': 7, 'train_correct': 4.0, 'train_acc': 0.571429, 'train_loss': 7.437615, 'train_f1': 0.416667, 'train_avg_loss': 1.062516}}
2023-02-24 22:31:27,306 (client:306) INFO: {'Role': 'Client #2', 'Round': 8, 'Results_raw': {'train_total': 36, 'train_correct': 24.0, 'train_acc': 0.666667, 'train_loss': 32.978428, 'train_f1': 0.552047, 'train_avg_loss': 0.916067}}
2023-02-24 22:31:28,110 (client:306) INFO: {'Role': 'Client #10', 'Round': 8, 'Results_raw': {'train_total': 67, 'train_correct': 55.0, 'train_acc': 0.820896, 'train_loss': 33.409331, 'train_f1': 0.539731, 'train_avg_loss': 0.498647}}
2023-02-24 22:31:29,580 (client:306) INFO: {'Role': 'Client #9', 'Round': 8, 'Results_raw': {'train_total': 142, 'train_correct': 108.0, 'train_acc': 0.760563, 'train_loss': 74.17951, 'train_f1': 0.6837, 'train_avg_loss': 0.522391}}
2023-02-24 22:31:30,539 (client:306) INFO: {'Role': 'Client #6', 'Round': 8, 'Results_raw': {'train_total': 84, 'train_correct': 43.0, 'train_acc': 0.511905, 'train_loss': 81.183895, 'train_f1': 0.455052, 'train_avg_loss': 0.966475}}
2023-02-24 22:31:31,034 (client:306) INFO: {'Role': 'Client #7', 'Round': 8, 'Results_raw': {'train_total': 45, 'train_correct': 34.0, 'train_acc': 0.755556, 'train_loss': 29.353351, 'train_f1': 0.35614, 'train_avg_loss': 0.652297}}
2023-02-24 22:31:33,019 (client:306) INFO: {'Role': 'Client #3', 'Round': 8, 'Results_raw': {'train_total': 169, 'train_correct': 158.0, 'train_acc': 0.934911, 'train_loss': 40.84442, 'train_f1': 0.323108, 'train_avg_loss': 0.241683}}
2023-02-24 22:31:35,613 (client:306) INFO: {'Role': 'Client #1', 'Round': 8, 'Results_raw': {'train_total': 200, 'train_correct': 174.0, 'train_acc': 0.87, 'train_loss': 88.190508, 'train_f1': 0.311828, 'train_avg_loss': 0.440953}}
2023-02-24 22:31:35,893 (client:306) INFO: {'Role': 'Client #8', 'Round': 8, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 31.486974, 'train_f1': 0.222222, 'train_avg_loss': 2.099132}}
2023-02-24 22:31:37,713 (client:306) INFO: {'Role': 'Client #5', 'Round': 8, 'Results_raw': {'train_total': 112, 'train_correct': 101.0, 'train_acc': 0.901786, 'train_loss': 32.246417, 'train_f1': 0.82194, 'train_avg_loss': 0.287914}}
2023-02-24 22:31:37,734 (monitor:541) INFO: {'Role': 'Server #', 'Round': 8, 'Results_model_metric': {}}
2023-02-24 22:31:37,736 (server:330) INFO: Server: Starting evaluation at the end of round 8.
2023-02-24 22:31:37,738 (server:336) INFO: ----------- Starting a new training round (Round #9) -------------
2023-02-24 22:31:38,006 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:38,174 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:38,301 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:38,616 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:39,012 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:39,072 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:39,810 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:39,901 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:40,118 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:40,223 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:40,225 (server:590) INFO: {'Role': 'Server #', 'Round': 8, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 16.2, 'test_acc': 0.736364, 'test_loss': 21.565415, 'test_f1': 0.533588, 'test_avg_loss': 0.678431}, 'Results_avg': {'test_total': 22.0, 'test_correct': 16.2, 'test_acc': 0.685987, 'test_loss': 14.92549, 'test_f1': 0.496411, 'test_avg_loss': 0.771554}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 16.2, 'test_acc_std': 0.238056, 'test_acc_bottom_decile': 0.428571, 'test_acc_top_decile': 0.916667, 'test_acc_min': 0.083333, 'test_acc_max': 0.916667, 'test_acc_bottom10%': 0.083333, 'test_acc_top10%': 0.916667, 'test_acc_cos1': 0.944731, 'test_acc_entropy': 2.218774, 'test_loss_std': 9.816153, 'test_loss_bottom_decile': 4.363623, 'test_loss_top_decile': 37.357952, 'test_loss_min': 3.036368, 'test_loss_max': 37.357952, 'test_loss_bottom10%': 3.036368, 'test_loss_top10%': 37.357952, 'test_loss_cos1': 0.835501, 'test_loss_entropy': 2.090752, 'test_f1_std': 0.254143, 'test_f1_bottom_decile': 0.210526, 'test_f1_top_decile': 0.911111, 'test_f1_min': 0.074074, 'test_f1_max': 0.911111, 'test_f1_bottom10%': 0.074074, 'test_f1_top10%': 0.911111, 'test_f1_cos1': 0.890128, 'test_f1_entropy': 2.1581, 'test_avg_loss_std': 0.417164, 'test_avg_loss_bottom_decile': 0.478822, 'test_avg_loss_top_decile': 1.877841, 'test_avg_loss_min': 0.433767, 'test_avg_loss_max': 1.877841, 'test_avg_loss_bottom10%': 0.433767, 'test_avg_loss_top10%': 1.877841, 'test_avg_loss_cos1': 0.879655, 'test_avg_loss_entropy': 2.182611}}
2023-02-24 22:31:40,302 (client:306) INFO: {'Role': 'Client #4', 'Round': 9, 'Results_raw': {'train_total': 7, 'train_correct': 4.0, 'train_acc': 0.571429, 'train_loss': 6.349071, 'train_f1': 0.444444, 'train_avg_loss': 0.90701}}
2023-02-24 22:31:42,117 (client:306) INFO: {'Role': 'Client #3', 'Round': 9, 'Results_raw': {'train_total': 169, 'train_correct': 156.0, 'train_acc': 0.923077, 'train_loss': 48.910483, 'train_f1': 0.32, 'train_avg_loss': 0.289411}}
2023-02-24 22:31:42,573 (client:306) INFO: {'Role': 'Client #2', 'Round': 9, 'Results_raw': {'train_total': 36, 'train_correct': 24.0, 'train_acc': 0.666667, 'train_loss': 27.099627, 'train_f1': 0.51027, 'train_avg_loss': 0.752767}}
2023-02-24 22:31:44,605 (client:306) INFO: {'Role': 'Client #9', 'Round': 9, 'Results_raw': {'train_total': 142, 'train_correct': 112.0, 'train_acc': 0.788732, 'train_loss': 61.148316, 'train_f1': 0.749824, 'train_avg_loss': 0.430622}}
2023-02-24 22:31:44,892 (client:306) INFO: {'Role': 'Client #8', 'Round': 9, 'Results_raw': {'train_total': 15, 'train_correct': 2.0, 'train_acc': 0.133333, 'train_loss': 25.450872, 'train_f1': 0.222222, 'train_avg_loss': 1.696725}}
2023-02-24 22:31:45,836 (client:306) INFO: {'Role': 'Client #10', 'Round': 9, 'Results_raw': {'train_total': 67, 'train_correct': 53.0, 'train_acc': 0.791045, 'train_loss': 37.390362, 'train_f1': 0.524789, 'train_avg_loss': 0.558065}}
2023-02-24 22:31:46,564 (client:306) INFO: {'Role': 'Client #7', 'Round': 9, 'Results_raw': {'train_total': 45, 'train_correct': 31.0, 'train_acc': 0.688889, 'train_loss': 28.174505, 'train_f1': 0.325826, 'train_avg_loss': 0.6261}}
2023-02-24 22:31:47,530 (client:306) INFO: {'Role': 'Client #6', 'Round': 9, 'Results_raw': {'train_total': 84, 'train_correct': 51.0, 'train_acc': 0.607143, 'train_loss': 72.737671, 'train_f1': 0.413908, 'train_avg_loss': 0.865925}}
2023-02-24 22:31:49,682 (client:306) INFO: {'Role': 'Client #1', 'Round': 9, 'Results_raw': {'train_total': 200, 'train_correct': 173.0, 'train_acc': 0.865, 'train_loss': 81.348775, 'train_f1': 0.394697, 'train_avg_loss': 0.406744}}
2023-02-24 22:31:50,908 (client:306) INFO: {'Role': 'Client #5', 'Round': 9, 'Results_raw': {'train_total': 112, 'train_correct': 98.0, 'train_acc': 0.875, 'train_loss': 34.486898, 'train_f1': 0.778155, 'train_avg_loss': 0.307919}}
2023-02-24 22:31:50,916 (monitor:541) INFO: {'Role': 'Server #', 'Round': 9, 'Results_model_metric': {}}
2023-02-24 22:31:50,919 (server:347) INFO: Server: Training is finished! Starting evaluation.
2023-02-24 22:31:51,159 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:51,316 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:51,426 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:51,692 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:52,124 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:52,186 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:52,829 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:52,893 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:53,071 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:53,207 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 22:31:53,208 (server:590) INFO: {'Role': 'Server #', 'Round': 9, 'Results_weighted_avg': {'test_total': 22.0, 'test_correct': 16.8, 'test_acc': 0.763636, 'test_loss': 19.265049, 'test_f1': 0.545281, 'test_avg_loss': 0.638513}, 'Results_avg': {'test_total': 22.0, 'test_correct': 16.8, 'test_acc': 0.710018, 'test_loss': 14.047275, 'test_f1': 0.509075, 'test_avg_loss': 0.760346}, 'Results_fairness': {'test_total': 22.0, 'test_correct': 16.8, 'test_acc_std': 0.270548, 'test_acc_bottom_decile': 0.357143, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.083333, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.083333, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.93446, 'test_acc_entropy': 2.204897, 'test_loss_std': 9.213075, 'test_loss_bottom_decile': 4.307653, 'test_loss_top_decile': 32.388259, 'test_loss_min': 2.079914, 'test_loss_max': 32.388259, 'test_loss_bottom10%': 2.079914, 'test_loss_top10%': 32.388259, 'test_loss_cos1': 0.836197, 'test_loss_entropy': 2.078357, 'test_f1_std': 0.270609, 'test_f1_bottom_decile': 0.196078, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.066667, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.066667, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.882999, 'test_f1_entropy': 2.147457, 'test_avg_loss_std': 0.525235, 'test_avg_loss_bottom_decile': 0.361779, 'test_avg_loss_top_decile': 2.089958, 'test_avg_loss_min': 0.297131, 'test_avg_loss_max': 2.089958, 'test_avg_loss_bottom10%': 0.297131, 'test_avg_loss_top10%': 2.089958, 'test_avg_loss_cos1': 0.822778, 'test_avg_loss_entropy': 2.103111}}
2023-02-24 22:31:53,209 (server:395) INFO: Server: Final evaluation is finished! Starting merging results.
2023-02-24 22:31:53,210 (server:521) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'test_loss': 2.079914, 'test_total': 6.0, 'test_correct': 51.0, 'test_acc': 1.0, 'test_f1': 1.0, 'test_avg_loss': 0.297131}, 'client_summarized_weighted_avg': {'test_loss': 19.265049, 'test_total': 22.0, 'test_correct': 16.8, 'test_acc': 0.763636, 'test_f1': 0.545281, 'test_avg_loss': 0.638513}, 'client_summarized_avg': {'test_loss': 14.047275, 'test_total': 22.0, 'test_correct': 16.8, 'test_acc': 0.710018, 'test_f1': 0.509075, 'test_avg_loss': 0.760346}, 'client_summarized_fairness': {'test_loss_entropy': 2.037627, 'test_loss_cos1': 0.786369, 'test_loss_top10%': 236.599852, 'test_loss_bottom10%': 22.727636, 'test_loss_max': 236.599852, 'test_loss_min': 22.727636, 'test_loss_top_decile': 236.599852, 'test_loss_bottom_decile': 26.362926, 'test_loss_std': 65.156333, 'test_total': 22.0, 'test_correct': 7.5, 'test_acc_std': 0.244493, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.65, 'test_acc_min': 0.0, 'test_acc_max': 0.65, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.65, 'test_acc_cos1': 0.810581, 'test_acc_entropy': 1.953909, 'test_f1_std': 0.141788, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.391304, 'test_f1_min': 0.0, 'test_f1_max': 0.391304, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.391304, 'test_f1_cos1': 0.79813, 'test_f1_entropy': 1.939322, 'test_avg_loss_std': 0.069574, 'test_avg_loss_bottom_decile': 3.727447, 'test_avg_loss_top_decile': 3.970145, 'test_avg_loss_min': 3.721034, 'test_avg_loss_max': 3.970145, 'test_avg_loss_bottom10%': 3.721034, 'test_avg_loss_top10%': 3.970145, 'test_avg_loss_cos1': 0.999831, 'test_avg_loss_entropy': 2.302418}}}
2023-02-24 22:31:53,211 (server:540) INFO: {'Role': 'Client #1', 'Round': 10, 'Results_raw': {'test_total': 14, 'test_correct': 5.0, 'test_acc': 0.357143, 'test_loss': 17.963714, 'test_f1': 0.196078, 'test_avg_loss': 1.283122}}
2023-02-24 22:31:53,211 (server:540) INFO: {'Role': 'Client #2', 'Round': 10, 'Results_raw': {'test_total': 12, 'test_correct': 12.0, 'test_acc': 1.0, 'test_loss': 4.341352, 'test_f1': 1.0, 'test_avg_loss': 0.361779}}
2023-02-24 22:31:53,211 (server:540) INFO: {'Role': 'Client #3', 'Round': 10, 'Results_raw': {'test_total': 13, 'test_correct': 9.0, 'test_acc': 0.692308, 'test_loss': 11.061968, 'test_f1': 0.571429, 'test_avg_loss': 0.850921}}
2023-02-24 22:31:53,211 (server:540) INFO: {'Role': 'Client #4', 'Round': 10, 'Results_raw': {'test_total': 29, 'test_correct': 27.0, 'test_acc': 0.931034, 'test_loss': 10.534874, 'test_f1': 0.327273, 'test_avg_loss': 0.363272}}
2023-02-24 22:31:53,212 (server:540) INFO: {'Role': 'Client #5', 'Round': 10, 'Results_raw': {'test_total': 44, 'test_correct': 39.0, 'test_acc': 0.886364, 'test_loss': 18.719823, 'test_f1': 0.885833, 'test_avg_loss': 0.425451}}
2023-02-24 22:31:53,212 (server:540) INFO: {'Role': 'Client #6', 'Round': 10, 'Results_raw': {'test_total': 6, 'test_correct': 5.0, 'test_acc': 0.833333, 'test_loss': 4.307653, 'test_f1': 0.619048, 'test_avg_loss': 0.717942}}
2023-02-24 22:31:53,212 (server:540) INFO: {'Role': 'Client #7', 'Round': 10, 'Results_raw': {'test_total': 63, 'test_correct': 51.0, 'test_acc': 0.809524, 'test_loss': 32.388259, 'test_f1': 0.519022, 'test_avg_loss': 0.514099}}
2023-02-24 22:31:53,212 (server:540) INFO: {'Role': 'Client #8', 'Round': 10, 'Results_raw': {'test_total': 7, 'test_correct': 6.0, 'test_acc': 0.857143, 'test_loss': 2.079914, 'test_f1': 0.461538, 'test_avg_loss': 0.297131}}
2023-02-24 22:31:53,213 (server:540) INFO: {'Role': 'Client #9', 'Round': 10, 'Results_raw': {'test_total': 20, 'test_correct': 13.0, 'test_acc': 0.65, 'test_loss': 13.995691, 'test_f1': 0.44386, 'test_avg_loss': 0.699785}}
2023-02-24 22:31:53,213 (server:540) INFO: {'Role': 'Client #10', 'Round': 10, 'Results_raw': {'test_total': 12, 'test_correct': 1.0, 'test_acc': 0.083333, 'test_loss': 25.079501, 'test_f1': 0.066667, 'test_avg_loss': 2.089958}}
2023-02-24 22:31:53,213 (monitor:172) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 2.168268, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 2488000, 'total_download_bytes': 1161640, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,214 (client:513) INFO: ================= client 1 received finish message =================
2023-02-24 22:31:53,216 (monitor:172) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 2.165015, 'total_model_size': 3260030, 'total_flops': 530876544000.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,217 (client:513) INFO: ================= client 2 received finish message =================
2023-02-24 22:31:53,219 (monitor:172) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 2.162634, 'total_model_size': 3260030, 'total_flops': 95557777920.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,219 (client:513) INFO: ================= client 3 received finish message =================
2023-02-24 22:31:53,222 (monitor:172) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 2.159431, 'total_model_size': 3260030, 'total_flops': 448590679680.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,222 (client:513) INFO: ================= client 4 received finish message =================
2023-02-24 22:31:53,225 (monitor:172) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 2.156343, 'total_model_size': 3260030, 'total_flops': 18580679040.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,225 (client:513) INFO: ================= client 5 received finish message =================
2023-02-24 22:31:53,227 (monitor:172) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 2.15349, 'total_model_size': 3260030, 'total_flops': 297290864640.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,228 (client:513) INFO: ================= client 6 received finish message =================
2023-02-24 22:31:53,230 (monitor:172) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 2.151941, 'total_model_size': 3260030, 'total_flops': 222968148480.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,230 (client:513) INFO: ================= client 7 received finish message =================
2023-02-24 22:31:53,232 (monitor:172) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 2.148981, 'total_model_size': 3260030, 'total_flops': 119447222400.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,233 (client:513) INFO: ================= client 8 received finish message =================
2023-02-24 22:31:53,235 (monitor:172) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 2.1474, 'total_model_size': 3260030, 'total_flops': 39815740800.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,236 (client:513) INFO: ================= client 9 received finish message =================
2023-02-24 22:31:53,238 (monitor:172) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 2.144269, 'total_model_size': 3260030, 'total_flops': 376922346240.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,238 (client:513) INFO: ================= client 10 received finish message =================
2023-02-24 22:31:53,241 (monitor:172) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 2.141842, 'total_model_size': 3260030, 'total_flops': 177843642240.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 22:31:53,242 (monitor:337) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one
2023-02-24 22:31:53,246 (monitor:245) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 2.15451, 'sys_avg/total_model_size': '2.83M', 'sys_avg/total_flops': '197.09G', 'sys_avg/total_upload_bytes': '324.09K', 'sys_avg/total_download_bytes': '103.13K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2023-02-24 22:31:53,246 (monitor:248) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.008243, 'sys_std/total_model_size': '915.23K', 'sys_std/total_flops': '160.71G', 'sys_std/total_upload_bytes': '665.85K', 'sys_std/total_download_bytes': '326.12K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
