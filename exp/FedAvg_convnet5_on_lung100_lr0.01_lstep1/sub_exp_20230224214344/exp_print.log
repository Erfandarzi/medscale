2023-02-24 21:43:44,864 (logging:124) INFO: the current machine is at 192.168.0.1
2023-02-24 21:43:44,865 (logging:126) INFO: the current dir is /home/ubuntu/medscale
2023-02-24 21:43:44,865 (logging:127) INFO: the output dir is exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
2023-02-24 21:43:54,825 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:54,825 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:54,849 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:55,029 (utils:144) INFO: The device information file is not provided
2023-02-24 21:43:55,228 (fed_runner:169) INFO: Server has been set up ... 
2023-02-24 21:43:55,401 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:55,426 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:55,437 (fed_runner:221) INFO: Client 1 has been set up ... 
2023-02-24 21:43:55,550 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:55,574 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:55,583 (fed_runner:221) INFO: Client 2 has been set up ... 
2023-02-24 21:43:55,789 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:55,813 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:55,834 (fed_runner:221) INFO: Client 3 has been set up ... 
2023-02-24 21:43:56,019 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:56,066 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:56,086 (fed_runner:221) INFO: Client 4 has been set up ... 
2023-02-24 21:43:56,202 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:56,246 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:56,266 (fed_runner:221) INFO: Client 5 has been set up ... 
2023-02-24 21:43:56,399 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:56,422 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:56,430 (fed_runner:221) INFO: Client 6 has been set up ... 
2023-02-24 21:43:56,562 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:56,585 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:56,594 (fed_runner:221) INFO: Client 7 has been set up ... 
2023-02-24 21:43:56,734 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:56,757 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:56,768 (fed_runner:221) INFO: Client 8 has been set up ... 
2023-02-24 21:43:56,899 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:56,923 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:56,935 (fed_runner:221) INFO: Client 9 has been set up ... 
2023-02-24 21:43:57,145 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.
2023-02-24 21:43:57,168 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.5}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224214344
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2023-02-24 21:43:57,181 (fed_runner:221) INFO: Client 10 has been set up ... 
<<<<<<< HEAD
2023-02-24 21:43:57,181 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.
=======
<<<<<<< HEAD
2023-02-24 21:43:57,181 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.
=======
2023-02-24 21:43:57,181 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.
>>>>>>> fe4962455354c9c11afd9c9806ceda28eb280737
>>>>>>> 64b283ee525ef53c32509882719e74890329b83f
2023-02-24 21:43:57,182 (trainer:349) INFO: Num of original para names: 39.
2023-02-24 21:43:57,182 (trainer:350) INFO: Num of original trainable para names: 24.
2023-02-24 21:43:57,182 (trainer:352) INFO: Num of preserved para names in local update: 39. 
Preserved para names in local update: {'conv1.bias', 'bn4.weight', 'bn5.running_mean', 'bn4.running_mean', 'bn5.num_batches_tracked', 'conv4.bias', 'conv5.weight', 'bn1.running_var', 'bn2.weight', 'conv3.weight', 'bn5.weight', 'bn1.weight', 'bn5.running_var', 'bn1.num_batches_tracked', 'bn2.running_var', 'conv1.weight', 'bn4.running_var', 'bn2.num_batches_tracked', 'conv5.bias', 'fc2.weight', 'fc2.bias', 'bn3.num_batches_tracked', 'fc1.bias', 'bn3.bias', 'conv2.bias', 'bn3.running_mean', 'bn4.num_batches_tracked', 'bn3.weight', 'bn3.running_var', 'bn2.bias', 'bn2.running_mean', 'conv4.weight', 'bn4.bias', 'fc1.weight', 'bn1.running_mean', 'conv3.bias', 'bn5.bias', 'conv2.weight', 'bn1.bias'}.
2023-02-24 21:43:57,182 (trainer:356) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2023-02-24 21:43:57,182 (trainer:361) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	}
2023-02-24 21:43:57,197 (server:804) INFO: ----------- Starting training (Round #0) -------------
2023-02-24 21:44:04,241 (client:306) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_loss': 221.386249, 'train_correct': 163.0, 'train_f1': 0.133826, 'train_total': 186, 'train_avg_loss': 1.190249, 'train_acc': 0.876344}}
2023-02-24 21:44:04,909 (client:306) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_loss': 139.61306, 'train_correct': 14.0, 'train_f1': 0.121739, 'train_total': 36, 'train_avg_loss': 3.878141, 'train_acc': 0.388889}}
2023-02-24 21:44:08,072 (client:306) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_loss': 295.913379, 'train_correct': 188.0, 'train_f1': 0.149562, 'train_total': 234, 'train_avg_loss': 1.264587, 'train_acc': 0.803419}}
2023-02-24 21:44:08,685 (client:306) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_loss': 201.459806, 'train_correct': 21.0, 'train_f1': 0.1, 'train_total': 56, 'train_avg_loss': 3.597497, 'train_acc': 0.375}}
2023-02-24 21:44:08,830 (client:306) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_loss': 14.342553, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 3, 'train_avg_loss': 4.780851, 'train_acc': 0.0}}
2023-02-24 21:44:10,045 (client:306) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_loss': 248.546931, 'train_correct': 32.0, 'train_f1': 0.09697, 'train_total': 83, 'train_avg_loss': 2.994541, 'train_acc': 0.385542}}
2023-02-24 21:44:10,201 (client:306) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_loss': 55.060295, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 12, 'train_avg_loss': 4.588358, 'train_acc': 0.0}}
2023-02-24 21:44:11,945 (client:306) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_loss': 280.004652, 'train_correct': 85.0, 'train_f1': 0.140622, 'train_total': 135, 'train_avg_loss': 2.074109, 'train_acc': 0.62963}}
2023-02-24 21:44:13,345 (client:306) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_loss': 250.22358, 'train_correct': 72.0, 'train_f1': 0.163636, 'train_total': 107, 'train_avg_loss': 2.338538, 'train_acc': 0.672897}}
2023-02-24 21:44:13,684 (client:306) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_loss': 110.747803, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 25, 'train_avg_loss': 4.429912, 'train_acc': 0.0}}
2023-02-24 21:44:13,701 (monitor:541) INFO: {'Role': 'Server #', 'Round': 0, 'Results_model_metric': {}}
2023-02-24 21:44:13,704 (server:330) INFO: Server: Starting evaluation at the end of round 0.
2023-02-24 21:44:13,706 (server:336) INFO: ----------- Starting a new training round (Round #1) -------------
2023-02-24 21:44:13,910 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:14,001 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:14,205 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:14,528 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:14,615 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:14,952 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:15,100 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:15,291 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:15,426 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:15,825 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:15,827 (server:590) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'test_loss': 101.292246, 'test_correct': 7.5, 'test_f1': 0.22853, 'test_total': 22.0, 'test_avg_loss': 3.774901, 'test_acc': 0.340909}, 'Results_avg': {'test_loss': 83.047814, 'test_correct': 7.5, 'test_f1': 0.241477, 'test_total': 22.0, 'test_avg_loss': 3.784907, 'test_acc': 0.357011}, 'Results_fairness': {'test_correct': 7.5, 'test_total': 22.0, 'test_loss_std': 38.816742, 'test_loss_bottom_decile': 39.630427, 'test_loss_top_decile': 172.917059, 'test_loss_min': 37.744591, 'test_loss_max': 172.917059, 'test_loss_bottom10%': 37.744591, 'test_loss_top10%': 172.917059, 'test_loss_cos1': 0.905928, 'test_loss_entropy': 2.201076, 'test_f1_std': 0.293461, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.635398, 'test_f1_entropy': 1.625408, 'test_avg_loss_std': 0.070683, 'test_avg_loss_bottom_decile': 3.705928, 'test_avg_loss_top_decile': 3.963043, 'test_avg_loss_min': 3.700572, 'test_avg_loss_max': 3.963043, 'test_avg_loss_bottom10%': 3.700572, 'test_avg_loss_top10%': 3.963043, 'test_avg_loss_cos1': 0.999826, 'test_avg_loss_entropy': 2.302412, 'test_acc_std': 0.362454, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.701737, 'test_acc_entropy': 1.721349}}
2023-02-24 21:44:16,348 (client:306) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'train_loss': 61.21489, 'train_correct': 38.0, 'train_f1': 0.418026, 'train_total': 56, 'train_avg_loss': 1.093123, 'train_acc': 0.678571}}
2023-02-24 21:44:16,514 (client:306) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'train_loss': 32.743255, 'train_correct': 1.0, 'train_f1': 0.133333, 'train_total': 12, 'train_avg_loss': 2.728605, 'train_acc': 0.083333}}
2023-02-24 21:44:17,762 (client:306) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'train_loss': 82.491201, 'train_correct': 88.0, 'train_f1': 0.498396, 'train_total': 107, 'train_avg_loss': 0.770946, 'train_acc': 0.82243}}
2023-02-24 21:44:18,233 (client:306) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'train_loss': 40.407362, 'train_correct': 31.0, 'train_f1': 0.573545, 'train_total': 36, 'train_avg_loss': 1.122427, 'train_acc': 0.861111}}
2023-02-24 21:44:20,061 (client:306) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'train_loss': 75.461383, 'train_correct': 177.0, 'train_f1': 0.325069, 'train_total': 186, 'train_avg_loss': 0.405706, 'train_acc': 0.951613}}
2023-02-24 21:44:22,587 (client:306) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'train_loss': 132.85311, 'train_correct': 208.0, 'train_f1': 0.315152, 'train_total': 234, 'train_avg_loss': 0.567748, 'train_acc': 0.888889}}
2023-02-24 21:44:22,681 (client:306) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'train_loss': 6.146622, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 3, 'train_avg_loss': 2.048874, 'train_acc': 0.0}}
2023-02-24 21:44:22,955 (client:306) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'train_loss': 46.423023, 'train_correct': 15.0, 'train_f1': 0.480769, 'train_total': 25, 'train_avg_loss': 1.856921, 'train_acc': 0.6}}
2023-02-24 21:44:23,847 (client:306) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'train_loss': 109.413857, 'train_correct': 45.0, 'train_f1': 0.375, 'train_total': 83, 'train_avg_loss': 1.318239, 'train_acc': 0.542169}}
2023-02-24 21:44:25,225 (client:306) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'train_loss': 107.29758, 'train_correct': 102.0, 'train_f1': 0.612642, 'train_total': 135, 'train_avg_loss': 0.794797, 'train_acc': 0.755556}}
2023-02-24 21:44:25,236 (monitor:541) INFO: {'Role': 'Server #', 'Round': 1, 'Results_model_metric': {}}
2023-02-24 21:44:25,241 (server:330) INFO: Server: Starting evaluation at the end of round 1.
2023-02-24 21:44:25,244 (server:336) INFO: ----------- Starting a new training round (Round #2) -------------
2023-02-24 21:44:25,435 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:25,525 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:25,743 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:25,994 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:26,159 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:26,441 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:26,573 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:26,794 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:26,971 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:27,400 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:27,402 (server:590) INFO: {'Role': 'Server #', 'Round': 1, 'Results_weighted_avg': {'test_loss': 74.851422, 'test_correct': 7.5, 'test_f1': 0.22853, 'test_total': 22.0, 'test_avg_loss': 2.798913, 'test_acc': 0.340909}, 'Results_avg': {'test_loss': 61.576086, 'test_correct': 7.5, 'test_f1': 0.241477, 'test_total': 22.0, 'test_avg_loss': 2.827638, 'test_acc': 0.357011}, 'Results_fairness': {'test_correct': 7.5, 'test_total': 22.0, 'test_loss_std': 28.393196, 'test_loss_bottom_decile': 33.484108, 'test_loss_top_decile': 126.522918, 'test_loss_min': 27.936435, 'test_loss_max': 126.522918, 'test_loss_bottom10%': 27.936435, 'test_loss_top10%': 126.522918, 'test_loss_cos1': 0.908108, 'test_loss_entropy': 2.204699, 'test_f1_std': 0.293461, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.635398, 'test_f1_entropy': 1.625408, 'test_avg_loss_std': 0.204602, 'test_avg_loss_bottom_decile': 2.608609, 'test_avg_loss_top_decile': 3.348411, 'test_avg_loss_min': 2.587534, 'test_avg_loss_max': 3.348411, 'test_avg_loss_bottom10%': 2.587534, 'test_avg_loss_top10%': 3.348411, 'test_avg_loss_cos1': 0.997392, 'test_avg_loss_entropy': 2.300044, 'test_acc_std': 0.362454, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.701737, 'test_acc_entropy': 1.721349}}
2023-02-24 21:44:27,460 (client:306) INFO: {'Role': 'Client #4', 'Round': 2, 'Results_raw': {'train_loss': 4.881516, 'train_correct': 1.0, 'train_f1': 0.166667, 'train_total': 3, 'train_avg_loss': 1.627172, 'train_acc': 0.333333}}
2023-02-24 21:44:28,836 (client:306) INFO: {'Role': 'Client #9', 'Round': 2, 'Results_raw': {'train_loss': 91.147538, 'train_correct': 93.0, 'train_f1': 0.595, 'train_total': 135, 'train_avg_loss': 0.675167, 'train_acc': 0.688889}}
2023-02-24 21:44:29,064 (client:306) INFO: {'Role': 'Client #8', 'Round': 2, 'Results_raw': {'train_loss': 31.402039, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 12, 'train_avg_loss': 2.616837, 'train_acc': 0.0}}
2023-02-24 21:44:29,345 (client:306) INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'train_loss': 30.452262, 'train_correct': 13.0, 'train_f1': 0.388889, 'train_total': 25, 'train_avg_loss': 1.21809, 'train_acc': 0.52}}
2023-02-24 21:44:30,537 (client:306) INFO: {'Role': 'Client #5', 'Round': 2, 'Results_raw': {'train_loss': 57.003651, 'train_correct': 86.0, 'train_f1': 0.603913, 'train_total': 107, 'train_avg_loss': 0.532744, 'train_acc': 0.803738}}
2023-02-24 21:44:33,591 (client:306) INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'train_loss': 107.616648, 'train_correct': 210.0, 'train_f1': 0.316027, 'train_total': 234, 'train_avg_loss': 0.4599, 'train_acc': 0.897436}}
2023-02-24 21:44:33,972 (client:306) INFO: {'Role': 'Client #7', 'Round': 2, 'Results_raw': {'train_loss': 30.516999, 'train_correct': 24.0, 'train_f1': 0.329612, 'train_total': 36, 'train_avg_loss': 0.847694, 'train_acc': 0.666667}}
2023-02-24 21:44:35,984 (client:306) INFO: {'Role': 'Client #3', 'Round': 2, 'Results_raw': {'train_loss': 55.190466, 'train_correct': 170.0, 'train_f1': 0.318352, 'train_total': 186, 'train_avg_loss': 0.296723, 'train_acc': 0.913978}}
2023-02-24 21:44:37,080 (client:306) INFO: {'Role': 'Client #6', 'Round': 2, 'Results_raw': {'train_loss': 96.368746, 'train_correct': 39.0, 'train_f1': 0.316762, 'train_total': 83, 'train_avg_loss': 1.161069, 'train_acc': 0.46988}}
2023-02-24 21:44:37,885 (client:306) INFO: {'Role': 'Client #10', 'Round': 2, 'Results_raw': {'train_loss': 51.252891, 'train_correct': 31.0, 'train_f1': 0.355418, 'train_total': 56, 'train_avg_loss': 0.91523, 'train_acc': 0.553571}}
2023-02-24 21:44:37,906 (monitor:541) INFO: {'Role': 'Server #', 'Round': 2, 'Results_model_metric': {}}
2023-02-24 21:44:37,908 (server:330) INFO: Server: Starting evaluation at the end of round 2.
2023-02-24 21:44:37,910 (server:336) INFO: ----------- Starting a new training round (Round #3) -------------
2023-02-24 21:44:38,111 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:38,254 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:38,464 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:38,720 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:38,813 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:39,206 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:39,334 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:39,588 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:39,716 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:40,083 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:40,085 (server:590) INFO: {'Role': 'Server #', 'Round': 2, 'Results_weighted_avg': {'test_loss': 44.023852, 'test_correct': 8.4, 'test_f1': 0.267062, 'test_total': 22.0, 'test_avg_loss': 1.642943, 'test_acc': 0.381818}, 'Results_avg': {'test_loss': 36.144743, 'test_correct': 8.4, 'test_f1': 0.281016, 'test_total': 22.0, 'test_avg_loss': 1.674015, 'test_acc': 0.39759}, 'Results_fairness': {'test_correct': 8.4, 'test_total': 22.0, 'test_loss_std': 17.82365, 'test_loss_bottom_decile': 23.465279, 'test_loss_top_decile': 74.18333, 'test_loss_min': 15.584602, 'test_loss_max': 74.18333, 'test_loss_bottom10%': 15.584602, 'test_loss_top10%': 74.18333, 'test_loss_cos1': 0.896882, 'test_loss_entropy': 2.191606, 'test_f1_std': 0.281999, 'test_f1_bottom_decile': 0.024691, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.70587, 'test_f1_entropy': 1.841689, 'test_avg_loss_std': 0.363972, 'test_avg_loss_bottom_decile': 1.204698, 'test_avg_loss_top_decile': 2.513196, 'test_avg_loss_min': 1.173264, 'test_avg_loss_max': 2.513196, 'test_avg_loss_bottom10%': 1.173264, 'test_avg_loss_top10%': 2.513196, 'test_avg_loss_cos1': 0.97717, 'test_avg_loss_entropy': 2.279728, 'test_acc_std': 0.33859, 'test_acc_bottom_decile': 0.037037, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.761335, 'test_acc_entropy': 1.910095}}
2023-02-24 21:44:41,231 (client:306) INFO: {'Role': 'Client #5', 'Round': 3, 'Results_raw': {'train_loss': 54.758992, 'train_correct': 87.0, 'train_f1': 0.632555, 'train_total': 107, 'train_avg_loss': 0.511766, 'train_acc': 0.813084}}
2023-02-24 21:44:41,855 (client:306) INFO: {'Role': 'Client #10', 'Round': 3, 'Results_raw': {'train_loss': 50.069987, 'train_correct': 25.0, 'train_f1': 0.290559, 'train_total': 56, 'train_avg_loss': 0.894107, 'train_acc': 0.446429}}
2023-02-24 21:44:41,978 (client:306) INFO: {'Role': 'Client #8', 'Round': 3, 'Results_raw': {'train_loss': 28.850638, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 12, 'train_avg_loss': 2.40422, 'train_acc': 0.0}}
2023-02-24 21:44:43,642 (client:306) INFO: {'Role': 'Client #9', 'Round': 3, 'Results_raw': {'train_loss': 75.006304, 'train_correct': 105.0, 'train_f1': 0.699198, 'train_total': 135, 'train_avg_loss': 0.555602, 'train_acc': 0.777778}}
2023-02-24 21:44:43,892 (client:306) INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'train_loss': 27.972549, 'train_correct': 14.0, 'train_f1': 0.444444, 'train_total': 25, 'train_avg_loss': 1.118902, 'train_acc': 0.56}}
2023-02-24 21:44:43,945 (client:306) INFO: {'Role': 'Client #4', 'Round': 3, 'Results_raw': {'train_loss': 4.015756, 'train_correct': 2.0, 'train_f1': 0.4, 'train_total': 3, 'train_avg_loss': 1.338585, 'train_acc': 0.666667}}
2023-02-24 21:44:46,471 (client:306) INFO: {'Role': 'Client #1', 'Round': 3, 'Results_raw': {'train_loss': 94.079445, 'train_correct': 211.0, 'train_f1': 0.316105, 'train_total': 234, 'train_avg_loss': 0.402049, 'train_acc': 0.901709}}
2023-02-24 21:44:48,423 (client:306) INFO: {'Role': 'Client #3', 'Round': 3, 'Results_raw': {'train_loss': 57.930064, 'train_correct': 167.0, 'train_f1': 0.315392, 'train_total': 186, 'train_avg_loss': 0.311452, 'train_acc': 0.897849}}
2023-02-24 21:44:48,764 (client:306) INFO: {'Role': 'Client #7', 'Round': 3, 'Results_raw': {'train_loss': 27.90077, 'train_correct': 24.0, 'train_f1': 0.365618, 'train_total': 36, 'train_avg_loss': 0.775021, 'train_acc': 0.666667}}
2023-02-24 21:44:49,572 (client:306) INFO: {'Role': 'Client #6', 'Round': 3, 'Results_raw': {'train_loss': 76.523549, 'train_correct': 45.0, 'train_f1': 0.36361, 'train_total': 83, 'train_avg_loss': 0.92197, 'train_acc': 0.542169}}
2023-02-24 21:44:49,590 (monitor:541) INFO: {'Role': 'Server #', 'Round': 3, 'Results_model_metric': {}}
2023-02-24 21:44:49,592 (server:330) INFO: Server: Starting evaluation at the end of round 3.
2023-02-24 21:44:49,599 (server:336) INFO: ----------- Starting a new training round (Round #4) -------------
2023-02-24 21:44:49,843 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:49,932 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:50,124 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:50,345 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:50,432 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:50,708 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:50,849 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:51,096 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:51,256 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:51,815 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:44:51,816 (server:590) INFO: {'Role': 'Server #', 'Round': 3, 'Results_weighted_avg': {'test_loss': 27.318988, 'test_correct': 14.2, 'test_f1': 0.434051, 'test_total': 22.0, 'test_avg_loss': 1.031696, 'test_acc': 0.645455}, 'Results_avg': {'test_loss': 22.697308, 'test_correct': 14.2, 'test_f1': 0.435277, 'test_total': 22.0, 'test_avg_loss': 1.072473, 'test_acc': 0.625229}, 'Results_fairness': {'test_correct': 14.2, 'test_total': 22.0, 'test_loss_std': 10.342946, 'test_loss_bottom_decile': 14.480553, 'test_loss_top_decile': 46.276587, 'test_loss_min': 10.772303, 'test_loss_max': 46.276587, 'test_loss_bottom10%': 10.772303, 'test_loss_top10%': 46.276587, 'test_loss_cos1': 0.909974, 'test_loss_entropy': 2.208854, 'test_f1_std': 0.220847, 'test_f1_bottom_decile': 0.243902, 'test_f1_top_decile': 0.75, 'test_f1_min': 0.0, 'test_f1_max': 0.75, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.75, 'test_f1_cos1': 0.891782, 'test_f1_entropy': 2.130383, 'test_avg_loss_std': 0.265101, 'test_avg_loss_bottom_decile': 0.872454, 'test_avg_loss_top_decile': 1.809264, 'test_avg_loss_min': 0.826708, 'test_avg_loss_max': 1.809264, 'test_avg_loss_bottom10%': 0.826708, 'test_avg_loss_top10%': 1.809264, 'test_avg_loss_cos1': 0.970782, 'test_avg_loss_entropy': 2.275651, 'test_acc_std': 0.247789, 'test_acc_bottom_decile': 0.555556, 'test_acc_top_decile': 0.95, 'test_acc_min': 0.0, 'test_acc_max': 0.95, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.95, 'test_acc_cos1': 0.929653, 'test_acc_entropy': 2.177341}}
2023-02-24 21:44:52,334 (client:306) INFO: {'Role': 'Client #7', 'Round': 4, 'Results_raw': {'train_loss': 24.891068, 'train_correct': 23.0, 'train_f1': 0.278788, 'train_total': 36, 'train_avg_loss': 0.691419, 'train_acc': 0.638889}}
2023-02-24 21:44:54,334 (client:306) INFO: {'Role': 'Client #9', 'Round': 4, 'Results_raw': {'train_loss': 69.461458, 'train_correct': 106.0, 'train_f1': 0.72291, 'train_total': 135, 'train_avg_loss': 0.514529, 'train_acc': 0.785185}}
2023-02-24 21:44:56,821 (client:306) INFO: {'Role': 'Client #3', 'Round': 4, 'Results_raw': {'train_loss': 47.116916, 'train_correct': 174.0, 'train_f1': 0.322222, 'train_total': 186, 'train_avg_loss': 0.253317, 'train_acc': 0.935484}}
2023-02-24 21:44:57,244 (client:306) INFO: {'Role': 'Client #2', 'Round': 4, 'Results_raw': {'train_loss': 25.810436, 'train_correct': 15.0, 'train_f1': 0.478983, 'train_total': 25, 'train_avg_loss': 1.032417, 'train_acc': 0.6}}
2023-02-24 21:44:58,290 (client:306) INFO: {'Role': 'Client #5', 'Round': 4, 'Results_raw': {'train_loss': 54.285592, 'train_correct': 87.0, 'train_f1': 0.562193, 'train_total': 107, 'train_avg_loss': 0.507342, 'train_acc': 0.813084}}
2023-02-24 21:44:59,208 (client:306) INFO: {'Role': 'Client #6', 'Round': 4, 'Results_raw': {'train_loss': 82.999499, 'train_correct': 34.0, 'train_f1': 0.281481, 'train_total': 83, 'train_avg_loss': 0.999994, 'train_acc': 0.409639}}
2023-02-24 21:45:01,830 (client:306) INFO: {'Role': 'Client #1', 'Round': 4, 'Results_raw': {'train_loss': 89.374439, 'train_correct': 209.0, 'train_f1': 0.315234, 'train_total': 234, 'train_avg_loss': 0.381942, 'train_acc': 0.893162}}
2023-02-24 21:45:01,911 (client:306) INFO: {'Role': 'Client #4', 'Round': 4, 'Results_raw': {'train_loss': 3.93506, 'train_correct': 1.0, 'train_f1': 0.166667, 'train_total': 3, 'train_avg_loss': 1.311687, 'train_acc': 0.333333}}
2023-02-24 21:45:02,539 (client:306) INFO: {'Role': 'Client #10', 'Round': 4, 'Results_raw': {'train_loss': 33.568941, 'train_correct': 43.0, 'train_f1': 0.497778, 'train_total': 56, 'train_avg_loss': 0.599445, 'train_acc': 0.767857}}
2023-02-24 21:45:02,792 (client:306) INFO: {'Role': 'Client #8', 'Round': 4, 'Results_raw': {'train_loss': 20.658857, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 12, 'train_avg_loss': 1.721571, 'train_acc': 0.0}}
2023-02-24 21:45:02,813 (monitor:541) INFO: {'Role': 'Server #', 'Round': 4, 'Results_model_metric': {}}
2023-02-24 21:45:02,815 (server:330) INFO: Server: Starting evaluation at the end of round 4.
2023-02-24 21:45:02,817 (server:336) INFO: ----------- Starting a new training round (Round #5) -------------
2023-02-24 21:45:03,172 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:03,340 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:03,514 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:03,771 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:03,877 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:04,152 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:04,297 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:04,598 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:04,731 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:05,255 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:05,256 (server:590) INFO: {'Role': 'Server #', 'Round': 4, 'Results_weighted_avg': {'test_loss': 20.662412, 'test_correct': 15.8, 'test_f1': 0.450976, 'test_total': 22.0, 'test_avg_loss': 0.811763, 'test_acc': 0.718182}, 'Results_avg': {'test_loss': 17.858796, 'test_correct': 15.8, 'test_f1': 0.447761, 'test_total': 22.0, 'test_avg_loss': 0.891998, 'test_acc': 0.677413}, 'Results_fairness': {'test_correct': 15.8, 'test_total': 22.0, 'test_loss_std': 7.076414, 'test_loss_bottom_decile': 9.891513, 'test_loss_top_decile': 32.960909, 'test_loss_min': 9.431881, 'test_loss_max': 32.960909, 'test_loss_bottom10%': 9.431881, 'test_loss_top10%': 32.960909, 'test_loss_cos1': 0.929676, 'test_loss_entropy': 2.228382, 'test_f1_std': 0.222865, 'test_f1_bottom_decile': 0.25, 'test_f1_top_decile': 0.866667, 'test_f1_min': 0.083333, 'test_f1_max': 0.866667, 'test_f1_bottom10%': 0.083333, 'test_f1_top10%': 0.866667, 'test_f1_cos1': 0.895238, 'test_f1_entropy': 2.172435, 'test_avg_loss_std': 0.383404, 'test_avg_loss_bottom_decile': 0.63107, 'test_avg_loss_top_decile': 1.982689, 'test_avg_loss_min': 0.589493, 'test_avg_loss_max': 1.982689, 'test_avg_loss_bottom10%': 0.589493, 'test_avg_loss_top10%': 1.982689, 'test_avg_loss_cos1': 0.918727, 'test_avg_loss_entropy': 2.227969, 'test_acc_std': 0.210128, 'test_acc_bottom_decile': 0.5625, 'test_acc_top_decile': 0.875, 'test_acc_min': 0.1, 'test_acc_max': 0.875, 'test_acc_bottom10%': 0.1, 'test_acc_top10%': 0.875, 'test_acc_cos1': 0.955105, 'test_acc_entropy': 2.234456}}
2023-02-24 21:45:06,457 (client:306) INFO: {'Role': 'Client #5', 'Round': 5, 'Results_raw': {'train_loss': 51.100592, 'train_correct': 89.0, 'train_f1': 0.605974, 'train_total': 107, 'train_avg_loss': 0.477576, 'train_acc': 0.831776}}
2023-02-24 21:45:06,559 (client:306) INFO: {'Role': 'Client #4', 'Round': 5, 'Results_raw': {'train_loss': 5.185431, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 3, 'train_avg_loss': 1.728477, 'train_acc': 0.0}}
2023-02-24 21:45:07,609 (client:306) INFO: {'Role': 'Client #6', 'Round': 5, 'Results_raw': {'train_loss': 79.680236, 'train_correct': 46.0, 'train_f1': 0.385498, 'train_total': 83, 'train_avg_loss': 0.960003, 'train_acc': 0.554217}}
2023-02-24 21:45:08,385 (client:306) INFO: {'Role': 'Client #10', 'Round': 5, 'Results_raw': {'train_loss': 31.835173, 'train_correct': 46.0, 'train_f1': 0.540541, 'train_total': 56, 'train_avg_loss': 0.568485, 'train_acc': 0.821429}}
2023-02-24 21:45:08,762 (client:306) INFO: {'Role': 'Client #2', 'Round': 5, 'Results_raw': {'train_loss': 25.693944, 'train_correct': 13.0, 'train_f1': 0.388889, 'train_total': 25, 'train_avg_loss': 1.027758, 'train_acc': 0.52}}
2023-02-24 21:45:10,219 (client:306) INFO: {'Role': 'Client #9', 'Round': 5, 'Results_raw': {'train_loss': 68.003782, 'train_correct': 108.0, 'train_f1': 0.754727, 'train_total': 135, 'train_avg_loss': 0.503732, 'train_acc': 0.8}}
2023-02-24 21:45:10,359 (client:306) INFO: {'Role': 'Client #8', 'Round': 5, 'Results_raw': {'train_loss': 23.592535, 'train_correct': 1.0, 'train_f1': 0.166667, 'train_total': 12, 'train_avg_loss': 1.966045, 'train_acc': 0.083333}}
2023-02-24 21:45:12,818 (client:306) INFO: {'Role': 'Client #1', 'Round': 5, 'Results_raw': {'train_loss': 102.243017, 'train_correct': 203.0, 'train_f1': 0.309687, 'train_total': 234, 'train_avg_loss': 0.436936, 'train_acc': 0.867521}}
2023-02-24 21:45:14,662 (client:306) INFO: {'Role': 'Client #3', 'Round': 5, 'Results_raw': {'train_loss': 46.123462, 'train_correct': 172.0, 'train_f1': 0.320298, 'train_total': 186, 'train_avg_loss': 0.247976, 'train_acc': 0.924731}}
2023-02-24 21:45:15,226 (client:306) INFO: {'Role': 'Client #7', 'Round': 5, 'Results_raw': {'train_loss': 20.191087, 'train_correct': 29.0, 'train_f1': 0.412186, 'train_total': 36, 'train_avg_loss': 0.560864, 'train_acc': 0.805556}}
2023-02-24 21:45:15,246 (monitor:541) INFO: {'Role': 'Server #', 'Round': 5, 'Results_model_metric': {}}
2023-02-24 21:45:15,261 (server:330) INFO: Server: Starting evaluation at the end of round 5.
2023-02-24 21:45:15,264 (server:336) INFO: ----------- Starting a new training round (Round #6) -------------
2023-02-24 21:45:15,491 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:15,638 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:15,878 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:16,088 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:16,176 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:16,585 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:16,757 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:17,032 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:17,251 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:17,666 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:17,667 (server:590) INFO: {'Role': 'Server #', 'Round': 5, 'Results_weighted_avg': {'test_loss': 18.840802, 'test_correct': 16.2, 'test_f1': 0.462749, 'test_total': 22.0, 'test_avg_loss': 0.754799, 'test_acc': 0.736364}, 'Results_avg': {'test_loss': 16.605574, 'test_correct': 16.2, 'test_f1': 0.455621, 'test_total': 22.0, 'test_avg_loss': 0.861531, 'test_acc': 0.694859}, 'Results_fairness': {'test_correct': 16.2, 'test_total': 22.0, 'test_loss_std': 7.294505, 'test_loss_bottom_decile': 9.659392, 'test_loss_top_decile': 29.429923, 'test_loss_min': 7.59887, 'test_loss_max': 29.429923, 'test_loss_bottom10%': 7.59887, 'test_loss_top10%': 29.429923, 'test_loss_cos1': 0.915558, 'test_loss_entropy': 2.209196, 'test_f1_std': 0.233397, 'test_f1_bottom_decile': 0.25, 'test_f1_top_decile': 0.873016, 'test_f1_min': 0.095238, 'test_f1_max': 0.873016, 'test_f1_bottom10%': 0.095238, 'test_f1_top10%': 0.873016, 'test_f1_cos1': 0.89002, 'test_f1_entropy': 2.167544, 'test_avg_loss_std': 0.533379, 'test_avg_loss_bottom_decile': 0.536357, 'test_avg_loss_top_decile': 2.376686, 'test_avg_loss_min': 0.474929, 'test_avg_loss_max': 2.376686, 'test_avg_loss_bottom10%': 0.474929, 'test_avg_loss_top10%': 2.376686, 'test_avg_loss_cos1': 0.850243, 'test_avg_loss_entropy': 2.156308, 'test_acc_std': 0.22657, 'test_acc_bottom_decile': 0.5625, 'test_acc_top_decile': 0.9, 'test_acc_min': 0.1, 'test_acc_max': 0.9, 'test_acc_bottom10%': 0.1, 'test_acc_top10%': 0.9, 'test_acc_cos1': 0.950736, 'test_acc_entropy': 2.229195}}
2023-02-24 21:45:17,727 (client:306) INFO: {'Role': 'Client #4', 'Round': 6, 'Results_raw': {'train_loss': 5.273345, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 3, 'train_avg_loss': 1.757782, 'train_acc': 0.0}}
2023-02-24 21:45:18,174 (client:306) INFO: {'Role': 'Client #2', 'Round': 6, 'Results_raw': {'train_loss': 26.100363, 'train_correct': 14.0, 'train_f1': 0.424691, 'train_total': 25, 'train_avg_loss': 1.044015, 'train_acc': 0.56}}
2023-02-24 21:45:19,074 (client:306) INFO: {'Role': 'Client #10', 'Round': 6, 'Results_raw': {'train_loss': 32.498724, 'train_correct': 43.0, 'train_f1': 0.505634, 'train_total': 56, 'train_avg_loss': 0.580334, 'train_acc': 0.767857}}
2023-02-24 21:45:20,782 (client:306) INFO: {'Role': 'Client #9', 'Round': 6, 'Results_raw': {'train_loss': 68.212856, 'train_correct': 99.0, 'train_f1': 0.639037, 'train_total': 135, 'train_avg_loss': 0.50528, 'train_acc': 0.733333}}
2023-02-24 21:45:21,771 (client:306) INFO: {'Role': 'Client #6', 'Round': 6, 'Results_raw': {'train_loss': 72.067613, 'train_correct': 47.0, 'train_f1': 0.385664, 'train_total': 83, 'train_avg_loss': 0.868284, 'train_acc': 0.566265}}
2023-02-24 21:45:22,162 (client:306) INFO: {'Role': 'Client #7', 'Round': 6, 'Results_raw': {'train_loss': 20.195792, 'train_correct': 28.0, 'train_f1': 0.455556, 'train_total': 36, 'train_avg_loss': 0.560994, 'train_acc': 0.777778}}
2023-02-24 21:45:24,165 (client:306) INFO: {'Role': 'Client #3', 'Round': 6, 'Results_raw': {'train_loss': 41.229115, 'train_correct': 176.0, 'train_f1': 0.324125, 'train_total': 186, 'train_avg_loss': 0.221662, 'train_acc': 0.946237}}
2023-02-24 21:45:26,629 (client:306) INFO: {'Role': 'Client #1', 'Round': 6, 'Results_raw': {'train_loss': 95.499974, 'train_correct': 205.0, 'train_f1': 0.312024, 'train_total': 234, 'train_avg_loss': 0.40812, 'train_acc': 0.876068}}
2023-02-24 21:45:26,786 (client:306) INFO: {'Role': 'Client #8', 'Round': 6, 'Results_raw': {'train_loss': 27.664081, 'train_correct': 1.0, 'train_f1': 0.222222, 'train_total': 12, 'train_avg_loss': 2.30534, 'train_acc': 0.083333}}
2023-02-24 21:45:28,073 (client:306) INFO: {'Role': 'Client #5', 'Round': 6, 'Results_raw': {'train_loss': 46.201153, 'train_correct': 90.0, 'train_f1': 0.639731, 'train_total': 107, 'train_avg_loss': 0.431786, 'train_acc': 0.841121}}
2023-02-24 21:45:28,085 (monitor:541) INFO: {'Role': 'Server #', 'Round': 6, 'Results_model_metric': {}}
2023-02-24 21:45:28,089 (server:330) INFO: Server: Starting evaluation at the end of round 6.
2023-02-24 21:45:28,092 (server:336) INFO: ----------- Starting a new training round (Round #7) -------------
2023-02-24 21:45:28,374 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:28,485 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:28,820 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:29,057 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:29,156 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:29,502 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:29,658 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:29,925 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:30,056 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:30,510 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:30,511 (server:590) INFO: {'Role': 'Server #', 'Round': 6, 'Results_weighted_avg': {'test_loss': 20.651428, 'test_correct': 14.3, 'test_f1': 0.487071, 'test_total': 22.0, 'test_avg_loss': 0.792886, 'test_acc': 0.65}, 'Results_avg': {'test_loss': 17.443492, 'test_correct': 14.3, 'test_f1': 0.494359, 'test_total': 22.0, 'test_avg_loss': 0.865979, 'test_acc': 0.633561}, 'Results_fairness': {'test_correct': 14.3, 'test_total': 22.0, 'test_loss_std': 9.454791, 'test_loss_bottom_decile': 8.301317, 'test_loss_top_decile': 34.80969, 'test_loss_min': 7.093552, 'test_loss_max': 34.80969, 'test_loss_bottom10%': 7.093552, 'test_loss_top10%': 34.80969, 'test_loss_cos1': 0.87916, 'test_loss_entropy': 2.159927, 'test_f1_std': 0.288943, 'test_f1_bottom_decile': 0.248062, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.863348, 'test_f1_entropy': 2.093963, 'test_avg_loss_std': 0.508931, 'test_avg_loss_bottom_decile': 0.406349, 'test_avg_loss_top_decile': 2.247273, 'test_avg_loss_min': 0.354678, 'test_avg_loss_max': 2.247273, 'test_avg_loss_bottom10%': 0.354678, 'test_avg_loss_top10%': 2.247273, 'test_avg_loss_cos1': 0.862138, 'test_avg_loss_entropy': 2.159422, 'test_acc_std': 0.2614, 'test_acc_bottom_decile': 0.5, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.92441, 'test_acc_entropy': 2.171678}}
2023-02-24 21:45:30,562 (client:306) INFO: {'Role': 'Client #4', 'Round': 7, 'Results_raw': {'train_loss': 4.688084, 'train_correct': 1.0, 'train_f1': 0.166667, 'train_total': 3, 'train_avg_loss': 1.562695, 'train_acc': 0.333333}}
2023-02-24 21:45:32,970 (client:306) INFO: {'Role': 'Client #3', 'Round': 7, 'Results_raw': {'train_loss': 47.76241, 'train_correct': 169.0, 'train_f1': 0.317371, 'train_total': 186, 'train_avg_loss': 0.256787, 'train_acc': 0.908602}}
2023-02-24 21:45:33,454 (client:306) INFO: {'Role': 'Client #2', 'Round': 7, 'Results_raw': {'train_loss': 24.568416, 'train_correct': 14.0, 'train_f1': 0.428571, 'train_total': 25, 'train_avg_loss': 0.982737, 'train_acc': 0.56}}
2023-02-24 21:45:34,992 (client:306) INFO: {'Role': 'Client #9', 'Round': 7, 'Results_raw': {'train_loss': 63.934536, 'train_correct': 105.0, 'train_f1': 0.733553, 'train_total': 135, 'train_avg_loss': 0.473589, 'train_acc': 0.777778}}
2023-02-24 21:45:35,168 (client:306) INFO: {'Role': 'Client #8', 'Round': 7, 'Results_raw': {'train_loss': 26.909549, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 12, 'train_avg_loss': 2.242462, 'train_acc': 0.0}}
2023-02-24 21:45:35,822 (client:306) INFO: {'Role': 'Client #10', 'Round': 7, 'Results_raw': {'train_loss': 29.854175, 'train_correct': 44.0, 'train_f1': 0.518288, 'train_total': 56, 'train_avg_loss': 0.53311, 'train_acc': 0.785714}}
2023-02-24 21:45:36,244 (client:306) INFO: {'Role': 'Client #7', 'Round': 7, 'Results_raw': {'train_loss': 25.96114, 'train_correct': 27.0, 'train_f1': 0.372222, 'train_total': 36, 'train_avg_loss': 0.721143, 'train_acc': 0.75}}
2023-02-24 21:45:37,286 (client:306) INFO: {'Role': 'Client #6', 'Round': 7, 'Results_raw': {'train_loss': 77.665419, 'train_correct': 42.0, 'train_f1': 0.341325, 'train_total': 83, 'train_avg_loss': 0.935728, 'train_acc': 0.506024}}
2023-02-24 21:45:40,148 (client:306) INFO: {'Role': 'Client #1', 'Round': 7, 'Results_raw': {'train_loss': 89.369921, 'train_correct': 211.0, 'train_f1': 0.316105, 'train_total': 234, 'train_avg_loss': 0.381923, 'train_acc': 0.901709}}
2023-02-24 21:45:41,135 (client:306) INFO: {'Role': 'Client #5', 'Round': 7, 'Results_raw': {'train_loss': 52.135345, 'train_correct': 85.0, 'train_f1': 0.615359, 'train_total': 107, 'train_avg_loss': 0.487246, 'train_acc': 0.794393}}
2023-02-24 21:45:41,148 (monitor:541) INFO: {'Role': 'Server #', 'Round': 7, 'Results_model_metric': {}}
2023-02-24 21:45:41,152 (server:330) INFO: Server: Starting evaluation at the end of round 7.
2023-02-24 21:45:41,155 (server:336) INFO: ----------- Starting a new training round (Round #8) -------------
2023-02-24 21:45:41,334 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:41,429 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:41,606 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:41,824 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:41,919 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:42,181 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:42,316 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:42,517 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:42,677 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:43,035 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:43,037 (server:590) INFO: {'Role': 'Server #', 'Round': 7, 'Results_weighted_avg': {'test_loss': 17.743321, 'test_correct': 15.8, 'test_f1': 0.459681, 'test_total': 22.0, 'test_avg_loss': 0.704769, 'test_acc': 0.718182}, 'Results_avg': {'test_loss': 15.504928, 'test_correct': 15.8, 'test_f1': 0.443667, 'test_total': 22.0, 'test_avg_loss': 0.797167, 'test_acc': 0.66874}, 'Results_fairness': {'test_correct': 15.8, 'test_total': 22.0, 'test_loss_std': 7.111996, 'test_loss_bottom_decile': 8.656505, 'test_loss_top_decile': 27.9271, 'test_loss_min': 7.056076, 'test_loss_max': 27.9271, 'test_loss_bottom10%': 7.056076, 'test_loss_top10%': 27.9271, 'test_loss_cos1': 0.908941, 'test_loss_entropy': 2.201098, 'test_f1_std': 0.247318, 'test_f1_bottom_decile': 0.25, 'test_f1_top_decile': 0.873016, 'test_f1_min': 0.0, 'test_f1_max': 0.873016, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.873016, 'test_f1_cos1': 0.873457, 'test_f1_entropy': 2.109295, 'test_avg_loss_std': 0.488464, 'test_avg_loss_bottom_decile': 0.479371, 'test_avg_loss_top_decile': 2.180715, 'test_avg_loss_min': 0.441005, 'test_avg_loss_max': 2.180715, 'test_avg_loss_bottom10%': 0.441005, 'test_avg_loss_top10%': 2.180715, 'test_avg_loss_cos1': 0.85266, 'test_avg_loss_entropy': 2.158325, 'test_acc_std': 0.243957, 'test_acc_bottom_decile': 0.5625, 'test_acc_top_decile': 0.875, 'test_acc_min': 0.0, 'test_acc_max': 0.875, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.875, 'test_acc_cos1': 0.939441, 'test_acc_entropy': 2.18706}}
2023-02-24 21:45:45,457 (client:306) INFO: {'Role': 'Client #1', 'Round': 8, 'Results_raw': {'train_loss': 85.307325, 'train_correct': 210.0, 'train_f1': 0.344219, 'train_total': 234, 'train_avg_loss': 0.364561, 'train_acc': 0.897436}}
2023-02-24 21:45:47,434 (client:306) INFO: {'Role': 'Client #9', 'Round': 8, 'Results_raw': {'train_loss': 56.98472, 'train_correct': 112.0, 'train_f1': 0.780239, 'train_total': 135, 'train_avg_loss': 0.422109, 'train_acc': 0.82963}}
2023-02-24 21:45:48,570 (client:306) INFO: {'Role': 'Client #6', 'Round': 8, 'Results_raw': {'train_loss': 74.019969, 'train_correct': 48.0, 'train_f1': 0.390858, 'train_total': 83, 'train_avg_loss': 0.891807, 'train_acc': 0.578313}}
2023-02-24 21:45:50,629 (client:306) INFO: {'Role': 'Client #3', 'Round': 8, 'Results_raw': {'train_loss': 40.015717, 'train_correct': 174.0, 'train_f1': 0.322222, 'train_total': 186, 'train_avg_loss': 0.215138, 'train_acc': 0.935484}}
2023-02-24 21:45:50,871 (client:306) INFO: {'Role': 'Client #2', 'Round': 8, 'Results_raw': {'train_loss': 25.650112, 'train_correct': 15.0, 'train_f1': 0.522407, 'train_total': 25, 'train_avg_loss': 1.026004, 'train_acc': 0.6}}
2023-02-24 21:45:51,508 (client:306) INFO: {'Role': 'Client #10', 'Round': 8, 'Results_raw': {'train_loss': 33.82593, 'train_correct': 42.0, 'train_f1': 0.497264, 'train_total': 56, 'train_avg_loss': 0.604034, 'train_acc': 0.75}}
2023-02-24 21:45:52,905 (client:306) INFO: {'Role': 'Client #5', 'Round': 8, 'Results_raw': {'train_loss': 52.759984, 'train_correct': 79.0, 'train_f1': 0.485577, 'train_total': 107, 'train_avg_loss': 0.493084, 'train_acc': 0.738318}}
2023-02-24 21:45:53,031 (client:306) INFO: {'Role': 'Client #8', 'Round': 8, 'Results_raw': {'train_loss': 26.336975, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 12, 'train_avg_loss': 2.194748, 'train_acc': 0.0}}
2023-02-24 21:45:53,086 (client:306) INFO: {'Role': 'Client #4', 'Round': 8, 'Results_raw': {'train_loss': 5.205057, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 3, 'train_avg_loss': 1.735019, 'train_acc': 0.0}}
2023-02-24 21:45:53,492 (client:306) INFO: {'Role': 'Client #7', 'Round': 8, 'Results_raw': {'train_loss': 19.930554, 'train_correct': 28.0, 'train_f1': 0.432184, 'train_total': 36, 'train_avg_loss': 0.553626, 'train_acc': 0.777778}}
2023-02-24 21:45:53,500 (monitor:541) INFO: {'Role': 'Server #', 'Round': 8, 'Results_model_metric': {}}
2023-02-24 21:45:53,503 (server:330) INFO: Server: Starting evaluation at the end of round 8.
2023-02-24 21:45:53,505 (server:336) INFO: ----------- Starting a new training round (Round #9) -------------
2023-02-24 21:45:53,751 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:53,841 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:54,029 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:54,320 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:54,414 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:54,747 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:54,875 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:55,081 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:55,213 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:55,663 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:45:55,664 (server:590) INFO: {'Role': 'Server #', 'Round': 8, 'Results_weighted_avg': {'test_loss': 17.312026, 'test_correct': 15.7, 'test_f1': 0.450253, 'test_total': 22.0, 'test_avg_loss': 0.678204, 'test_acc': 0.713636}, 'Results_avg': {'test_loss': 14.920492, 'test_correct': 15.7, 'test_f1': 0.437243, 'test_total': 22.0, 'test_avg_loss': 0.750025, 'test_acc': 0.667797}, 'Results_fairness': {'test_correct': 15.7, 'test_total': 22.0, 'test_loss_std': 6.647146, 'test_loss_bottom_decile': 7.87317, 'test_loss_top_decile': 27.92474, 'test_loss_min': 7.034091, 'test_loss_max': 27.92474, 'test_loss_bottom10%': 7.034091, 'test_loss_top10%': 27.92474, 'test_loss_cos1': 0.913452, 'test_loss_entropy': 2.207302, 'test_f1_std': 0.23432, 'test_f1_bottom_decile': 0.26087, 'test_f1_top_decile': 0.811765, 'test_f1_min': 0.0, 'test_f1_max': 0.811765, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.811765, 'test_f1_cos1': 0.881411, 'test_f1_entropy': 2.119526, 'test_avg_loss_std': 0.392143, 'test_avg_loss_bottom_decile': 0.458152, 'test_avg_loss_top_decile': 1.854098, 'test_avg_loss_min': 0.439631, 'test_avg_loss_max': 1.854098, 'test_avg_loss_bottom10%': 0.439631, 'test_avg_loss_top10%': 1.854098, 'test_avg_loss_cos1': 0.886184, 'test_avg_loss_entropy': 2.194048, 'test_acc_std': 0.242776, 'test_acc_bottom_decile': 0.5625, 'test_acc_top_decile': 0.9, 'test_acc_min': 0.0, 'test_acc_max': 0.9, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.9, 'test_acc_cos1': 0.93982, 'test_acc_entropy': 2.187516}}
2023-02-24 21:45:55,714 (client:306) INFO: {'Role': 'Client #4', 'Round': 9, 'Results_raw': {'train_loss': 5.185078, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 3, 'train_avg_loss': 1.728359, 'train_acc': 0.0}}
2023-02-24 21:45:57,764 (client:306) INFO: {'Role': 'Client #3', 'Round': 9, 'Results_raw': {'train_loss': 43.249256, 'train_correct': 171.0, 'train_f1': 0.319328, 'train_total': 186, 'train_avg_loss': 0.232523, 'train_acc': 0.919355}}
2023-02-24 21:46:00,395 (client:306) INFO: {'Role': 'Client #1', 'Round': 9, 'Results_raw': {'train_loss': 80.762076, 'train_correct': 212.0, 'train_f1': 0.34531, 'train_total': 234, 'train_avg_loss': 0.345137, 'train_acc': 0.905983}}
2023-02-24 21:46:00,549 (client:306) INFO: {'Role': 'Client #8', 'Round': 9, 'Results_raw': {'train_loss': 21.693897, 'train_correct': 0.0, 'train_f1': 0.0, 'train_total': 12, 'train_avg_loss': 1.807825, 'train_acc': 0.0}}
2023-02-24 21:46:01,367 (client:306) INFO: {'Role': 'Client #10', 'Round': 9, 'Results_raw': {'train_loss': 33.66764, 'train_correct': 43.0, 'train_f1': 0.511081, 'train_total': 56, 'train_avg_loss': 0.601208, 'train_acc': 0.767857}}
2023-02-24 21:46:01,917 (client:306) INFO: {'Role': 'Client #7', 'Round': 9, 'Results_raw': {'train_loss': 23.254649, 'train_correct': 26.0, 'train_f1': 0.35656, 'train_total': 36, 'train_avg_loss': 0.645962, 'train_acc': 0.722222}}
2023-02-24 21:46:03,070 (client:306) INFO: {'Role': 'Client #5', 'Round': 9, 'Results_raw': {'train_loss': 49.813976, 'train_correct': 90.0, 'train_f1': 0.639731, 'train_total': 107, 'train_avg_loss': 0.465551, 'train_acc': 0.841121}}
2023-02-24 21:46:04,859 (client:306) INFO: {'Role': 'Client #9', 'Round': 9, 'Results_raw': {'train_loss': 58.796992, 'train_correct': 110.0, 'train_f1': 0.746717, 'train_total': 135, 'train_avg_loss': 0.435533, 'train_acc': 0.814815}}
2023-02-24 21:46:05,233 (client:306) INFO: {'Role': 'Client #2', 'Round': 9, 'Results_raw': {'train_loss': 28.446162, 'train_correct': 14.0, 'train_f1': 0.472222, 'train_total': 25, 'train_avg_loss': 1.137846, 'train_acc': 0.56}}
2023-02-24 21:46:06,217 (client:306) INFO: {'Role': 'Client #6', 'Round': 9, 'Results_raw': {'train_loss': 76.011768, 'train_correct': 49.0, 'train_f1': 0.408085, 'train_total': 83, 'train_avg_loss': 0.915804, 'train_acc': 0.590361}}
2023-02-24 21:46:06,228 (monitor:541) INFO: {'Role': 'Server #', 'Round': 9, 'Results_model_metric': {}}
2023-02-24 21:46:06,230 (server:347) INFO: Server: Training is finished! Starting evaluation.
2023-02-24 21:46:06,415 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:06,506 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:06,684 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:06,901 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:07,034 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:07,363 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:07,580 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:07,797 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:07,931 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:08,305 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name
2023-02-24 21:46:08,307 (server:590) INFO: {'Role': 'Server #', 'Round': 9, 'Results_weighted_avg': {'test_loss': 16.710151, 'test_correct': 17.0, 'test_f1': 0.484574, 'test_total': 22.0, 'test_avg_loss': 0.650795, 'test_acc': 0.772727}, 'Results_avg': {'test_loss': 14.317494, 'test_correct': 17.0, 'test_f1': 0.480264, 'test_total': 22.0, 'test_avg_loss': 0.707598, 'test_acc': 0.727166}, 'Results_fairness': {'test_correct': 17.0, 'test_total': 22.0, 'test_loss_std': 5.752262, 'test_loss_bottom_decile': 8.085278, 'test_loss_top_decile': 27.761527, 'test_loss_min': 7.257524, 'test_loss_max': 27.761527, 'test_loss_bottom10%': 7.257524, 'test_loss_top10%': 27.761527, 'test_loss_cos1': 0.927911, 'test_loss_entropy': 2.227653, 'test_f1_std': 0.244028, 'test_f1_bottom_decile': 0.25, 'test_f1_top_decile': 0.935223, 'test_f1_min': 0.095238, 'test_f1_max': 0.935223, 'test_f1_bottom10%': 0.095238, 'test_f1_top10%': 0.935223, 'test_f1_cos1': 0.891516, 'test_f1_entropy': 2.167637, 'test_avg_loss_std': 0.271004, 'test_avg_loss_bottom_decile': 0.541893, 'test_avg_loss_top_decile': 1.469935, 'test_avg_loss_min': 0.453595, 'test_avg_loss_max': 1.469935, 'test_avg_loss_bottom10%': 0.453595, 'test_avg_loss_top10%': 1.469935, 'test_avg_loss_cos1': 0.933853, 'test_avg_loss_entropy': 2.241664, 'test_acc_std': 0.235543, 'test_acc_bottom_decile': 0.5625, 'test_acc_top_decile': 0.962963, 'test_acc_min': 0.1, 'test_acc_max': 0.962963, 'test_acc_bottom10%': 0.1, 'test_acc_top10%': 0.962963, 'test_acc_cos1': 0.951336, 'test_acc_entropy': 2.229114}}
2023-02-24 21:46:08,308 (server:395) INFO: Server: Final evaluation is finished! Starting merging results.
2023-02-24 21:46:08,308 (server:521) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'test_loss': 7.034091, 'test_correct': 33.0, 'test_f1': 0.811765, 'test_total': 10.0, 'test_avg_loss': 0.439631, 'test_acc': 0.9}, 'client_summarized_weighted_avg': {'test_loss': 16.710151, 'test_correct': 17.0, 'test_f1': 0.484574, 'test_total': 22.0, 'test_avg_loss': 0.650795, 'test_acc': 0.772727}, 'client_summarized_avg': {'test_loss': 14.317494, 'test_correct': 17.0, 'test_f1': 0.480264, 'test_total': 22.0, 'test_avg_loss': 0.707598, 'test_acc': 0.727166}, 'client_summarized_fairness': {'test_loss_entropy': 2.159927, 'test_loss_cos1': 0.87916, 'test_loss_top10%': 34.80969, 'test_loss_bottom10%': 7.093552, 'test_loss_max': 34.80969, 'test_loss_min': 7.093552, 'test_loss_top_decile': 34.80969, 'test_loss_bottom_decile': 8.301317, 'test_loss_std': 9.454791, 'test_correct': 14.3, 'test_total': 22.0, 'test_f1_std': 0.288943, 'test_f1_bottom_decile': 0.248062, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.863348, 'test_f1_entropy': 2.093963, 'test_avg_loss_std': 0.508931, 'test_avg_loss_bottom_decile': 0.406349, 'test_avg_loss_top_decile': 2.247273, 'test_avg_loss_min': 0.354678, 'test_avg_loss_max': 2.247273, 'test_avg_loss_bottom10%': 0.354678, 'test_avg_loss_top10%': 2.247273, 'test_avg_loss_cos1': 0.862138, 'test_avg_loss_entropy': 2.159422, 'test_acc_std': 0.2614, 'test_acc_bottom_decile': 0.5, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.92441, 'test_acc_entropy': 2.171678}}}
2023-02-24 21:46:08,309 (server:540) INFO: {'Role': 'Client #1', 'Round': 10, 'Results_raw': {'test_loss': 12.506143, 'test_correct': 16.0, 'test_f1': 0.444444, 'test_total': 20, 'test_avg_loss': 0.625307, 'test_acc': 0.8}}
2023-02-24 21:46:08,310 (server:540) INFO: {'Role': 'Client #2', 'Round': 10, 'Results_raw': {'test_loss': 8.085278, 'test_correct': 7.0, 'test_f1': 0.5, 'test_total': 10, 'test_avg_loss': 0.808528, 'test_acc': 0.7}}
2023-02-24 21:46:08,310 (server:540) INFO: {'Role': 'Client #3', 'Round': 10, 'Results_raw': {'test_loss': 10.837862, 'test_correct': 17.0, 'test_f1': 0.811912, 'test_total': 20, 'test_avg_loss': 0.541893, 'test_acc': 0.85}}
2023-02-24 21:46:08,310 (server:540) INFO: {'Role': 'Client #4', 'Round': 10, 'Results_raw': {'test_loss': 14.753239, 'test_correct': 26.0, 'test_f1': 0.333333, 'test_total': 27, 'test_avg_loss': 0.546416, 'test_acc': 0.962963}}
2023-02-24 21:46:08,310 (server:540) INFO: {'Role': 'Client #5', 'Round': 10, 'Results_raw': {'test_loss': 14.699352, 'test_correct': 1.0, 'test_f1': 0.095238, 'test_total': 10, 'test_avg_loss': 1.469935, 'test_acc': 0.1}}
2023-02-24 21:46:08,311 (server:540) INFO: {'Role': 'Client #6', 'Round': 10, 'Results_raw': {'test_loss': 20.487344, 'test_correct': 24.0, 'test_f1': 0.296296, 'test_total': 32, 'test_avg_loss': 0.640229, 'test_acc': 0.75}}
2023-02-24 21:46:08,311 (server:540) INFO: {'Role': 'Client #7', 'Round': 10, 'Results_raw': {'test_loss': 7.257524, 'test_correct': 15.0, 'test_f1': 0.935223, 'test_total': 16, 'test_avg_loss': 0.453595, 'test_acc': 0.9375}}
2023-02-24 21:46:08,311 (server:540) INFO: {'Role': 'Client #8', 'Round': 10, 'Results_raw': {'test_loss': 15.119568, 'test_correct': 19.0, 'test_f1': 0.616667, 'test_total': 23, 'test_avg_loss': 0.657373, 'test_acc': 0.826087}}
2023-02-24 21:46:08,311 (server:540) INFO: {'Role': 'Client #9', 'Round': 10, 'Results_raw': {'test_loss': 11.667101, 'test_correct': 9.0, 'test_f1': 0.25, 'test_total': 16, 'test_avg_loss': 0.729194, 'test_acc': 0.5625}}
2023-02-24 21:46:08,312 (server:540) INFO: {'Role': 'Client #10', 'Round': 10, 'Results_raw': {'test_loss': 27.761527, 'test_correct': 36.0, 'test_f1': 0.519529, 'test_total': 46, 'test_avg_loss': 0.603511, 'test_acc': 0.782609}}
2023-02-24 21:46:08,312 (monitor:172) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 2.218085, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 2488000, 'total_download_bytes': 1161640, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,313 (client:513) INFO: ================= client 1 received finish message =================
2023-02-24 21:46:08,315 (monitor:172) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 2.215255, 'total_model_size': 3260030, 'total_flops': 621125556480.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,316 (client:513) INFO: ================= client 2 received finish message =================
2023-02-24 21:46:08,318 (monitor:172) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 2.212803, 'total_model_size': 3260030, 'total_flops': 66359568000.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,318 (client:513) INFO: ================= client 3 received finish message =================
2023-02-24 21:46:08,321 (monitor:172) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 2.208861, 'total_model_size': 3260030, 'total_flops': 493715185920.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,321 (client:513) INFO: ================= client 4 received finish message =================
2023-02-24 21:46:08,323 (monitor:172) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 2.205086, 'total_model_size': 3260030, 'total_flops': 7963148160.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,324 (client:513) INFO: ================= client 5 received finish message =================
2023-02-24 21:46:08,326 (monitor:172) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 2.202077, 'total_model_size': 3260030, 'total_flops': 284018951040.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,327 (client:513) INFO: ================= client 6 received finish message =================
2023-02-24 21:46:08,329 (monitor:172) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 2.198851, 'total_model_size': 3260030, 'total_flops': 220313765760.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,329 (client:513) INFO: ================= client 7 received finish message =================
2023-02-24 21:46:08,332 (monitor:172) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 2.196182, 'total_model_size': 3260030, 'total_flops': 95557777920.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,332 (client:513) INFO: ================= client 8 received finish message =================
2023-02-24 21:46:08,334 (monitor:172) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 2.193349, 'total_model_size': 3260030, 'total_flops': 31852592640.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,335 (client:513) INFO: ================= client 9 received finish message =================
2023-02-24 21:46:08,338 (monitor:172) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 2.190649, 'total_model_size': 3260030, 'total_flops': 358341667200.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,338 (client:513) INFO: ================= client 10 received finish message =================
2023-02-24 21:46:08,340 (monitor:172) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 2.186603, 'total_model_size': 3260030, 'total_flops': 148645432320.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2023-02-24 21:46:08,341 (monitor:337) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one
2023-02-24 21:46:08,345 (monitor:245) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 2.202527, 'sys_avg/total_model_size': '2.83M', 'sys_avg/total_flops': '197.09G', 'sys_avg/total_upload_bytes': '324.09K', 'sys_avg/total_download_bytes': '103.13K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2023-02-24 21:46:08,345 (monitor:248) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.009951, 'sys_std/total_model_size': '915.23K', 'sys_std/total_flops': '184.4G', 'sys_std/total_upload_bytes': '665.85K', 'sys_std/total_download_bytes': '326.12K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
