 is at 192.168.0.1[0m
[38;5;39m2023-02-24 20:57:34,902 (logging:126) INFO: the current dir is /home/ubuntu/medscale[0m
[38;5;39m2023-02-24 20:57:34,902 (logging:127) INFO: the output dir is exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734[0m
[33;20m2023-02-24 20:57:44,913 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[33;20m2023-02-24 20:57:44,913 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:44,937 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:45,052 (utils:144) INFO: The device information file is not provided[0m
[38;5;39m2023-02-24 20:57:45,235 (fed_runner:169) INFO: Server has been set up ... [0m
[33;20m2023-02-24 20:57:45,395 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:45,442 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:45,453 (fed_runner:221) INFO: Client 1 has been set up ... [0m
[33;20m2023-02-24 20:57:45,588 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:45,620 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:45,637 (fed_runner:221) INFO: Client 2 has been set up ... [0m
[33;20m2023-02-24 20:57:45,752 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:45,800 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:45,821 (fed_runner:221) INFO: Client 3 has been set up ... [0m
[33;20m2023-02-24 20:57:46,084 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:46,131 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:46,152 (fed_runner:221) INFO: Client 4 has been set up ... [0m
[33;20m2023-02-24 20:57:46,200 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:46,223 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:46,232 (fed_runner:221) INFO: Client 5 has been set up ... [0m
[33;20m2023-02-24 20:57:46,371 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:46,395 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:46,403 (fed_runner:221) INFO: Client 6 has been set up ... [0m
[33;20m2023-02-24 20:57:46,464 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:46,487 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:46,496 (fed_runner:221) INFO: Client 7 has been set up ... [0m
[33;20m2023-02-24 20:57:46,650 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:46,673 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:46,684 (fed_runner:221) INFO: Client 8 has been set up ... [0m
[33;20m2023-02-24 20:57:46,926 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:46,980 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:46,992 (fed_runner:221) INFO: Client 9 has been set up ... [0m
[33;20m2023-02-24 20:57:47,192 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 20:57:47,223 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.3}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224205734
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 20:57:47,237 (fed_runner:221) INFO: Client 10 has been set up ... [0m
<<<<<<< HEAD
[38;5;39m2023-02-24 20:57:47,237 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.[0m
=======
<<<<<<< HEAD
[38;5;39m2023-02-24 20:57:47,237 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.[0m
=======
[38;5;39m2023-02-24 20:57:47,237 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.[0m
>>>>>>> fe4962455354c9c11afd9c9806ceda28eb280737
>>>>>>> 64b283ee525ef53c32509882719e74890329b83f
[38;5;39m2023-02-24 20:57:47,238 (trainer:349) INFO: Num of original para names: 39.[0m
[38;5;39m2023-02-24 20:57:47,238 (trainer:350) INFO: Num of original trainable para names: 24.[0m
[38;5;39m2023-02-24 20:57:47,238 (trainer:352) INFO: Num of preserved para names in local update: 39. 
Preserved para names in local update: {'bn3.num_batches_tracked', 'conv3.weight', 'bn5.running_mean', 'bn4.weight', 'conv3.bias', 'bn2.num_batches_tracked', 'bn4.running_var', 'fc1.weight', 'conv5.weight', 'conv1.bias', 'bn1.bias', 'bn5.running_var', 'conv2.bias', 'conv1.weight', 'bn4.bias', 'conv5.bias', 'bn1.weight', 'bn2.running_mean', 'fc1.bias', 'conv4.weight', 'bn5.num_batches_tracked', 'bn3.bias', 'bn4.running_mean', 'bn2.bias', 'bn1.num_batches_tracked', 'bn3.running_mean', 'bn3.weight', 'conv2.weight', 'bn5.bias', 'conv4.bias', 'bn1.running_mean', 'fc2.weight', 'fc2.bias', 'bn4.num_batches_tracked', 'bn2.running_var', 'bn2.weight', 'bn1.running_var', 'bn5.weight', 'bn3.running_var'}.[0m
[38;5;39m2023-02-24 20:57:47,238 (trainer:356) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().[0m
[38;5;39m2023-02-24 20:57:47,238 (trainer:361) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	}[0m
[38;5;39m2023-02-24 20:57:47,254 (server:804) INFO: ----------- Starting training (Round #0) -------------[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:57:51,808 (client:306) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 4.726425, 'train_acc': 0.0, 'train_loss': 28.358551, 'train_total': 6, 'train_f1': 0.0}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:57:54,073 (client:306) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_correct': 194.0, 'train_avg_loss': 1.141981, 'train_acc': 0.869955, 'train_loss': 254.661775, 'train_total': 223, 'train_f1': 0.186091}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:57:56,185 (client:306) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_correct': 86.0, 'train_avg_loss': 2.079872, 'train_acc': 0.581081, 'train_loss': 307.821103, 'train_total': 148, 'train_f1': 0.156982}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:57:58,290 (client:306) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_correct': 126.0, 'train_avg_loss': 1.590552, 'train_acc': 0.724138, 'train_loss': 276.756054, 'train_total': 174, 'train_f1': 0.142857}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:57:58,427 (client:306) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 4.616558, 'train_acc': 0.0, 'train_loss': 32.315907, 'train_total': 7, 'train_f1': 0.0}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:57:58,777 (client:306) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_correct': 5.0, 'train_avg_loss': 4.235815, 'train_acc': 0.16129, 'train_loss': 131.31028, 'train_total': 31, 'train_f1': 0.050505}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:57:59,941 (client:306) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_correct': 77.0, 'train_avg_loss': 1.951915, 'train_acc': 0.785714, 'train_loss': 191.287658, 'train_total': 98, 'train_f1': 0.22}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:58:00,167 (client:306) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 4.630018, 'train_acc': 0.0, 'train_loss': 74.080284, 'train_total': 16, 'train_f1': 0.0}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:58:01,998 (client:306) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_correct': 82.0, 'train_avg_loss': 1.882868, 'train_acc': 0.49697, 'train_loss': 310.673218, 'train_total': 165, 'train_f1': 0.170718}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 20:58:02,183 (client:306) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 4.635056, 'train_acc': 0.0, 'train_loss': 41.7155, 'train_total': 9, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:58:02,208 (monitor:541) INFO: {'Role': 'Server #', 'Round': 0, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:58:02,210 (server:330) INFO: Server: Starting evaluation at the end of round 0.[0m
[38;5;39m2023-02-24 20:58:02,224 (server:336) INFO: ----------- Starting a new training round (Round #1) -------------[0m
[33;20m2023-02-24 20:58:02,804 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:03,043 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:03,112 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:03,348 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:03,371 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:03,639 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:03,696 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:04,055 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:04,209 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:04,380 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:58:04,382 (server:590) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'test_correct': 12.2, 'test_avg_loss': 3.615815, 'test_acc': 0.554545, 'test_loss': 110.867031, 'test_total': 22.0, 'test_f1': 0.333633}, 'Results_avg': {'test_correct': 12.2, 'test_avg_loss': 3.701411, 'test_acc': 0.464281, 'test_loss': 79.547937, 'test_total': 22.0, 'test_f1': 0.293998}, 'Results_fairness': {'test_correct': 12.2, 'test_total': 22.0, 'test_avg_loss_std': 0.200294, 'test_avg_loss_bottom_decile': 3.546271, 'test_avg_loss_top_decile': 4.088068, 'test_avg_loss_min': 3.542077, 'test_avg_loss_max': 4.088068, 'test_avg_loss_bottom10%': 3.542077, 'test_avg_loss_top10%': 4.088068, 'test_avg_loss_cos1': 0.998539, 'test_avg_loss_entropy': 2.301152, 'test_acc_std': 0.387263, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.767926, 'test_acc_entropy': 1.854699, 'test_loss_std': 49.509677, 'test_loss_bottom_decile': 20.440338, 'test_loss_top_decile': 164.515259, 'test_loss_min': 4.081591, 'test_loss_max': 164.515259, 'test_loss_bottom10%': 4.081591, 'test_loss_top10%': 164.515259, 'test_loss_cos1': 0.848993, 'test_loss_entropy': 2.069611, 'test_f1_std': 0.289968, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.71197, 'test_f1_entropy': 1.799419}}[0m
[38;5;39m2023-02-24 20:58:04,458 (client:306) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 4.542425, 'train_acc': 0.0, 'train_loss': 31.796976, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:58:04,728 (client:306) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'train_correct': 16.0, 'train_avg_loss': 1.175497, 'train_acc': 0.516129, 'train_loss': 36.440393, 'train_total': 31, 'train_f1': 0.296296}}[0m
[38;5;39m2023-02-24 20:58:06,185 (client:306) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'train_correct': 100.0, 'train_avg_loss': 0.73165, 'train_acc': 0.675676, 'train_loss': 108.284264, 'train_total': 148, 'train_f1': 0.385051}}[0m
[38;5;39m2023-02-24 20:58:07,637 (client:306) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'train_correct': 95.0, 'train_avg_loss': 1.066183, 'train_acc': 0.575758, 'train_loss': 175.920243, 'train_total': 165, 'train_f1': 0.541667}}[0m
[38;5;39m2023-02-24 20:58:07,812 (client:306) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 3.770317, 'train_acc': 0.0, 'train_loss': 60.325073, 'train_total': 16, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:58:09,627 (client:306) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'train_correct': 136.0, 'train_avg_loss': 0.544086, 'train_acc': 0.781609, 'train_loss': 94.670964, 'train_total': 174, 'train_f1': 0.505682}}[0m
[38;5;39m2023-02-24 20:58:11,943 (client:306) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'train_correct': 212.0, 'train_avg_loss': 0.279687, 'train_acc': 0.950673, 'train_loss': 62.370276, 'train_total': 223, 'train_f1': 0.487356}}[0m
[38;5;39m2023-02-24 20:58:12,050 (client:306) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'train_correct': 7.0, 'train_avg_loss': 1.470694, 'train_acc': 0.777778, 'train_loss': 13.236244, 'train_total': 9, 'train_f1': 0.4375}}[0m
[38;5;39m2023-02-24 20:58:13,131 (client:306) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'train_correct': 82.0, 'train_avg_loss': 0.35073, 'train_acc': 0.836735, 'train_loss': 34.37156, 'train_total': 98, 'train_f1': 0.455556}}[0m
[38;5;39m2023-02-24 20:58:13,220 (client:306) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 1.566887, 'train_acc': 0.0, 'train_loss': 9.401325, 'train_total': 6, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:58:13,247 (monitor:541) INFO: {'Role': 'Server #', 'Round': 1, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:58:13,249 (server:330) INFO: Server: Starting evaluation at the end of round 1.[0m
[38;5;39m2023-02-24 20:58:13,252 (server:336) INFO: ----------- Starting a new training round (Round #2) -------------[0m
[33;20m2023-02-24 20:58:13,685 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:14,087 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:14,140 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:14,416 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:14,467 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:14,824 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:14,882 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:15,185 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:15,342 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:15,530 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:58:15,531 (server:590) INFO: {'Role': 'Server #', 'Round': 1, 'Results_weighted_avg': {'test_correct': 15.9, 'test_avg_loss': 2.406515, 'test_acc': 0.722727, 'test_loss': 72.659674, 'test_total': 22.0, 'test_f1': 0.437828}, 'Results_avg': {'test_correct': 15.9, 'test_avg_loss': 2.643767, 'test_acc': 0.579098, 'test_loss': 52.943335, 'test_total': 22.0, 'test_f1': 0.343243}, 'Results_fairness': {'test_correct': 15.9, 'test_total': 22.0, 'test_avg_loss_std': 0.551778, 'test_avg_loss_bottom_decile': 2.238663, 'test_avg_loss_top_decile': 3.72247, 'test_avg_loss_min': 2.230316, 'test_avg_loss_max': 3.72247, 'test_avg_loss_bottom10%': 2.230316, 'test_avg_loss_top10%': 3.72247, 'test_avg_loss_cos1': 0.978907, 'test_avg_loss_entropy': 2.282317, 'test_acc_std': 0.323872, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.92, 'test_acc_min': 0.0, 'test_acc_max': 0.92, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.92, 'test_acc_cos1': 0.872777, 'test_acc_entropy': 2.05335, 'test_loss_std': 31.348362, 'test_loss_bottom_decile': 13.381897, 'test_loss_top_decile': 106.061378, 'test_loss_min': 3.678215, 'test_loss_max': 106.061378, 'test_loss_bottom10%': 3.678215, 'test_loss_top10%': 106.061378, 'test_loss_cos1': 0.860473, 'test_loss_entropy': 2.092538, 'test_f1_std': 0.206623, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.676198, 'test_f1_min': 0.0, 'test_f1_max': 0.676198, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.676198, 'test_f1_cos1': 0.856746, 'test_f1_entropy': 2.034402}}[0m
[38;5;39m2023-02-24 20:58:15,803 (client:306) INFO: {'Role': 'Client #7', 'Round': 2, 'Results_raw': {'train_correct': 1.0, 'train_avg_loss': 3.71725, 'train_acc': 0.0625, 'train_loss': 59.475998, 'train_total': 16, 'train_f1': 0.095238}}[0m
[38;5;39m2023-02-24 20:58:16,257 (client:306) INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'train_correct': 20.0, 'train_avg_loss': 0.839563, 'train_acc': 0.645161, 'train_loss': 26.026453, 'train_total': 31, 'train_f1': 0.357883}}[0m
[38;5;39m2023-02-24 20:58:18,216 (client:306) INFO: {'Role': 'Client #8', 'Round': 2, 'Results_raw': {'train_correct': 138.0, 'train_avg_loss': 0.435478, 'train_acc': 0.793103, 'train_loss': 75.773172, 'train_total': 174, 'train_f1': 0.549093}}[0m
[38;5;39m2023-02-24 20:58:19,941 (client:306) INFO: {'Role': 'Client #5', 'Round': 2, 'Results_raw': {'train_correct': 99.0, 'train_avg_loss': 0.984047, 'train_acc': 0.6, 'train_loss': 162.367723, 'train_total': 165, 'train_f1': 0.385799}}[0m
[38;5;39m2023-02-24 20:58:20,079 (client:306) INFO: {'Role': 'Client #6', 'Round': 2, 'Results_raw': {'train_correct': 7.0, 'train_avg_loss': 1.360816, 'train_acc': 0.777778, 'train_loss': 12.247347, 'train_total': 9, 'train_f1': 0.4375}}[0m
[38;5;39m2023-02-24 20:58:20,180 (client:306) INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 4.72335, 'train_acc': 0.0, 'train_loss': 33.06345, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:58:20,297 (client:306) INFO: {'Role': 'Client #9', 'Round': 2, 'Results_raw': {'train_correct': 1.0, 'train_avg_loss': 1.041491, 'train_acc': 0.166667, 'train_loss': 6.248948, 'train_total': 6, 'train_f1': 0.142857}}[0m
[38;5;39m2023-02-24 20:58:21,795 (client:306) INFO: {'Role': 'Client #10', 'Round': 2, 'Results_raw': {'train_correct': 112.0, 'train_avg_loss': 0.563227, 'train_acc': 0.756757, 'train_loss': 83.357654, 'train_total': 148, 'train_f1': 0.463432}}[0m
[38;5;39m2023-02-24 20:58:22,960 (client:306) INFO: {'Role': 'Client #4', 'Round': 2, 'Results_raw': {'train_correct': 92.0, 'train_avg_loss': 0.17131, 'train_acc': 0.938776, 'train_loss': 16.788338, 'train_total': 98, 'train_f1': 0.484211}}[0m
[38;5;39m2023-02-24 20:58:25,280 (client:306) INFO: {'Role': 'Client #3', 'Round': 2, 'Results_raw': {'train_correct': 212.0, 'train_avg_loss': 0.246217, 'train_acc': 0.950673, 'train_loss': 54.906312, 'train_total': 223, 'train_f1': 0.487356}}[0m
[38;5;39m2023-02-24 20:58:25,290 (monitor:541) INFO: {'Role': 'Server #', 'Round': 2, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:58:25,304 (server:330) INFO: Server: Starting evaluation at the end of round 2.[0m
[38;5;39m2023-02-24 20:58:25,306 (server:336) INFO: ----------- Starting a new training round (Round #3) -------------[0m
[33;20m2023-02-24 20:58:25,798 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:26,038 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:26,091 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:26,349 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:26,377 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:26,802 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:26,893 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:27,301 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:27,483 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:27,746 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:58:27,748 (server:590) INFO: {'Role': 'Server #', 'Round': 2, 'Results_weighted_avg': {'test_correct': 14.5, 'test_avg_loss': 1.27693, 'test_acc': 0.659091, 'test_loss': 36.965996, 'test_total': 22.0, 'test_f1': 0.426161}, 'Results_avg': {'test_correct': 14.5, 'test_avg_loss': 1.676115, 'test_acc': 0.515087, 'test_loss': 28.092466, 'test_total': 22.0, 'test_f1': 0.347492}, 'Results_fairness': {'test_correct': 14.5, 'test_total': 22.0, 'test_avg_loss_std': 0.903658, 'test_avg_loss_bottom_decile': 1.017844, 'test_avg_loss_top_decile': 3.436264, 'test_avg_loss_min': 0.946781, 'test_avg_loss_max': 3.436264, 'test_avg_loss_bottom10%': 0.946781, 'test_avg_loss_top10%': 3.436264, 'test_avg_loss_cos1': 0.880222, 'test_avg_loss_entropy': 2.175781, 'test_acc_std': 0.290801, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.9375, 'test_acc_min': 0.0, 'test_acc_max': 0.9375, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.9375, 'test_acc_cos1': 0.870805, 'test_acc_entropy': 2.052487, 'test_loss_std': 15.463291, 'test_loss_bottom_decile': 6.703263, 'test_loss_top_decile': 54.456807, 'test_loss_min': 3.436264, 'test_loss_max': 54.456807, 'test_loss_bottom10%': 3.436264, 'test_loss_top10%': 54.456807, 'test_loss_cos1': 0.876052, 'test_loss_entropy': 2.127207, 'test_f1_std': 0.22698, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.733333, 'test_f1_min': 0.0, 'test_f1_max': 0.733333, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.733333, 'test_f1_cos1': 0.837219, 'test_f1_entropy': 2.010319}}[0m
[38;5;39m2023-02-24 20:58:27,921 (client:306) INFO: {'Role': 'Client #7', 'Round': 3, 'Results_raw': {'train_correct': 3.0, 'train_avg_loss': 3.212403, 'train_acc': 0.1875, 'train_loss': 51.398441, 'train_total': 16, 'train_f1': 0.111111}}[0m
[38;5;39m2023-02-24 20:58:30,571 (client:306) INFO: {'Role': 'Client #3', 'Round': 3, 'Results_raw': {'train_correct': 208.0, 'train_avg_loss': 0.227291, 'train_acc': 0.932735, 'train_loss': 50.685925, 'train_total': 223, 'train_f1': 0.541341}}[0m
[38;5;39m2023-02-24 20:58:30,697 (client:306) INFO: {'Role': 'Client #9', 'Round': 3, 'Results_raw': {'train_correct': 4.0, 'train_avg_loss': 0.606012, 'train_acc': 0.666667, 'train_loss': 3.636073, 'train_total': 6, 'train_f1': 0.4}}[0m
[38;5;39m2023-02-24 20:58:31,181 (client:306) INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'train_correct': 27.0, 'train_avg_loss': 0.52407, 'train_acc': 0.870968, 'train_loss': 16.246177, 'train_total': 31, 'train_f1': 0.504942}}[0m
[38;5;39m2023-02-24 20:58:32,226 (client:306) INFO: {'Role': 'Client #4', 'Round': 3, 'Results_raw': {'train_correct': 94.0, 'train_avg_loss': 0.114394, 'train_acc': 0.959184, 'train_loss': 11.210648, 'train_total': 98, 'train_f1': 0.489583}}[0m
[38;5;39m2023-02-24 20:58:34,134 (client:306) INFO: {'Role': 'Client #8', 'Round': 3, 'Results_raw': {'train_correct': 151.0, 'train_avg_loss': 0.353532, 'train_acc': 0.867816, 'train_loss': 61.514631, 'train_total': 174, 'train_f1': 0.694853}}[0m
[38;5;39m2023-02-24 20:58:34,321 (client:306) INFO: {'Role': 'Client #6', 'Round': 3, 'Results_raw': {'train_correct': 4.0, 'train_avg_loss': 1.38859, 'train_acc': 0.444444, 'train_loss': 12.497308, 'train_total': 9, 'train_f1': 0.242424}}[0m
[38;5;39m2023-02-24 20:58:34,473 (client:306) INFO: {'Role': 'Client #1', 'Round': 3, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 4.243904, 'train_acc': 0.0, 'train_loss': 29.707329, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:58:36,393 (client:306) INFO: {'Role': 'Client #10', 'Round': 3, 'Results_raw': {'train_correct': 102.0, 'train_avg_loss': 0.638935, 'train_acc': 0.689189, 'train_loss': 94.562393, 'train_total': 148, 'train_f1': 0.386947}}[0m
[38;5;39m2023-02-24 20:58:37,933 (client:306) INFO: {'Role': 'Client #5', 'Round': 3, 'Results_raw': {'train_correct': 100.0, 'train_avg_loss': 0.964455, 'train_acc': 0.606061, 'train_loss': 159.135028, 'train_total': 165, 'train_f1': 0.392009}}[0m
[38;5;39m2023-02-24 20:58:37,944 (monitor:541) INFO: {'Role': 'Server #', 'Round': 3, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:58:37,946 (server:330) INFO: Server: Starting evaluation at the end of round 3.[0m
[38;5;39m2023-02-24 20:58:37,948 (server:336) INFO: ----------- Starting a new training round (Round #4) -------------[0m
[33;20m2023-02-24 20:58:38,451 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:38,726 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:38,793 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:39,033 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:39,081 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:39,375 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:39,490 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:39,838 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:40,014 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:40,202 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:58:40,204 (server:590) INFO: {'Role': 'Server #', 'Round': 3, 'Results_weighted_avg': {'test_correct': 16.2, 'test_avg_loss': 0.882875, 'test_acc': 0.736364, 'test_loss': 24.1057, 'test_total': 22.0, 'test_f1': 0.465063}, 'Results_avg': {'test_correct': 16.2, 'test_avg_loss': 1.363692, 'test_acc': 0.576672, 'test_loss': 19.423251, 'test_total': 22.0, 'test_f1': 0.357163}, 'Results_fairness': {'test_correct': 16.2, 'test_total': 22.0, 'test_avg_loss_std': 1.087688, 'test_avg_loss_bottom_decile': 0.63578, 'test_avg_loss_top_decile': 3.521056, 'test_avg_loss_min': 0.506897, 'test_avg_loss_max': 3.521056, 'test_avg_loss_bottom10%': 0.506897, 'test_avg_loss_top10%': 3.521056, 'test_avg_loss_cos1': 0.781781, 'test_avg_loss_entropy': 2.03614, 'test_acc_std': 0.308991, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.934783, 'test_acc_min': 0.0, 'test_acc_max': 0.934783, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.934783, 'test_acc_cos1': 0.881442, 'test_acc_entropy': 2.064382, 'test_loss_std': 9.81999, 'test_loss_bottom_decile': 3.81468, 'test_loss_top_decile': 32.283828, 'test_loss_min': 3.390589, 'test_loss_max': 32.283828, 'test_loss_bottom10%': 3.390589, 'test_loss_top10%': 32.283828, 'test_loss_cos1': 0.892427, 'test_loss_entropy': 2.147719, 'test_f1_std': 0.21553, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.694056, 'test_f1_min': 0.0, 'test_f1_max': 0.694056, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.694056, 'test_f1_cos1': 0.856187, 'test_f1_entropy': 2.033273}}[0m
[38;5;39m2023-02-24 20:58:42,567 (client:306) INFO: {'Role': 'Client #3', 'Round': 4, 'Results_raw': {'train_correct': 211.0, 'train_avg_loss': 0.221671, 'train_acc': 0.946188, 'train_loss': 49.432727, 'train_total': 223, 'train_f1': 0.486175}}[0m
[38;5;39m2023-02-24 20:58:42,761 (client:306) INFO: {'Role': 'Client #7', 'Round': 4, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 3.001914, 'train_acc': 0.125, 'train_loss': 48.030624, 'train_total': 16, 'train_f1': 0.088889}}[0m
[38;5;39m2023-02-24 20:58:42,869 (client:306) INFO: {'Role': 'Client #1', 'Round': 4, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 3.780052, 'train_acc': 0.0, 'train_loss': 26.460367, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:58:44,405 (client:306) INFO: {'Role': 'Client #10', 'Round': 4, 'Results_raw': {'train_correct': 121.0, 'train_avg_loss': 0.476534, 'train_acc': 0.817568, 'train_loss': 70.52699, 'train_total': 148, 'train_f1': 0.510028}}[0m
[38;5;39m2023-02-24 20:58:45,547 (client:306) INFO: {'Role': 'Client #4', 'Round': 4, 'Results_raw': {'train_correct': 92.0, 'train_avg_loss': 0.14009, 'train_acc': 0.938776, 'train_loss': 13.728867, 'train_total': 98, 'train_f1': 0.484211}}[0m
[38;5;39m2023-02-24 20:58:45,660 (client:306) INFO: {'Role': 'Client #6', 'Round': 4, 'Results_raw': {'train_correct': 6.0, 'train_avg_loss': 1.112605, 'train_acc': 0.666667, 'train_loss': 10.013442, 'train_total': 9, 'train_f1': 0.285714}}[0m
[38;5;39m2023-02-24 20:58:47,481 (client:306) INFO: {'Role': 'Client #8', 'Round': 4, 'Results_raw': {'train_correct': 154.0, 'train_avg_loss': 0.305517, 'train_acc': 0.885057, 'train_loss': 53.159976, 'train_total': 174, 'train_f1': 0.729225}}[0m
[38;5;39m2023-02-24 20:58:47,561 (client:306) INFO: {'Role': 'Client #9', 'Round': 4, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 0.785891, 'train_acc': 0.333333, 'train_loss': 4.715349, 'train_total': 6, 'train_f1': 0.25}}[0m
[38;5;39m2023-02-24 20:58:49,187 (client:306) INFO: {'Role': 'Client #5', 'Round': 4, 'Results_raw': {'train_correct': 116.0, 'train_avg_loss': 0.736565, 'train_acc': 0.70303, 'train_loss': 121.533147, 'train_total': 165, 'train_f1': 0.473258}}[0m
[38;5;39m2023-02-24 20:58:49,470 (client:306) INFO: {'Role': 'Client #2', 'Round': 4, 'Results_raw': {'train_correct': 23.0, 'train_avg_loss': 0.678825, 'train_acc': 0.741935, 'train_loss': 21.043566, 'train_total': 31, 'train_f1': 0.361656}}[0m
[38;5;39m2023-02-24 20:58:49,479 (monitor:541) INFO: {'Role': 'Server #', 'Round': 4, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:58:49,481 (server:330) INFO: Server: Starting evaluation at the end of round 4.[0m
[38;5;39m2023-02-24 20:58:49,483 (server:336) INFO: ----------- Starting a new training round (Round #5) -------------[0m
[33;20m2023-02-24 20:58:49,917 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:50,269 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:50,385 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:50,624 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:50,676 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:51,189 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:51,247 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:51,600 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:51,788 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:58:51,962 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:58:51,963 (server:590) INFO: {'Role': 'Server #', 'Round': 4, 'Results_weighted_avg': {'test_correct': 16.4, 'test_avg_loss': 0.79717, 'test_acc': 0.745455, 'test_loss': 20.996327, 'test_total': 22.0, 'test_f1': 0.462952}, 'Results_avg': {'test_correct': 16.4, 'test_avg_loss': 1.343903, 'test_acc': 0.59782, 'test_loss': 17.537748, 'test_total': 22.0, 'test_f1': 0.360112}, 'Results_fairness': {'test_correct': 16.4, 'test_total': 22.0, 'test_avg_loss_std': 1.237978, 'test_avg_loss_bottom_decile': 0.507225, 'test_avg_loss_top_decile': 3.822109, 'test_avg_loss_min': 0.297204, 'test_avg_loss_max': 3.822109, 'test_avg_loss_bottom10%': 0.297204, 'test_avg_loss_top10%': 3.822109, 'test_avg_loss_cos1': 0.735498, 'test_avg_loss_entropy': 1.947066, 'test_acc_std': 0.328433, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.96, 'test_acc_min': 0.0, 'test_acc_max': 0.96, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.96, 'test_acc_cos1': 0.876444, 'test_acc_entropy': 2.058225, 'test_loss_std': 9.253326, 'test_loss_bottom_decile': 3.624811, 'test_loss_top_decile': 29.566394, 'test_loss_min': 3.095116, 'test_loss_max': 29.566394, 'test_loss_bottom10%': 3.095116, 'test_loss_top10%': 29.566394, 'test_loss_cos1': 0.884441, 'test_loss_entropy': 2.134427, 'test_f1_std': 0.214256, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.676198, 'test_f1_min': 0.0, 'test_f1_max': 0.676198, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.676198, 'test_f1_cos1': 0.859395, 'test_f1_entropy': 2.036519}}[0m
[38;5;39m2023-02-24 20:58:53,794 (client:306) INFO: {'Role': 'Client #10', 'Round': 5, 'Results_raw': {'train_correct': 124.0, 'train_avg_loss': 0.416564, 'train_acc': 0.837838, 'train_loss': 61.651538, 'train_total': 148, 'train_f1': 0.538375}}[0m
[38;5;39m2023-02-24 20:58:55,626 (client:306) INFO: {'Role': 'Client #8', 'Round': 5, 'Results_raw': {'train_correct': 150.0, 'train_avg_loss': 0.311475, 'train_acc': 0.862069, 'train_loss': 54.196683, 'train_total': 174, 'train_f1': 0.699395}}[0m
[38;5;39m2023-02-24 20:58:57,480 (client:306) INFO: {'Role': 'Client #5', 'Round': 5, 'Results_raw': {'train_correct': 114.0, 'train_avg_loss': 0.744856, 'train_acc': 0.690909, 'train_loss': 122.901292, 'train_total': 165, 'train_f1': 0.462727}}[0m
[38;5;39m2023-02-24 20:58:57,635 (client:306) INFO: {'Role': 'Client #7', 'Round': 5, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 2.990272, 'train_acc': 0.125, 'train_loss': 47.844345, 'train_total': 16, 'train_f1': 0.083333}}[0m
[38;5;39m2023-02-24 20:59:00,060 (client:306) INFO: {'Role': 'Client #3', 'Round': 5, 'Results_raw': {'train_correct': 212.0, 'train_avg_loss': 0.158351, 'train_acc': 0.950673, 'train_loss': 35.312285, 'train_total': 223, 'train_f1': 0.620572}}[0m
[38;5;39m2023-02-24 20:59:00,146 (client:306) INFO: {'Role': 'Client #1', 'Round': 5, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 3.854441, 'train_acc': 0.0, 'train_loss': 26.981085, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:59:01,333 (client:306) INFO: {'Role': 'Client #4', 'Round': 5, 'Results_raw': {'train_correct': 95.0, 'train_avg_loss': 0.123488, 'train_acc': 0.969388, 'train_loss': 12.101848, 'train_total': 98, 'train_f1': 0.492228}}[0m
[38;5;39m2023-02-24 20:59:01,670 (client:306) INFO: {'Role': 'Client #2', 'Round': 5, 'Results_raw': {'train_correct': 28.0, 'train_avg_loss': 0.530622, 'train_acc': 0.903226, 'train_loss': 16.449286, 'train_total': 31, 'train_f1': 0.54321}}[0m
[38;5;39m2023-02-24 20:59:01,877 (client:306) INFO: {'Role': 'Client #6', 'Round': 5, 'Results_raw': {'train_correct': 7.0, 'train_avg_loss': 1.086955, 'train_acc': 0.777778, 'train_loss': 9.782592, 'train_total': 9, 'train_f1': 0.311111}}[0m
[38;5;39m2023-02-24 20:59:01,997 (client:306) INFO: {'Role': 'Client #9', 'Round': 5, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 0.783591, 'train_acc': 0.333333, 'train_loss': 4.701549, 'train_total': 6, 'train_f1': 0.25}}[0m
[38;5;39m2023-02-24 20:59:02,017 (monitor:541) INFO: {'Role': 'Server #', 'Round': 5, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:59:02,019 (server:330) INFO: Server: Starting evaluation at the end of round 5.[0m
[38;5;39m2023-02-24 20:59:02,021 (server:336) INFO: ----------- Starting a new training round (Round #6) -------------[0m
[33;20m2023-02-24 20:59:02,417 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:02,791 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:02,844 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:03,176 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:03,199 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:03,574 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:03,632 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:04,004 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:04,181 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:04,362 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:59:04,364 (server:590) INFO: {'Role': 'Server #', 'Round': 5, 'Results_weighted_avg': {'test_correct': 16.5, 'test_avg_loss': 0.761784, 'test_acc': 0.75, 'test_loss': 19.860276, 'test_total': 22.0, 'test_f1': 0.458827}, 'Results_avg': {'test_correct': 16.5, 'test_avg_loss': 1.278373, 'test_acc': 0.604538, 'test_loss': 16.75925, 'test_total': 22.0, 'test_f1': 0.357944}, 'Results_fairness': {'test_correct': 16.5, 'test_total': 22.0, 'test_avg_loss_std': 1.176038, 'test_avg_loss_bottom_decile': 0.415875, 'test_avg_loss_top_decile': 3.734591, 'test_avg_loss_min': 0.214627, 'test_avg_loss_max': 3.734591, 'test_avg_loss_bottom10%': 0.214627, 'test_avg_loss_top10%': 3.734591, 'test_avg_loss_cos1': 0.73595, 'test_avg_loss_entropy': 1.943141, 'test_acc_std': 0.328913, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.96, 'test_acc_min': 0.0, 'test_acc_max': 0.96, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.96, 'test_acc_cos1': 0.878405, 'test_acc_entropy': 2.060746, 'test_loss_std': 9.041215, 'test_loss_bottom_decile': 3.313526, 'test_loss_top_decile': 27.853073, 'test_loss_min': 2.868037, 'test_loss_max': 27.853073, 'test_loss_bottom10%': 2.868037, 'test_loss_top10%': 27.853073, 'test_loss_cos1': 0.880098, 'test_loss_entropy': 2.122475, 'test_f1_std': 0.208845, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.65, 'test_f1_min': 0.0, 'test_f1_max': 0.65, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.65, 'test_f1_cos1': 0.863732, 'test_f1_entropy': 2.042565}}[0m
[38;5;39m2023-02-24 20:59:05,489 (client:306) INFO: {'Role': 'Client #4', 'Round': 6, 'Results_raw': {'train_correct': 92.0, 'train_avg_loss': 0.141648, 'train_acc': 0.938776, 'train_loss': 13.881511, 'train_total': 98, 'train_f1': 0.484211}}[0m
[38;5;39m2023-02-24 20:59:05,647 (client:306) INFO: {'Role': 'Client #9', 'Round': 6, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 0.944502, 'train_acc': 0.333333, 'train_loss': 5.667014, 'train_total': 6, 'train_f1': 0.25}}[0m
[38;5;39m2023-02-24 20:59:07,506 (client:306) INFO: {'Role': 'Client #8', 'Round': 6, 'Results_raw': {'train_correct': 152.0, 'train_avg_loss': 0.287289, 'train_acc': 0.873563, 'train_loss': 49.98822, 'train_total': 174, 'train_f1': 0.713816}}[0m
[38;5;39m2023-02-24 20:59:07,908 (client:306) INFO: {'Role': 'Client #2', 'Round': 6, 'Results_raw': {'train_correct': 28.0, 'train_avg_loss': 0.569726, 'train_acc': 0.903226, 'train_loss': 17.661517, 'train_total': 31, 'train_f1': 0.54321}}[0m
[38;5;39m2023-02-24 20:59:09,655 (client:306) INFO: {'Role': 'Client #5', 'Round': 6, 'Results_raw': {'train_correct': 120.0, 'train_avg_loss': 0.681367, 'train_acc': 0.727273, 'train_loss': 112.425508, 'train_total': 165, 'train_f1': 0.486772}}[0m
[38;5;39m2023-02-24 20:59:09,755 (client:306) INFO: {'Role': 'Client #1', 'Round': 6, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 3.628357, 'train_acc': 0.0, 'train_loss': 25.398497, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:59:09,930 (client:306) INFO: {'Role': 'Client #7', 'Round': 6, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 2.810778, 'train_acc': 0.125, 'train_loss': 44.972443, 'train_total': 16, 'train_f1': 0.095238}}[0m
[38;5;39m2023-02-24 20:59:12,560 (client:306) INFO: {'Role': 'Client #3', 'Round': 6, 'Results_raw': {'train_correct': 214.0, 'train_avg_loss': 0.13903, 'train_acc': 0.959641, 'train_loss': 31.003597, 'train_total': 223, 'train_f1': 0.689559}}[0m
[38;5;39m2023-02-24 20:59:12,662 (client:306) INFO: {'Role': 'Client #6', 'Round': 6, 'Results_raw': {'train_correct': 7.0, 'train_avg_loss': 0.968016, 'train_acc': 0.777778, 'train_loss': 8.712145, 'train_total': 9, 'train_f1': 0.311111}}[0m
[38;5;39m2023-02-24 20:59:14,078 (client:306) INFO: {'Role': 'Client #10', 'Round': 6, 'Results_raw': {'train_correct': 121.0, 'train_avg_loss': 0.426599, 'train_acc': 0.817568, 'train_loss': 63.136609, 'train_total': 148, 'train_f1': 0.515057}}[0m
[38;5;39m2023-02-24 20:59:14,087 (monitor:541) INFO: {'Role': 'Server #', 'Round': 6, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:59:14,089 (server:330) INFO: Server: Starting evaluation at the end of round 6.[0m
[38;5;39m2023-02-24 20:59:14,091 (server:336) INFO: ----------- Starting a new training round (Round #7) -------------[0m
[33;20m2023-02-24 20:59:14,575 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:14,984 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:15,068 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:15,275 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:15,299 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:15,644 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:15,706 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:16,061 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:16,222 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:16,436 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:59:16,438 (server:590) INFO: {'Role': 'Server #', 'Round': 6, 'Results_weighted_avg': {'test_correct': 16.3, 'test_avg_loss': 0.737847, 'test_acc': 0.740909, 'test_loss': 19.028058, 'test_total': 22.0, 'test_f1': 0.456466}, 'Results_avg': {'test_correct': 16.3, 'test_avg_loss': 1.264706, 'test_acc': 0.596115, 'test_loss': 16.232639, 'test_total': 22.0, 'test_f1': 0.354837}, 'Results_fairness': {'test_correct': 16.3, 'test_total': 22.0, 'test_avg_loss_std': 1.208656, 'test_avg_loss_bottom_decile': 0.37375, 'test_avg_loss_top_decile': 3.76681, 'test_avg_loss_min': 0.183759, 'test_avg_loss_max': 3.76681, 'test_avg_loss_bottom10%': 0.183759, 'test_avg_loss_top10%': 3.76681, 'test_avg_loss_cos1': 0.722945, 'test_avg_loss_entropy': 1.913117, 'test_acc_std': 0.329808, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.96, 'test_acc_min': 0.0, 'test_acc_max': 0.96, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.96, 'test_acc_cos1': 0.875008, 'test_acc_entropy': 2.056352, 'test_loss_std': 8.911316, 'test_loss_bottom_decile': 3.379556, 'test_loss_top_decile': 27.651707, 'test_loss_min': 2.304682, 'test_loss_max': 27.651707, 'test_loss_bottom10%': 2.304682, 'test_loss_top10%': 27.651707, 'test_loss_cos1': 0.876595, 'test_loss_entropy': 2.113385, 'test_f1_std': 0.213882, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.702381, 'test_f1_min': 0.0, 'test_f1_max': 0.702381, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.702381, 'test_f1_cos1': 0.856448, 'test_f1_entropy': 2.034901}}[0m
[38;5;39m2023-02-24 20:59:18,242 (client:306) INFO: {'Role': 'Client #5', 'Round': 7, 'Results_raw': {'train_correct': 128.0, 'train_avg_loss': 0.56803, 'train_acc': 0.775758, 'train_loss': 93.724874, 'train_total': 165, 'train_f1': 0.766994}}[0m
[38;5;39m2023-02-24 20:59:19,673 (client:306) INFO: {'Role': 'Client #10', 'Round': 7, 'Results_raw': {'train_correct': 125.0, 'train_avg_loss': 0.419834, 'train_acc': 0.844595, 'train_loss': 62.135386, 'train_total': 148, 'train_f1': 0.537749}}[0m
[38;5;39m2023-02-24 20:59:21,416 (client:306) INFO: {'Role': 'Client #8', 'Round': 7, 'Results_raw': {'train_correct': 150.0, 'train_avg_loss': 0.276992, 'train_acc': 0.862069, 'train_loss': 48.196547, 'train_total': 174, 'train_f1': 0.72869}}[0m
[38;5;39m2023-02-24 20:59:21,497 (client:306) INFO: {'Role': 'Client #9', 'Round': 7, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 0.836527, 'train_acc': 0.333333, 'train_loss': 5.019162, 'train_total': 6, 'train_f1': 0.25}}[0m
[38;5;39m2023-02-24 20:59:21,823 (client:306) INFO: {'Role': 'Client #2', 'Round': 7, 'Results_raw': {'train_correct': 27.0, 'train_avg_loss': 0.573344, 'train_acc': 0.870968, 'train_loss': 17.773666, 'train_total': 31, 'train_f1': 0.530864}}[0m
[38;5;39m2023-02-24 20:59:22,933 (client:306) INFO: {'Role': 'Client #4', 'Round': 7, 'Results_raw': {'train_correct': 91.0, 'train_avg_loss': 0.138156, 'train_acc': 0.928571, 'train_loss': 13.539297, 'train_total': 98, 'train_f1': 0.481481}}[0m
[38;5;39m2023-02-24 20:59:23,066 (client:306) INFO: {'Role': 'Client #1', 'Round': 7, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 3.382651, 'train_acc': 0.0, 'train_loss': 23.678554, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:59:25,290 (client:306) INFO: {'Role': 'Client #3', 'Round': 7, 'Results_raw': {'train_correct': 210.0, 'train_avg_loss': 0.170012, 'train_acc': 0.941704, 'train_loss': 37.912608, 'train_total': 223, 'train_f1': 0.551585}}[0m
[38;5;39m2023-02-24 20:59:25,593 (client:306) INFO: {'Role': 'Client #7', 'Round': 7, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 2.625702, 'train_acc': 0.125, 'train_loss': 42.011238, 'train_total': 16, 'train_f1': 0.088889}}[0m
[38;5;39m2023-02-24 20:59:25,703 (client:306) INFO: {'Role': 'Client #6', 'Round': 7, 'Results_raw': {'train_correct': 7.0, 'train_avg_loss': 0.895131, 'train_acc': 0.777778, 'train_loss': 8.05618, 'train_total': 9, 'train_f1': 0.311111}}[0m
[38;5;39m2023-02-24 20:59:25,725 (monitor:541) INFO: {'Role': 'Server #', 'Round': 7, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:59:25,740 (server:330) INFO: Server: Starting evaluation at the end of round 7.[0m
[38;5;39m2023-02-24 20:59:25,742 (server:336) INFO: ----------- Starting a new training round (Round #8) -------------[0m
[33;20m2023-02-24 20:59:26,135 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:26,461 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:26,513 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:26,730 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:26,753 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:27,080 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:27,142 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:27,561 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:27,772 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:28,143 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:59:28,144 (server:590) INFO: {'Role': 'Server #', 'Round': 7, 'Results_weighted_avg': {'test_correct': 17.2, 'test_avg_loss': 0.687049, 'test_acc': 0.781818, 'test_loss': 17.610552, 'test_total': 22.0, 'test_f1': 0.500507}, 'Results_avg': {'test_correct': 17.2, 'test_avg_loss': 1.247244, 'test_acc': 0.624708, 'test_loss': 15.115076, 'test_total': 22.0, 'test_f1': 0.388088}, 'Results_fairness': {'test_correct': 17.2, 'test_total': 22.0, 'test_avg_loss_std': 1.250034, 'test_avg_loss_bottom_decile': 0.379015, 'test_avg_loss_top_decile': 3.699695, 'test_avg_loss_min': 0.335413, 'test_avg_loss_max': 3.699695, 'test_avg_loss_bottom10%': 0.335413, 'test_avg_loss_top10%': 3.699695, 'test_avg_loss_cos1': 0.706316, 'test_avg_loss_entropy': 1.885759, 'test_acc_std': 0.333097, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 0.934783, 'test_acc_min': 0.0, 'test_acc_max': 0.934783, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 0.934783, 'test_acc_cos1': 0.8824, 'test_acc_entropy': 2.064721, 'test_loss_std': 8.534505, 'test_loss_bottom_decile': 3.599707, 'test_loss_top_decile': 28.300831, 'test_loss_min': 2.888124, 'test_loss_max': 28.300831, 'test_loss_bottom10%': 2.888124, 'test_loss_top10%': 28.300831, 'test_loss_cos1': 0.87078, 'test_loss_entropy': 2.127541, 'test_f1_std': 0.238758, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 0.761905, 'test_f1_min': 0.0, 'test_f1_max': 0.761905, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 0.761905, 'test_f1_cos1': 0.851722, 'test_f1_entropy': 2.026966}}[0m
[38;5;39m2023-02-24 20:59:28,433 (client:306) INFO: {'Role': 'Client #7', 'Round': 8, 'Results_raw': {'train_correct': 3.0, 'train_avg_loss': 2.643269, 'train_acc': 0.1875, 'train_loss': 42.292309, 'train_total': 16, 'train_f1': 0.111111}}[0m
[38;5;39m2023-02-24 20:59:28,574 (client:306) INFO: {'Role': 'Client #9', 'Round': 8, 'Results_raw': {'train_correct': 5.0, 'train_avg_loss': 0.483996, 'train_acc': 0.833333, 'train_loss': 2.903975, 'train_total': 6, 'train_f1': 0.454545}}[0m
[38;5;39m2023-02-24 20:59:31,451 (client:306) INFO: {'Role': 'Client #3', 'Round': 8, 'Results_raw': {'train_correct': 210.0, 'train_avg_loss': 0.155144, 'train_acc': 0.941704, 'train_loss': 34.59717, 'train_total': 223, 'train_f1': 0.602496}}[0m
[38;5;39m2023-02-24 20:59:31,744 (client:306) INFO: {'Role': 'Client #2', 'Round': 8, 'Results_raw': {'train_correct': 28.0, 'train_avg_loss': 0.408604, 'train_acc': 0.903226, 'train_loss': 12.666737, 'train_total': 31, 'train_f1': 0.581818}}[0m
[38;5;39m2023-02-24 20:59:33,193 (client:306) INFO: {'Role': 'Client #5', 'Round': 8, 'Results_raw': {'train_correct': 113.0, 'train_avg_loss': 0.708154, 'train_acc': 0.684848, 'train_loss': 116.845424, 'train_total': 165, 'train_f1': 0.455596}}[0m
[38;5;39m2023-02-24 20:59:33,306 (client:306) INFO: {'Role': 'Client #6', 'Round': 8, 'Results_raw': {'train_correct': 6.0, 'train_avg_loss': 0.962486, 'train_acc': 0.666667, 'train_loss': 8.662374, 'train_total': 9, 'train_f1': 0.307692}}[0m
[38;5;39m2023-02-24 20:59:33,397 (client:306) INFO: {'Role': 'Client #1', 'Round': 8, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 3.404731, 'train_acc': 0.0, 'train_loss': 23.833117, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:59:34,353 (client:306) INFO: {'Role': 'Client #4', 'Round': 8, 'Results_raw': {'train_correct': 97.0, 'train_avg_loss': 0.055607, 'train_acc': 0.989796, 'train_loss': 5.449449, 'train_total': 98, 'train_f1': 0.497436}}[0m
[38;5;39m2023-02-24 20:59:35,744 (client:306) INFO: {'Role': 'Client #10', 'Round': 8, 'Results_raw': {'train_correct': 124.0, 'train_avg_loss': 0.381317, 'train_acc': 0.837838, 'train_loss': 56.434957, 'train_total': 148, 'train_f1': 0.538721}}[0m
[38;5;39m2023-02-24 20:59:37,488 (client:306) INFO: {'Role': 'Client #8', 'Round': 8, 'Results_raw': {'train_correct': 158.0, 'train_avg_loss': 0.235252, 'train_acc': 0.908046, 'train_loss': 40.933931, 'train_total': 174, 'train_f1': 0.78338}}[0m
[38;5;39m2023-02-24 20:59:37,496 (monitor:541) INFO: {'Role': 'Server #', 'Round': 8, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:59:37,503 (server:330) INFO: Server: Starting evaluation at the end of round 8.[0m
[38;5;39m2023-02-24 20:59:37,509 (server:336) INFO: ----------- Starting a new training round (Round #9) -------------[0m
[33;20m2023-02-24 20:59:38,004 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:38,309 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:38,369 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:38,663 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:38,688 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:38,969 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:39,028 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:39,405 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:39,677 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:39,874 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:59:39,875 (server:590) INFO: {'Role': 'Server #', 'Round': 8, 'Results_weighted_avg': {'test_correct': 17.0, 'test_avg_loss': 0.689898, 'test_acc': 0.772727, 'test_loss': 17.457165, 'test_total': 22.0, 'test_f1': 0.529546}, 'Results_avg': {'test_correct': 17.0, 'test_avg_loss': 1.237811, 'test_acc': 0.625413, 'test_loss': 15.177762, 'test_total': 22.0, 'test_f1': 0.420073}, 'Results_fairness': {'test_correct': 17.0, 'test_total': 22.0, 'test_avg_loss_std': 1.238458, 'test_avg_loss_bottom_decile': 0.334428, 'test_avg_loss_top_decile': 3.828399, 'test_avg_loss_min': 0.132819, 'test_avg_loss_max': 3.828399, 'test_avg_loss_bottom10%': 0.132819, 'test_avg_loss_top10%': 3.828399, 'test_avg_loss_cos1': 0.706922, 'test_avg_loss_entropy': 1.878473, 'test_acc_std': 0.332862, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.882758, 'test_acc_entropy': 2.066314, 'test_loss_std': 8.489535, 'test_loss_bottom_decile': 3.320476, 'test_loss_top_decile': 26.517619, 'test_loss_min': 2.509546, 'test_loss_max': 26.517619, 'test_loss_bottom10%': 2.509546, 'test_loss_top10%': 26.517619, 'test_loss_cos1': 0.872751, 'test_loss_entropy': 2.108233, 'test_f1_std': 0.287556, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.825182, 'test_f1_entropy': 1.99816}}[0m
[38;5;39m2023-02-24 20:59:41,763 (client:306) INFO: {'Role': 'Client #5', 'Round': 9, 'Results_raw': {'train_correct': 123.0, 'train_avg_loss': 0.636343, 'train_acc': 0.745455, 'train_loss': 104.996533, 'train_total': 165, 'train_f1': 0.498525}}[0m
[38;5;39m2023-02-24 20:59:42,833 (client:306) INFO: {'Role': 'Client #4', 'Round': 9, 'Results_raw': {'train_correct': 96.0, 'train_avg_loss': 0.091569, 'train_acc': 0.979592, 'train_loss': 8.973772, 'train_total': 98, 'train_f1': 0.494845}}[0m
[38;5;39m2023-02-24 20:59:42,950 (client:306) INFO: {'Role': 'Client #6', 'Round': 9, 'Results_raw': {'train_correct': 7.0, 'train_avg_loss': 0.881913, 'train_acc': 0.777778, 'train_loss': 7.937215, 'train_total': 9, 'train_f1': 0.311111}}[0m
[38;5;39m2023-02-24 20:59:44,897 (client:306) INFO: {'Role': 'Client #10', 'Round': 9, 'Results_raw': {'train_correct': 127.0, 'train_avg_loss': 0.353841, 'train_acc': 0.858108, 'train_loss': 52.368499, 'train_total': 148, 'train_f1': 0.555654}}[0m
[38;5;39m2023-02-24 20:59:45,210 (client:306) INFO: {'Role': 'Client #2', 'Round': 9, 'Results_raw': {'train_correct': 25.0, 'train_avg_loss': 0.56682, 'train_acc': 0.806452, 'train_loss': 17.571435, 'train_total': 31, 'train_f1': 0.448802}}[0m
[38;5;39m2023-02-24 20:59:45,311 (client:306) INFO: {'Role': 'Client #9', 'Round': 9, 'Results_raw': {'train_correct': 4.0, 'train_avg_loss': 0.615999, 'train_acc': 0.666667, 'train_loss': 3.695995, 'train_total': 6, 'train_f1': 0.4}}[0m
[38;5;39m2023-02-24 20:59:46,959 (client:306) INFO: {'Role': 'Client #8', 'Round': 9, 'Results_raw': {'train_correct': 162.0, 'train_avg_loss': 0.22418, 'train_acc': 0.931034, 'train_loss': 39.007404, 'train_total': 174, 'train_f1': 0.830519}}[0m
[38;5;39m2023-02-24 20:59:47,043 (client:306) INFO: {'Role': 'Client #1', 'Round': 9, 'Results_raw': {'train_correct': 0.0, 'train_avg_loss': 3.410846, 'train_acc': 0.0, 'train_loss': 23.875922, 'train_total': 7, 'train_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:59:49,117 (client:306) INFO: {'Role': 'Client #3', 'Round': 9, 'Results_raw': {'train_correct': 213.0, 'train_avg_loss': 0.126798, 'train_acc': 0.955157, 'train_loss': 28.275899, 'train_total': 223, 'train_f1': 0.675872}}[0m
[38;5;39m2023-02-24 20:59:49,289 (client:306) INFO: {'Role': 'Client #7', 'Round': 9, 'Results_raw': {'train_correct': 2.0, 'train_avg_loss': 2.592295, 'train_acc': 0.125, 'train_loss': 41.476715, 'train_total': 16, 'train_f1': 0.083333}}[0m
[38;5;39m2023-02-24 20:59:49,311 (monitor:541) INFO: {'Role': 'Server #', 'Round': 9, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 20:59:49,313 (server:347) INFO: Server: Training is finished! Starting evaluation.[0m
[33;20m2023-02-24 20:59:49,768 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:50,136 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:50,242 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:50,536 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:50,559 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:50,887 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:50,950 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:51,335 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:51,491 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 20:59:51,749 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 20:59:51,751 (server:590) INFO: {'Role': 'Server #', 'Round': 9, 'Results_weighted_avg': {'test_correct': 17.1, 'test_avg_loss': 0.639963, 'test_acc': 0.777273, 'test_loss': 16.361293, 'test_total': 22.0, 'test_f1': 0.535371}, 'Results_avg': {'test_correct': 17.1, 'test_avg_loss': 1.139709, 'test_acc': 0.629907, 'test_loss': 14.079192, 'test_total': 22.0, 'test_f1': 0.424826}, 'Results_fairness': {'test_correct': 17.1, 'test_total': 22.0, 'test_avg_loss_std': 1.136328, 'test_avg_loss_bottom_decile': 0.319905, 'test_avg_loss_top_decile': 3.4263, 'test_avg_loss_min': 0.125422, 'test_avg_loss_max': 3.4263, 'test_avg_loss_bottom10%': 0.125422, 'test_avg_loss_top10%': 3.4263, 'test_avg_loss_cos1': 0.708157, 'test_avg_loss_entropy': 1.880268, 'test_acc_std': 0.334209, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.883365, 'test_acc_entropy': 2.066941, 'test_loss_std': 7.936347, 'test_loss_bottom_decile': 3.135543, 'test_loss_top_decile': 24.615811, 'test_loss_min': 2.152262, 'test_loss_max': 24.615811, 'test_loss_bottom10%': 2.152262, 'test_loss_top10%': 24.615811, 'test_loss_cos1': 0.871131, 'test_loss_entropy': 2.105609, 'test_f1_std': 0.289675, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.826208, 'test_f1_entropy': 1.999289}}[0m
[38;5;39m2023-02-24 20:59:51,752 (server:395) INFO: Server: Final evaluation is finished! Starting merging results.[0m
[38;5;39m2023-02-24 20:59:51,753 (server:521) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'test_loss': 2.152262, 'test_correct': 43.0, 'test_avg_loss': 0.125422, 'test_acc': 1.0, 'test_total': 1.0, 'test_f1': 1.0}, 'client_summarized_weighted_avg': {'test_loss': 16.361293, 'test_correct': 17.1, 'test_avg_loss': 0.639963, 'test_acc': 0.777273, 'test_total': 22.0, 'test_f1': 0.535371}, 'client_summarized_avg': {'test_loss': 14.079192, 'test_correct': 17.1, 'test_avg_loss': 1.139709, 'test_acc': 0.629907, 'test_total': 22.0, 'test_f1': 0.424826}, 'client_summarized_fairness': {'test_loss_entropy': 2.069611, 'test_loss_cos1': 0.848993, 'test_loss_top10%': 164.515259, 'test_loss_bottom10%': 4.081591, 'test_loss_max': 164.515259, 'test_loss_min': 4.081591, 'test_loss_top_decile': 164.515259, 'test_loss_bottom_decile': 20.440338, 'test_loss_std': 49.509677, 'test_correct': 12.2, 'test_total': 22.0, 'test_avg_loss_std': 0.200294, 'test_avg_loss_bottom_decile': 3.546271, 'test_avg_loss_top_decile': 4.088068, 'test_avg_loss_min': 3.542077, 'test_avg_loss_max': 4.088068, 'test_avg_loss_bottom10%': 3.542077, 'test_avg_loss_top10%': 4.088068, 'test_avg_loss_cos1': 0.998539, 'test_avg_loss_entropy': 2.301152, 'test_acc_std': 0.387263, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.767926, 'test_acc_entropy': 1.854699, 'test_f1_std': 0.289968, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.71197, 'test_f1_entropy': 1.799419}}}[0m
[38;5;39m2023-02-24 20:59:51,754 (server:540) INFO: {'Role': 'Client #1', 'Round': 10, 'Results_raw': {'test_correct': 43.0, 'test_avg_loss': 0.319905, 'test_acc': 0.934783, 'test_loss': 14.715648, 'test_total': 46, 'test_f1': 0.547804}}[0m
[38;5;39m2023-02-24 20:59:51,754 (server:540) INFO: {'Role': 'Client #2', 'Round': 10, 'Results_raw': {'test_correct': 25.0, 'test_avg_loss': 0.732321, 'test_acc': 0.806452, 'test_loss': 22.701939, 'test_total': 31, 'test_f1': 0.314465}}[0m
[38;5;39m2023-02-24 20:59:51,754 (server:540) INFO: {'Role': 'Client #3', 'Round': 10, 'Results_raw': {'test_correct': 0.0, 'test_avg_loss': 3.4263, 'test_acc': 0.0, 'test_loss': 17.1315, 'test_total': 5, 'test_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:59:51,755 (server:540) INFO: {'Role': 'Client #4', 'Round': 10, 'Results_raw': {'test_correct': 25.0, 'test_avg_loss': 0.125422, 'test_acc': 1.0, 'test_loss': 3.135543, 'test_total': 25, 'test_f1': 1.0}}[0m
[38;5;39m2023-02-24 20:59:51,755 (server:540) INFO: {'Role': 'Client #5', 'Round': 10, 'Results_raw': {'test_correct': 0.0, 'test_avg_loss': 3.218225, 'test_acc': 0.0, 'test_loss': 3.218225, 'test_total': 1, 'test_f1': 0.0}}[0m
[38;5;39m2023-02-24 20:59:51,755 (server:540) INFO: {'Role': 'Client #6', 'Round': 10, 'Results_raw': {'test_correct': 21.0, 'test_avg_loss': 0.620144, 'test_acc': 0.65625, 'test_loss': 19.8446, 'test_total': 32, 'test_f1': 0.46908}}[0m
[38;5;39m2023-02-24 20:59:51,756 (server:540) INFO: {'Role': 'Client #7', 'Round': 10, 'Results_raw': {'test_correct': 5.0, 'test_avg_loss': 0.35871, 'test_acc': 0.833333, 'test_loss': 2.152262, 'test_total': 6, 'test_f1': 0.454545}}[0m
[38;5;39m2023-02-24 20:59:51,756 (server:540) INFO: {'Role': 'Client #8', 'Round': 10, 'Results_raw': {'test_correct': 26.0, 'test_avg_loss': 0.534104, 'test_acc': 0.742857, 'test_loss': 18.693631, 'test_total': 35, 'test_f1': 0.728682}}[0m
[38;5;39m2023-02-24 20:59:51,756 (server:540) INFO: {'Role': 'Client #9', 'Round': 10, 'Results_raw': {'test_correct': 11.0, 'test_avg_loss': 1.367545, 'test_acc': 0.611111, 'test_loss': 24.615811, 'test_total': 18, 'test_f1': 0.271605}}[0m
[38;5;39m2023-02-24 20:59:51,756 (server:540) INFO: {'Role': 'Client #10', 'Round': 10, 'Results_raw': {'test_correct': 15.0, 'test_avg_loss': 0.694417, 'test_acc': 0.714286, 'test_loss': 14.582757, 'test_total': 21, 'test_f1': 0.462081}}[0m
[38;5;39m2023-02-24 20:59:51,757 (monitor:172) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 2.10872, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 2488000, 'total_download_bytes': 1161640, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,758 (client:513) INFO: ================= client 1 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,760 (monitor:172) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 2.106112, 'total_model_size': 3260030, 'total_flops': 18580679040.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,761 (client:513) INFO: ================= client 2 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,763 (monitor:172) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 2.102926, 'total_model_size': 3260030, 'total_flops': 82285864320.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,764 (client:513) INFO: ================= client 3 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,766 (monitor:172) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 2.100243, 'total_model_size': 3260030, 'total_flops': 591927346560.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,767 (client:513) INFO: ================= client 4 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,769 (monitor:172) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 2.094758, 'total_model_size': 3260030, 'total_flops': 260129506560.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,770 (client:513) INFO: ================= client 5 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,772 (monitor:172) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 2.092876, 'total_model_size': 3260030, 'total_flops': 437973148800.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,772 (client:513) INFO: ================= client 6 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,775 (monitor:172) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 2.090065, 'total_model_size': 3260030, 'total_flops': 23889444480.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,775 (client:513) INFO: ================= client 7 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,777 (monitor:172) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 2.088565, 'total_model_size': 3260030, 'total_flops': 42470123520.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,778 (client:513) INFO: ================= client 8 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,780 (monitor:172) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 2.085506, 'total_model_size': 3260030, 'total_flops': 461862593280.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,781 (client:513) INFO: ================= client 9 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,783 (monitor:172) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 2.080951, 'total_model_size': 3260030, 'total_flops': 15926296320.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,783 (client:513) INFO: ================= client 10 received finish message =================[0m
[38;5;39m2023-02-24 20:59:51,785 (monitor:172) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 2.076568, 'total_model_size': 3260030, 'total_flops': 392848642560.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 20:59:51,786 (monitor:337) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one[0m
[38;5;39m2023-02-24 20:59:51,790 (monitor:245) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 2.09339, 'sys_avg/total_model_size': '2.83M', 'sys_avg/total_flops': '197.09G', 'sys_avg/total_upload_bytes': '324.09K', 'sys_avg/total_download_bytes': '103.13K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})[0m
[38;5;39m2023-02-24 20:59:51,790 (monitor:248) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.009868, 'sys_std/total_model_size': '915.23K', 'sys_std/total_flops': '197.49G', 'sys_std/total_upload_bytes': '665.85K', 'sys_std/total_download_bytes': '326.12K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})[0m
None
220
<generator object BaseDataTranslator.split_to_client.<locals>.<genexpr> at 0x7fca00080200>
