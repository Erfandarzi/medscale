 is at 192.168.0.1[0m
[38;5;39m2023-02-24 22:06:12,631 (logging:126) INFO: the current dir is /home/ubuntu/medscale[0m
[38;5;39m2023-02-24 22:06:12,631 (logging:127) INFO: the output dir is exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612[0m
[33;20m2023-02-24 22:06:23,083 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[33;20m2023-02-24 22:06:23,083 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:23,107 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:23,269 (utils:144) INFO: The device information file is not provided[0m
[38;5;39m2023-02-24 22:06:23,635 (fed_runner:169) INFO: Server has been set up ... [0m
[33;20m2023-02-24 22:06:23,801 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:23,849 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:23,876 (fed_runner:221) INFO: Client 1 has been set up ... [0m
[33;20m2023-02-24 22:06:24,032 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:24,057 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:24,082 (fed_runner:221) INFO: Client 2 has been set up ... [0m
[33;20m2023-02-24 22:06:24,295 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:24,318 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:24,327 (fed_runner:221) INFO: Client 3 has been set up ... [0m
[33;20m2023-02-24 22:06:24,466 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:24,489 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:24,498 (fed_runner:221) INFO: Client 4 has been set up ... [0m
[33;20m2023-02-24 22:06:24,704 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:24,727 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:24,736 (fed_runner:221) INFO: Client 5 has been set up ... [0m
[33;20m2023-02-24 22:06:24,872 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:24,896 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:24,905 (fed_runner:221) INFO: Client 6 has been set up ... [0m
[33;20m2023-02-24 22:06:25,124 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:25,147 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:25,156 (fed_runner:221) INFO: Client 7 has been set up ... [0m
[33;20m2023-02-24 22:06:25,314 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:25,345 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:25,356 (fed_runner:221) INFO: Client 8 has been set up ... [0m
[33;20m2023-02-24 22:06:25,411 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:25,435 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:25,447 (fed_runner:221) INFO: Client 9 has been set up ... [0m
[33;20m2023-02-24 22:06:25,679 (cfg_data:145) WARNING: config `cfg.data.batch_size` will be removed in the future, use `cfg.dataloader.batch_size` instead.[0m
[38;5;39m2023-02-24 22:06:25,702 (config:243) INFO: the used configs are: 
aggregator:
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.1307]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.3081]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 16
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: False
  drop_last: False
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0, 0.2]
  splitter: lda
  splitter_args: [{'alpha': 0.6}]
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: lung100
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 16
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 5
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: True
  freq: 1
  metrics: ['acc', 'correct', 'f1']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['test', 'val']
expname: FedAvg_convnet5_on_lung100_lr0.01_lstep1
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_num: 10
  data_weighted_aggr: False
  ignore_weight: False
  join_in_info: []
  make_global_eval: False
  merge_test_data: False
  method: FedAvg
  mode: standalone
  online_aggr: True
  resource_info_file: 
  restore_from: 
  sample_client_num: 10
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 10
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  mu: 0.5
  use: True
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: 5.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  working_folder: hpo
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.0
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 2048
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 62
  pretrain_tasks: []
  stage: 
  task: node
  type: convnet5
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/FedAvg_convnet5_on_lung100_lr0.01_lstep1/sub_exp_20230224220612
personalization:
  K: 5
  beta: 1.0
  local_param: []
  local_update_steps: 1
  lr: 0.01
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.5
  type: proximal_regularizer
seed: 2
sgdmf:
  use: False
train:
  batch_or_epoch: epoch
  local_update_steps: 1
  optimizer:
    lr: 0.01
    type: SGD
    weight_decay: 0.0
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.001
    gamma: 0.0001
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: cvtrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False[0m
[38;5;39m2023-02-24 22:06:25,715 (fed_runner:221) INFO: Client 10 has been set up ... [0m
<<<<<<< HEAD
[38;5;39m2023-02-24 22:06:25,715 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.[0m
=======
<<<<<<< HEAD
[38;5;39m2023-02-24 22:06:25,715 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.[0m
=======
[38;5;39m2023-02-24 22:06:25,715 (trainer:341) INFO: Model meta-info: <class 'medscale.cv.model.cnn.ConvNet5'>.[0m
>>>>>>> fe4962455354c9c11afd9c9806ceda28eb280737
>>>>>>> 64b283ee525ef53c32509882719e74890329b83f
[38;5;39m2023-02-24 22:06:25,716 (trainer:349) INFO: Num of original para names: 39.[0m
[38;5;39m2023-02-24 22:06:25,716 (trainer:350) INFO: Num of original trainable para names: 24.[0m
[38;5;39m2023-02-24 22:06:25,716 (trainer:352) INFO: Num of preserved para names in local update: 39. 
Preserved para names in local update: {'conv1.weight', 'bn5.running_var', 'bn5.weight', 'bn1.num_batches_tracked', 'bn4.weight', 'fc2.weight', 'conv3.bias', 'bn2.running_var', 'bn2.running_mean', 'conv1.bias', 'bn2.weight', 'bn4.num_batches_tracked', 'conv5.weight', 'bn3.bias', 'bn2.num_batches_tracked', 'fc1.weight', 'conv5.bias', 'fc1.bias', 'bn1.running_var', 'bn1.bias', 'bn3.num_batches_tracked', 'bn5.bias', 'fc2.bias', 'conv2.weight', 'conv4.bias', 'conv4.weight', 'bn1.running_mean', 'bn3.weight', 'bn4.bias', 'bn5.running_mean', 'bn4.running_var', 'bn1.weight', 'conv2.bias', 'bn4.running_mean', 'bn5.num_batches_tracked', 'bn2.bias', 'conv3.weight', 'bn3.running_var', 'bn3.running_mean'}.[0m
[38;5;39m2023-02-24 22:06:25,716 (trainer:356) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().[0m
[38;5;39m2023-02-24 22:06:25,716 (trainer:361) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_init",
	    "_hook_record_initialization"
	  ],
	  "on_epoch_start": [
	    "_hook_on_epoch_start"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_del_initialization"
	  ]
	}[0m
[38;5;39m2023-02-24 22:06:25,731 (server:804) INFO: ----------- Starting training (Round #0) -------------[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:32,106 (client:306) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_correct': 152.0, 'train_total': 177, 'train_acc': 0.858757, 'train_avg_loss': 1.312542, 'train_f1': 0.154002, 'train_loss': 232.319908}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:32,687 (client:306) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_correct': 20.0, 'train_total': 41, 'train_acc': 0.487805, 'train_avg_loss': 3.782964, 'train_f1': 0.116959, 'train_loss': 155.101505}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:34,185 (client:306) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_correct': 77.0, 'train_total': 138, 'train_acc': 0.557971, 'train_avg_loss': 2.139373, 'train_f1': 0.158543, 'train_loss': 295.233507}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:35,456 (client:306) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_correct': 75.0, 'train_total': 110, 'train_acc': 0.681818, 'train_avg_loss': 2.281975, 'train_f1': 0.163043, 'train_loss': 251.017208}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:35,817 (client:306) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_correct': 0.0, 'train_total': 32, 'train_acc': 0.0, 'train_avg_loss': 4.323767, 'train_f1': 0.0, 'train_loss': 138.360554}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:35,907 (client:306) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_correct': 0.0, 'train_total': 5, 'train_acc': 0.0, 'train_avg_loss': 4.668561, 'train_f1': 0.0, 'train_loss': 23.342807}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:36,554 (client:306) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_correct': 20.0, 'train_total': 62, 'train_acc': 0.322581, 'train_avg_loss': 3.436633, 'train_f1': 0.091324, 'train_loss': 213.071218}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:37,458 (client:306) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_correct': 32.0, 'train_total': 82, 'train_acc': 0.390244, 'train_avg_loss': 3.031386, 'train_f1': 0.134897, 'train_loss': 248.573612}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:37,688 (client:306) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_correct': 0.0, 'train_total': 14, 'train_acc': 0.0, 'train_avg_loss': 4.616735, 'train_f1': 0.0, 'train_loss': 64.63429}}[0m
Unsupported operator aten::max_pool2d encountered 5 time(s)
[38;5;39m2023-02-24 22:06:40,441 (client:306) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_correct': 170.0, 'train_total': 216, 'train_acc': 0.787037, 'train_avg_loss': 1.313694, 'train_f1': 0.126818, 'train_loss': 283.757913}}[0m
[38;5;39m2023-02-24 22:06:40,452 (monitor:541) INFO: {'Role': 'Server #', 'Round': 0, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:06:40,455 (server:330) INFO: Server: Starting evaluation at the end of round 0.[0m
[38;5;39m2023-02-24 22:06:40,457 (server:336) INFO: ----------- Starting a new training round (Round #1) -------------[0m
[33;20m2023-02-24 22:06:40,534 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:40,624 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:40,785 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:41,188 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:41,431 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:41,703 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:42,179 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:42,435 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:42,505 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:42,744 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:06:42,746 (server:590) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'test_correct': 7.5, 'test_total': 22.0, 'test_acc': 0.340909, 'test_avg_loss': 3.761973, 'test_f1': 0.1928, 'test_loss': 120.39524}, 'Results_avg': {'test_correct': 7.5, 'test_total': 22.0, 'test_acc': 0.405732, 'test_avg_loss': 3.760732, 'test_f1': 0.248108, 'test_loss': 82.763402}, 'Results_fairness': {'test_correct': 7.5, 'test_total': 22.0, 'test_acc_std': 0.359253, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.748688, 'test_acc_entropy': 1.788129, 'test_avg_loss_std': 0.082331, 'test_avg_loss_bottom_decile': 3.705273, 'test_avg_loss_top_decile': 3.987761, 'test_avg_loss_min': 3.67605, 'test_avg_loss_max': 3.987761, 'test_avg_loss_bottom10%': 3.67605, 'test_avg_loss_top10%': 3.987761, 'test_avg_loss_cos1': 0.99976, 'test_avg_loss_entropy': 2.302349, 'test_f1_std': 0.285801, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.655554, 'test_f1_entropy': 1.667438, 'test_loss_std': 55.660377, 'test_loss_bottom_decile': 29.408401, 'test_loss_top_decile': 186.014372, 'test_loss_min': 18.778868, 'test_loss_max': 186.014372, 'test_loss_bottom10%': 18.778868, 'test_loss_top10%': 186.014372, 'test_loss_cos1': 0.8298, 'test_loss_entropy': 2.0905}}[0m
[38;5;39m2023-02-24 22:06:43,367 (client:306) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'train_correct': 44.0, 'train_total': 62, 'train_acc': 0.709677, 'train_avg_loss': 1.003404, 'train_f1': 0.428283, 'train_loss': 62.211048}}[0m
[38;5;39m2023-02-24 22:06:45,451 (client:306) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'train_correct': 168.0, 'train_total': 177, 'train_acc': 0.949153, 'train_avg_loss': 0.414101, 'train_f1': 0.324638, 'train_loss': 73.295911}}[0m
[38;5;39m2023-02-24 22:06:47,025 (client:306) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'train_correct': 102.0, 'train_total': 138, 'train_acc': 0.73913, 'train_avg_loss': 0.708306, 'train_f1': 0.616667, 'train_loss': 97.746225}}[0m
[38;5;39m2023-02-24 22:06:47,254 (client:306) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'train_correct': 2.0, 'train_total': 14, 'train_acc': 0.142857, 'train_avg_loss': 3.512351, 'train_f1': 0.166667, 'train_loss': 49.172918}}[0m
[38;5;39m2023-02-24 22:06:47,647 (client:306) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'train_correct': 34.0, 'train_total': 41, 'train_acc': 0.829268, 'train_avg_loss': 1.164364, 'train_f1': 0.434703, 'train_loss': 47.738906}}[0m
[38;5;39m2023-02-24 22:06:48,877 (client:306) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'train_correct': 27.0, 'train_total': 82, 'train_acc': 0.329268, 'train_avg_loss': 1.694633, 'train_f1': 0.232144, 'train_loss': 138.959911}}[0m
[38;5;39m2023-02-24 22:06:51,411 (client:306) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'train_correct': 187.0, 'train_total': 216, 'train_acc': 0.865741, 'train_avg_loss': 0.617578, 'train_f1': 0.310889, 'train_loss': 133.396898}}[0m
[38;5;39m2023-02-24 22:06:51,840 (client:306) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'train_correct': 18.0, 'train_total': 32, 'train_acc': 0.5625, 'train_avg_loss': 1.856952, 'train_f1': 0.432065, 'train_loss': 59.422459}}[0m
[38;5;39m2023-02-24 22:06:51,913 (client:306) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'train_correct': 3.0, 'train_total': 5, 'train_acc': 0.6, 'train_avg_loss': 2.263114, 'train_f1': 0.333333, 'train_loss': 11.315572}}[0m
[38;5;39m2023-02-24 22:06:53,215 (client:306) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'train_correct': 89.0, 'train_total': 110, 'train_acc': 0.809091, 'train_avg_loss': 0.662919, 'train_f1': 0.582957, 'train_loss': 72.921056}}[0m
[38;5;39m2023-02-24 22:06:53,231 (monitor:541) INFO: {'Role': 'Server #', 'Round': 1, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:06:53,234 (server:330) INFO: Server: Starting evaluation at the end of round 1.[0m
[38;5;39m2023-02-24 22:06:53,236 (server:336) INFO: ----------- Starting a new training round (Round #2) -------------[0m
[33;20m2023-02-24 22:06:53,376 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:53,459 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:53,618 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:54,017 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:54,199 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:54,417 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:54,835 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:55,084 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:55,159 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:06:55,477 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:06:55,482 (server:590) INFO: {'Role': 'Server #', 'Round': 1, 'Results_weighted_avg': {'test_correct': 8.2, 'test_total': 22.0, 'test_acc': 0.372727, 'test_avg_loss': 2.851006, 'test_f1': 0.224152, 'test_loss': 90.796673}, 'Results_avg': {'test_correct': 8.2, 'test_total': 22.0, 'test_acc': 0.422657, 'test_avg_loss': 2.849301, 'test_f1': 0.264022, 'test_loss': 62.722122}, 'Results_fairness': {'test_correct': 8.2, 'test_total': 22.0, 'test_acc_std': 0.346279, 'test_acc_bottom_decile': 0.0, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.0, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.0, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.773537, 'test_acc_entropy': 1.87886, 'test_avg_loss_std': 0.187921, 'test_avg_loss_bottom_decile': 2.723042, 'test_avg_loss_top_decile': 3.372993, 'test_avg_loss_min': 2.666442, 'test_avg_loss_max': 3.372993, 'test_avg_loss_bottom10%': 2.666442, 'test_avg_loss_top10%': 3.372993, 'test_avg_loss_cos1': 0.997832, 'test_avg_loss_entropy': 2.300498, 'test_f1_std': 0.281041, 'test_f1_bottom_decile': 0.0, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.0, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.0, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.684694, 'test_f1_entropy': 1.771654, 'test_loss_std': 41.650429, 'test_loss_bottom_decile': 21.331535, 'test_loss_top_decile': 139.541307, 'test_loss_min': 14.076977, 'test_loss_max': 139.541307, 'test_loss_bottom10%': 14.076977, 'test_loss_top10%': 139.541307, 'test_loss_cos1': 0.833056, 'test_loss_entropy': 2.092429}}[0m
[38;5;39m2023-02-24 22:06:55,581 (client:306) INFO: {'Role': 'Client #4', 'Round': 2, 'Results_raw': {'train_correct': 3.0, 'train_total': 5, 'train_acc': 0.6, 'train_avg_loss': 1.703891, 'train_f1': 0.375, 'train_loss': 8.519453}}[0m
[38;5;39m2023-02-24 22:06:56,222 (client:306) INFO: {'Role': 'Client #10', 'Round': 2, 'Results_raw': {'train_correct': 39.0, 'train_total': 62, 'train_acc': 0.629032, 'train_avg_loss': 0.812906, 'train_f1': 0.419753, 'train_loss': 50.400201}}[0m
[38;5;39m2023-02-24 22:06:56,357 (client:306) INFO: {'Role': 'Client #8', 'Round': 2, 'Results_raw': {'train_correct': 2.0, 'train_total': 14, 'train_acc': 0.142857, 'train_avg_loss': 3.10758, 'train_f1': 0.222222, 'train_loss': 43.506119}}[0m
[38;5;39m2023-02-24 22:06:56,788 (client:306) INFO: {'Role': 'Client #2', 'Round': 2, 'Results_raw': {'train_correct': 17.0, 'train_total': 32, 'train_acc': 0.53125, 'train_avg_loss': 1.351078, 'train_f1': 0.40404, 'train_loss': 43.234491}}[0m
[38;5;39m2023-02-24 22:06:58,464 (client:306) INFO: {'Role': 'Client #5', 'Round': 2, 'Results_raw': {'train_correct': 89.0, 'train_total': 110, 'train_acc': 0.809091, 'train_avg_loss': 0.535388, 'train_f1': 0.605735, 'train_loss': 58.892723}}[0m
[38;5;39m2023-02-24 22:07:01,230 (client:306) INFO: {'Role': 'Client #1', 'Round': 2, 'Results_raw': {'train_correct': 191.0, 'train_total': 216, 'train_acc': 0.884259, 'train_avg_loss': 0.472864, 'train_f1': 0.313629, 'train_loss': 102.138656}}[0m
[38;5;39m2023-02-24 22:07:01,763 (client:306) INFO: {'Role': 'Client #7', 'Round': 2, 'Results_raw': {'train_correct': 31.0, 'train_total': 41, 'train_acc': 0.756098, 'train_avg_loss': 0.733578, 'train_f1': 0.360784, 'train_loss': 30.076681}}[0m
[38;5;39m2023-02-24 22:07:03,416 (client:306) INFO: {'Role': 'Client #3', 'Round': 2, 'Results_raw': {'train_correct': 165.0, 'train_total': 177, 'train_acc': 0.932203, 'train_avg_loss': 0.302425, 'train_f1': 0.322581, 'train_loss': 53.529292}}[0m
[38;5;39m2023-02-24 22:07:04,326 (client:306) INFO: {'Role': 'Client #6', 'Round': 2, 'Results_raw': {'train_correct': 40.0, 'train_total': 82, 'train_acc': 0.487805, 'train_avg_loss': 1.277102, 'train_f1': 0.332235, 'train_loss': 104.72237}}[0m
[38;5;39m2023-02-24 22:07:05,656 (client:306) INFO: {'Role': 'Client #9', 'Round': 2, 'Results_raw': {'train_correct': 113.0, 'train_total': 138, 'train_acc': 0.818841, 'train_avg_loss': 0.515133, 'train_f1': 0.748451, 'train_loss': 71.088304}}[0m
[38;5;39m2023-02-24 22:07:05,666 (monitor:541) INFO: {'Role': 'Server #', 'Round': 2, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:07:05,669 (server:330) INFO: Server: Starting evaluation at the end of round 2.[0m
[38;5;39m2023-02-24 22:07:05,671 (server:336) INFO: ----------- Starting a new training round (Round #3) -------------[0m
[33;20m2023-02-24 22:07:05,792 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:05,876 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:06,028 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:06,552 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:06,750 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:06,894 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:07,317 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:07,504 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:07,597 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:07,835 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:07:07,837 (server:590) INFO: {'Role': 'Server #', 'Round': 2, 'Results_weighted_avg': {'test_correct': 11.8, 'test_total': 22.0, 'test_acc': 0.536364, 'test_avg_loss': 1.689249, 'test_f1': 0.368335, 'test_loss': 53.364829}, 'Results_avg': {'test_correct': 11.8, 'test_total': 22.0, 'test_acc': 0.537472, 'test_avg_loss': 1.691059, 'test_f1': 0.341104, 'test_loss': 37.163477}, 'Results_fairness': {'test_correct': 11.8, 'test_total': 22.0, 'test_acc_std': 0.243233, 'test_acc_bottom_decile': 0.2, 'test_acc_top_decile': 0.875, 'test_acc_min': 0.111111, 'test_acc_max': 0.875, 'test_acc_bottom10%': 0.111111, 'test_acc_top10%': 0.875, 'test_acc_cos1': 0.91105, 'test_acc_entropy': 2.183756, 'test_avg_loss_std': 0.251959, 'test_avg_loss_bottom_decile': 1.5101, 'test_avg_loss_top_decile': 2.386543, 'test_avg_loss_min': 1.440332, 'test_avg_loss_max': 2.386543, 'test_avg_loss_bottom10%': 1.440332, 'test_avg_loss_top10%': 2.386543, 'test_avg_loss_cos1': 0.989082, 'test_avg_loss_entropy': 2.292351, 'test_f1_std': 0.150696, 'test_f1_bottom_decile': 0.174359, 'test_f1_top_decile': 0.671593, 'test_f1_min': 0.166667, 'test_f1_max': 0.671593, 'test_f1_bottom10%': 0.166667, 'test_f1_top10%': 0.671593, 'test_f1_cos1': 0.914711, 'test_f1_entropy': 2.208833, 'test_loss_std': 24.411415, 'test_loss_bottom_decile': 11.522656, 'test_loss_top_decile': 83.751392, 'test_loss_min': 8.595955, 'test_loss_max': 83.751392, 'test_loss_bottom10%': 8.595955, 'test_loss_top10%': 83.751392, 'test_loss_cos1': 0.835812, 'test_loss_entropy': 2.091983}}[0m
[38;5;39m2023-02-24 22:07:09,366 (client:306) INFO: {'Role': 'Client #5', 'Round': 3, 'Results_raw': {'train_correct': 87.0, 'train_total': 110, 'train_acc': 0.790909, 'train_avg_loss': 0.609714, 'train_f1': 0.609266, 'train_loss': 67.068499}}[0m
[38;5;39m2023-02-24 22:07:10,317 (client:306) INFO: {'Role': 'Client #10', 'Round': 3, 'Results_raw': {'train_correct': 45.0, 'train_total': 62, 'train_acc': 0.725806, 'train_avg_loss': 0.646789, 'train_f1': 0.46506, 'train_loss': 40.100893}}[0m
[38;5;39m2023-02-24 22:07:10,477 (client:306) INFO: {'Role': 'Client #8', 'Round': 3, 'Results_raw': {'train_correct': 1.0, 'train_total': 14, 'train_acc': 0.071429, 'train_avg_loss': 2.389579, 'train_f1': 0.166667, 'train_loss': 33.454107}}[0m
[38;5;39m2023-02-24 22:07:11,925 (client:306) INFO: {'Role': 'Client #9', 'Round': 3, 'Results_raw': {'train_correct': 98.0, 'train_total': 138, 'train_acc': 0.710145, 'train_avg_loss': 0.537676, 'train_f1': 0.647959, 'train_loss': 74.199307}}[0m
[38;5;39m2023-02-24 22:07:12,280 (client:306) INFO: {'Role': 'Client #2', 'Round': 3, 'Results_raw': {'train_correct': 17.0, 'train_total': 32, 'train_acc': 0.53125, 'train_avg_loss': 1.164752, 'train_f1': 0.40308, 'train_loss': 37.27207}}[0m
[38;5;39m2023-02-24 22:07:12,433 (client:306) INFO: {'Role': 'Client #4', 'Round': 3, 'Results_raw': {'train_correct': 3.0, 'train_total': 5, 'train_acc': 0.6, 'train_avg_loss': 1.410772, 'train_f1': 0.375, 'train_loss': 7.053862}}[0m
[38;5;39m2023-02-24 22:07:15,020 (client:306) INFO: {'Role': 'Client #1', 'Round': 3, 'Results_raw': {'train_correct': 189.0, 'train_total': 216, 'train_acc': 0.875, 'train_avg_loss': 0.480364, 'train_f1': 0.311111, 'train_loss': 103.758582}}[0m
[38;5;39m2023-02-24 22:07:17,121 (client:306) INFO: {'Role': 'Client #3', 'Round': 3, 'Results_raw': {'train_correct': 164.0, 'train_total': 177, 'train_acc': 0.926554, 'train_avg_loss': 0.249394, 'train_f1': 0.381157, 'train_loss': 44.142784}}[0m
[38;5;39m2023-02-24 22:07:17,568 (client:306) INFO: {'Role': 'Client #7', 'Round': 3, 'Results_raw': {'train_correct': 27.0, 'train_total': 41, 'train_acc': 0.658537, 'train_avg_loss': 0.800604, 'train_f1': 0.317949, 'train_loss': 32.824774}}[0m
[38;5;39m2023-02-24 22:07:18,551 (client:306) INFO: {'Role': 'Client #6', 'Round': 3, 'Results_raw': {'train_correct': 37.0, 'train_total': 82, 'train_acc': 0.45122, 'train_avg_loss': 1.070979, 'train_f1': 0.292805, 'train_loss': 87.820238}}[0m
[38;5;39m2023-02-24 22:07:18,559 (monitor:541) INFO: {'Role': 'Server #', 'Round': 3, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:07:18,561 (server:330) INFO: Server: Starting evaluation at the end of round 3.[0m
[38;5;39m2023-02-24 22:07:18,563 (server:336) INFO: ----------- Starting a new training round (Round #4) -------------[0m
[33;20m2023-02-24 22:07:18,642 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:18,768 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:18,960 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:19,408 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:19,700 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:19,866 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:20,401 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:20,625 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:20,674 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:20,912 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:07:20,913 (server:590) INFO: {'Role': 'Server #', 'Round': 3, 'Results_weighted_avg': {'test_correct': 13.7, 'test_total': 22.0, 'test_acc': 0.622727, 'test_avg_loss': 1.04804, 'test_f1': 0.435022, 'test_loss': 31.842221}, 'Results_avg': {'test_correct': 13.7, 'test_total': 22.0, 'test_acc': 0.583407, 'test_avg_loss': 1.07391, 'test_f1': 0.399213, 'test_loss': 23.056886}, 'Results_fairness': {'test_correct': 13.7, 'test_total': 22.0, 'test_acc_std': 0.203839, 'test_acc_bottom_decile': 0.222222, 'test_acc_top_decile': 0.82, 'test_acc_min': 0.2, 'test_acc_max': 0.82, 'test_acc_bottom10%': 0.2, 'test_acc_top10%': 0.82, 'test_acc_cos1': 0.944036, 'test_acc_entropy': 2.229908, 'test_avg_loss_std': 0.33463, 'test_avg_loss_bottom_decile': 0.822381, 'test_avg_loss_top_decile': 2.023792, 'test_avg_loss_min': 0.810518, 'test_avg_loss_max': 2.023792, 'test_avg_loss_bottom10%': 0.810518, 'test_avg_loss_top10%': 2.023792, 'test_avg_loss_cos1': 0.954724, 'test_avg_loss_entropy': 2.261528, 'test_f1_std': 0.213226, 'test_f1_bottom_decile': 0.177778, 'test_f1_top_decile': 0.819928, 'test_f1_min': 0.166667, 'test_f1_max': 0.819928, 'test_f1_bottom10%': 0.166667, 'test_f1_top10%': 0.819928, 'test_f1_cos1': 0.882066, 'test_f1_entropy': 2.169568, 'test_loss_std': 14.470953, 'test_loss_bottom_decile': 6.579045, 'test_loss_top_decile': 49.839687, 'test_loss_min': 5.332815, 'test_loss_max': 49.839687, 'test_loss_bottom10%': 5.332815, 'test_loss_top10%': 49.839687, 'test_loss_cos1': 0.847, 'test_loss_entropy': 2.099106}}[0m
[38;5;39m2023-02-24 22:07:21,396 (client:306) INFO: {'Role': 'Client #7', 'Round': 4, 'Results_raw': {'train_correct': 30.0, 'train_total': 41, 'train_acc': 0.731707, 'train_avg_loss': 0.6521, 'train_f1': 0.35098, 'train_loss': 26.736088}}[0m
[38;5;39m2023-02-24 22:07:22,715 (client:306) INFO: {'Role': 'Client #9', 'Round': 4, 'Results_raw': {'train_correct': 104.0, 'train_total': 138, 'train_acc': 0.753623, 'train_avg_loss': 0.4937, 'train_f1': 0.696193, 'train_loss': 68.13061}}[0m
[38;5;39m2023-02-24 22:07:25,014 (client:306) INFO: {'Role': 'Client #3', 'Round': 4, 'Results_raw': {'train_correct': 164.0, 'train_total': 177, 'train_acc': 0.926554, 'train_avg_loss': 0.270798, 'train_f1': 0.320626, 'train_loss': 47.931225}}[0m
[38;5;39m2023-02-24 22:07:25,351 (client:306) INFO: {'Role': 'Client #2', 'Round': 4, 'Results_raw': {'train_correct': 17.0, 'train_total': 32, 'train_acc': 0.53125, 'train_avg_loss': 1.171289, 'train_f1': 0.401401, 'train_loss': 37.481236}}[0m
[38;5;39m2023-02-24 22:07:26,623 (client:306) INFO: {'Role': 'Client #5', 'Round': 4, 'Results_raw': {'train_correct': 85.0, 'train_total': 110, 'train_acc': 0.772727, 'train_avg_loss': 0.511836, 'train_f1': 0.593856, 'train_loss': 56.301972}}[0m
[38;5;39m2023-02-24 22:07:27,539 (client:306) INFO: {'Role': 'Client #6', 'Round': 4, 'Results_raw': {'train_correct': 40.0, 'train_total': 82, 'train_acc': 0.487805, 'train_avg_loss': 0.937247, 'train_f1': 0.334603, 'train_loss': 76.854269}}[0m
[38;5;39m2023-02-24 22:07:29,609 (client:306) INFO: {'Role': 'Client #1', 'Round': 4, 'Results_raw': {'train_correct': 191.0, 'train_total': 216, 'train_acc': 0.884259, 'train_avg_loss': 0.451443, 'train_f1': 0.313629, 'train_loss': 97.511585}}[0m
[38;5;39m2023-02-24 22:07:29,701 (client:306) INFO: {'Role': 'Client #4', 'Round': 4, 'Results_raw': {'train_correct': 3.0, 'train_total': 5, 'train_acc': 0.6, 'train_avg_loss': 1.32235, 'train_f1': 0.375, 'train_loss': 6.61175}}[0m
[38;5;39m2023-02-24 22:07:30,414 (client:306) INFO: {'Role': 'Client #10', 'Round': 4, 'Results_raw': {'train_correct': 46.0, 'train_total': 62, 'train_acc': 0.741935, 'train_avg_loss': 0.618739, 'train_f1': 0.490108, 'train_loss': 38.361834}}[0m
[38;5;39m2023-02-24 22:07:30,589 (client:306) INFO: {'Role': 'Client #8', 'Round': 4, 'Results_raw': {'train_correct': 2.0, 'train_total': 14, 'train_acc': 0.142857, 'train_avg_loss': 2.375131, 'train_f1': 0.266667, 'train_loss': 33.251839}}[0m
[38;5;39m2023-02-24 22:07:30,597 (monitor:541) INFO: {'Role': 'Server #', 'Round': 4, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:07:30,603 (server:330) INFO: Server: Starting evaluation at the end of round 4.[0m
[38;5;39m2023-02-24 22:07:30,610 (server:336) INFO: ----------- Starting a new training round (Round #5) -------------[0m
[33;20m2023-02-24 22:07:30,731 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:30,818 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:30,988 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:31,534 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:31,900 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:32,199 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:32,814 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:33,048 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:33,097 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:33,345 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:07:33,346 (server:590) INFO: {'Role': 'Server #', 'Round': 4, 'Results_weighted_avg': {'test_correct': 16.5, 'test_total': 22.0, 'test_acc': 0.75, 'test_avg_loss': 0.816634, 'test_f1': 0.502149, 'test_loss': 23.291189}, 'Results_avg': {'test_correct': 16.5, 'test_total': 22.0, 'test_acc': 0.715123, 'test_avg_loss': 0.883635, 'test_f1': 0.479847, 'test_loss': 17.965959}, 'Results_fairness': {'test_correct': 16.5, 'test_total': 22.0, 'test_acc_std': 0.207598, 'test_acc_bottom_decile': 0.5, 'test_acc_top_decile': 1.0, 'test_acc_min': 0.277778, 'test_acc_max': 1.0, 'test_acc_bottom10%': 0.277778, 'test_acc_top10%': 1.0, 'test_acc_cos1': 0.960353, 'test_acc_entropy': 2.255646, 'test_avg_loss_std': 0.312989, 'test_avg_loss_bottom_decile': 0.588267, 'test_avg_loss_top_decile': 1.680472, 'test_avg_loss_min': 0.582163, 'test_avg_loss_max': 1.680472, 'test_avg_loss_bottom10%': 0.582163, 'test_avg_loss_top10%': 1.680472, 'test_avg_loss_cos1': 0.942615, 'test_avg_loss_entropy': 2.246331, 'test_f1_std': 0.259455, 'test_f1_bottom_decile': 0.303704, 'test_f1_top_decile': 1.0, 'test_f1_min': 0.175439, 'test_f1_max': 1.0, 'test_f1_bottom10%': 0.175439, 'test_f1_top10%': 1.0, 'test_f1_cos1': 0.879647, 'test_f1_entropy': 2.170922, 'test_loss_std': 9.826706, 'test_loss_bottom_decile': 6.497394, 'test_loss_top_decile': 30.680941, 'test_loss_min': 3.615119, 'test_loss_max': 30.680941, 'test_loss_bottom10%': 3.615119, 'test_loss_top10%': 30.680941, 'test_loss_cos1': 0.877339, 'test_loss_entropy': 2.137987}}[0m
[38;5;39m2023-02-24 22:07:34,581 (client:306) INFO: {'Role': 'Client #5', 'Round': 5, 'Results_raw': {'train_correct': 91.0, 'train_total': 110, 'train_acc': 0.827273, 'train_avg_loss': 0.452849, 'train_f1': 0.622676, 'train_loss': 49.813336}}[0m
[38;5;39m2023-02-24 22:07:34,674 (client:306) INFO: {'Role': 'Client #4', 'Round': 5, 'Results_raw': {'train_correct': 2.0, 'train_total': 5, 'train_acc': 0.4, 'train_avg_loss': 1.300603, 'train_f1': 0.222222, 'train_loss': 6.503013}}[0m
[38;5;39m2023-02-24 22:07:35,469 (client:306) INFO: {'Role': 'Client #6', 'Round': 5, 'Results_raw': {'train_correct': 42.0, 'train_total': 82, 'train_acc': 0.512195, 'train_avg_loss': 1.010462, 'train_f1': 0.355703, 'train_loss': 82.857881}}[0m
[38;5;39m2023-02-24 22:07:36,008 (client:306) INFO: {'Role': 'Client #10', 'Round': 5, 'Results_raw': {'train_correct': 46.0, 'train_total': 62, 'train_acc': 0.741935, 'train_avg_loss': 0.616531, 'train_f1': 0.482163, 'train_loss': 38.224909}}[0m
[38;5;39m2023-02-24 22:07:36,325 (client:306) INFO: {'Role': 'Client #2', 'Round': 5, 'Results_raw': {'train_correct': 15.0, 'train_total': 32, 'train_acc': 0.46875, 'train_avg_loss': 1.116625, 'train_f1': 0.354809, 'train_loss': 35.732004}}[0m
[38;5;39m2023-02-24 22:07:37,733 (client:306) INFO: {'Role': 'Client #9', 'Round': 5, 'Results_raw': {'train_correct': 109.0, 'train_total': 138, 'train_acc': 0.789855, 'train_avg_loss': 0.52299, 'train_f1': 0.725043, 'train_loss': 72.172632}}[0m
[38;5;39m2023-02-24 22:07:37,938 (client:306) INFO: {'Role': 'Client #8', 'Round': 5, 'Results_raw': {'train_correct': 2.0, 'train_total': 14, 'train_acc': 0.142857, 'train_avg_loss': 1.840706, 'train_f1': 0.190476, 'train_loss': 25.769886}}[0m
[38;5;39m2023-02-24 22:07:41,169 (client:306) INFO: {'Role': 'Client #1', 'Round': 5, 'Results_raw': {'train_correct': 192.0, 'train_total': 216, 'train_acc': 0.888889, 'train_avg_loss': 0.43252, 'train_f1': 0.314496, 'train_loss': 93.424346}}[0m
[38;5;39m2023-02-24 22:07:43,153 (client:306) INFO: {'Role': 'Client #3', 'Round': 5, 'Results_raw': {'train_correct': 164.0, 'train_total': 177, 'train_acc': 0.926554, 'train_avg_loss': 0.214128, 'train_f1': 0.320626, 'train_loss': 37.900617}}[0m
[38;5;39m2023-02-24 22:07:43,596 (client:306) INFO: {'Role': 'Client #7', 'Round': 5, 'Results_raw': {'train_correct': 33.0, 'train_total': 41, 'train_acc': 0.804878, 'train_avg_loss': 0.642751, 'train_f1': 0.395708, 'train_loss': 26.352802}}[0m
[38;5;39m2023-02-24 22:07:43,621 (monitor:541) INFO: {'Role': 'Server #', 'Round': 5, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:07:43,623 (server:330) INFO: Server: Starting evaluation at the end of round 5.[0m
[38;5;39m2023-02-24 22:07:43,626 (server:336) INFO: ----------- Starting a new training round (Round #6) -------------[0m
[33;20m2023-02-24 22:07:43,704 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:43,855 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:44,068 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:44,498 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:44,675 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:44,827 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:45,218 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:45,419 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:45,469 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:45,772 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:07:45,773 (server:590) INFO: {'Role': 'Server #', 'Round': 5, 'Results_weighted_avg': {'test_correct': 14.7, 'test_total': 22.0, 'test_acc': 0.668182, 'test_avg_loss': 0.793799, 'test_f1': 0.464818, 'test_loss': 22.783022}, 'Results_avg': {'test_correct': 14.7, 'test_total': 22.0, 'test_acc': 0.633801, 'test_avg_loss': 0.839415, 'test_f1': 0.437939, 'test_loss': 17.463585}, 'Results_fairness': {'test_correct': 14.7, 'test_total': 22.0, 'test_acc_std': 0.184911, 'test_acc_bottom_decile': 0.4, 'test_acc_top_decile': 0.888889, 'test_acc_min': 0.277778, 'test_acc_max': 0.888889, 'test_acc_bottom10%': 0.277778, 'test_acc_top10%': 0.888889, 'test_acc_cos1': 0.959979, 'test_acc_entropy': 2.256596, 'test_avg_loss_std': 0.442143, 'test_avg_loss_bottom_decile': 0.558686, 'test_avg_loss_top_decile': 2.088883, 'test_avg_loss_min': 0.469686, 'test_avg_loss_max': 2.088883, 'test_avg_loss_bottom10%': 0.469686, 'test_avg_loss_top10%': 2.088883, 'test_avg_loss_cos1': 0.884768, 'test_avg_loss_entropy': 2.193214, 'test_f1_std': 0.231587, 'test_f1_bottom_decile': 0.236559, 'test_f1_top_decile': 0.883117, 'test_f1_min': 0.196078, 'test_f1_max': 0.883117, 'test_f1_bottom10%': 0.196078, 'test_f1_top10%': 0.883117, 'test_f1_cos1': 0.884008, 'test_f1_entropy': 2.17626, 'test_loss_std': 11.553613, 'test_loss_bottom_decile': 4.708552, 'test_loss_top_decile': 37.599891, 'test_loss_min': 4.048041, 'test_loss_max': 37.599891, 'test_loss_bottom10%': 4.048041, 'test_loss_top10%': 37.599891, 'test_loss_cos1': 0.834002, 'test_loss_entropy': 2.078116}}[0m
[38;5;39m2023-02-24 22:07:45,837 (client:306) INFO: {'Role': 'Client #4', 'Round': 6, 'Results_raw': {'train_correct': 2.0, 'train_total': 5, 'train_acc': 0.4, 'train_avg_loss': 1.414597, 'train_f1': 0.190476, 'train_loss': 7.072986}}[0m
[38;5;39m2023-02-24 22:07:46,157 (client:306) INFO: {'Role': 'Client #2', 'Round': 6, 'Results_raw': {'train_correct': 19.0, 'train_total': 32, 'train_acc': 0.59375, 'train_avg_loss': 1.039033, 'train_f1': 0.447619, 'train_loss': 33.249066}}[0m
[38;5;39m2023-02-24 22:07:46,915 (client:306) INFO: {'Role': 'Client #10', 'Round': 6, 'Results_raw': {'train_correct': 48.0, 'train_total': 62, 'train_acc': 0.774194, 'train_avg_loss': 0.579045, 'train_f1': 0.507558, 'train_loss': 35.900786}}[0m
[38;5;39m2023-02-24 22:07:48,236 (client:306) INFO: {'Role': 'Client #9', 'Round': 6, 'Results_raw': {'train_correct': 112.0, 'train_total': 138, 'train_acc': 0.811594, 'train_avg_loss': 0.42173, 'train_f1': 0.763947, 'train_loss': 58.198751}}[0m
[38;5;39m2023-02-24 22:07:49,426 (client:306) INFO: {'Role': 'Client #6', 'Round': 6, 'Results_raw': {'train_correct': 36.0, 'train_total': 82, 'train_acc': 0.439024, 'train_avg_loss': 1.038281, 'train_f1': 0.294253, 'train_loss': 85.13908}}[0m
[38;5;39m2023-02-24 22:07:49,942 (client:306) INFO: {'Role': 'Client #7', 'Round': 6, 'Results_raw': {'train_correct': 32.0, 'train_total': 41, 'train_acc': 0.780488, 'train_avg_loss': 0.72869, 'train_f1': 0.373591, 'train_loss': 29.876302}}[0m
[38;5;39m2023-02-24 22:07:52,063 (client:306) INFO: {'Role': 'Client #3', 'Round': 6, 'Results_raw': {'train_correct': 162.0, 'train_total': 177, 'train_acc': 0.915254, 'train_avg_loss': 0.25885, 'train_f1': 0.318584, 'train_loss': 45.816473}}[0m
[38;5;39m2023-02-24 22:07:54,471 (client:306) INFO: {'Role': 'Client #1', 'Round': 6, 'Results_raw': {'train_correct': 191.0, 'train_total': 216, 'train_acc': 0.884259, 'train_avg_loss': 0.406522, 'train_f1': 0.313629, 'train_loss': 87.808844}}[0m
[38;5;39m2023-02-24 22:07:54,741 (client:306) INFO: {'Role': 'Client #8', 'Round': 6, 'Results_raw': {'train_correct': 1.0, 'train_total': 14, 'train_acc': 0.071429, 'train_avg_loss': 2.441477, 'train_f1': 0.166667, 'train_loss': 34.180676}}[0m
[38;5;39m2023-02-24 22:07:56,238 (client:306) INFO: {'Role': 'Client #5', 'Round': 6, 'Results_raw': {'train_correct': 90.0, 'train_total': 110, 'train_acc': 0.818182, 'train_avg_loss': 0.439427, 'train_f1': 0.652119, 'train_loss': 48.336965}}[0m
[38;5;39m2023-02-24 22:07:56,249 (monitor:541) INFO: {'Role': 'Server #', 'Round': 6, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:07:56,252 (server:330) INFO: Server: Starting evaluation at the end of round 6.[0m
[38;5;39m2023-02-24 22:07:56,254 (server:336) INFO: ----------- Starting a new training round (Round #7) -------------[0m
[33;20m2023-02-24 22:07:56,336 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:56,435 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:56,601 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:57,041 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:57,218 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:57,474 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:58,219 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:58,608 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:58,698 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:07:59,037 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:07:59,039 (server:590) INFO: {'Role': 'Server #', 'Round': 6, 'Results_weighted_avg': {'test_correct': 16.4, 'test_total': 22.0, 'test_acc': 0.745455, 'test_avg_loss': 0.734686, 'test_f1': 0.524639, 'test_loss': 20.265065}, 'Results_avg': {'test_correct': 16.4, 'test_total': 22.0, 'test_acc': 0.714145, 'test_avg_loss': 0.804942, 'test_f1': 0.49722, 'test_loss': 16.1631}, 'Results_fairness': {'test_correct': 16.4, 'test_total': 22.0, 'test_acc_std': 0.193, 'test_acc_bottom_decile': 0.565217, 'test_acc_top_decile': 0.96, 'test_acc_min': 0.277778, 'test_acc_max': 0.96, 'test_acc_bottom10%': 0.277778, 'test_acc_top10%': 0.96, 'test_acc_cos1': 0.965367, 'test_acc_entropy': 2.261199, 'test_avg_loss_std': 0.430506, 'test_avg_loss_bottom_decile': 0.513044, 'test_avg_loss_top_decile': 2.001235, 'test_avg_loss_min': 0.393968, 'test_avg_loss_max': 2.001235, 'test_avg_loss_bottom10%': 0.393968, 'test_avg_loss_top10%': 2.001235, 'test_avg_loss_cos1': 0.881805, 'test_avg_loss_entropy': 2.187145, 'test_f1_std': 0.232361, 'test_f1_bottom_decile': 0.286822, 'test_f1_top_decile': 0.958949, 'test_f1_min': 0.196078, 'test_f1_max': 0.958949, 'test_f1_bottom10%': 0.196078, 'test_f1_top10%': 0.958949, 'test_f1_cos1': 0.905956, 'test_f1_entropy': 2.200514, 'test_loss_std': 10.24848, 'test_loss_bottom_decile': 4.754819, 'test_loss_top_decile': 36.022238, 'test_loss_min': 3.689631, 'test_loss_max': 36.022238, 'test_loss_bottom10%': 3.689631, 'test_loss_top10%': 36.022238, 'test_loss_cos1': 0.844539, 'test_loss_entropy': 2.097631}}[0m
[38;5;39m2023-02-24 22:07:59,159 (client:306) INFO: {'Role': 'Client #4', 'Round': 7, 'Results_raw': {'train_correct': 3.0, 'train_total': 5, 'train_acc': 0.6, 'train_avg_loss': 1.322608, 'train_f1': 0.285714, 'train_loss': 6.613041}}[0m
[38;5;39m2023-02-24 22:08:01,441 (client:306) INFO: {'Role': 'Client #3', 'Round': 7, 'Results_raw': {'train_correct': 167.0, 'train_total': 177, 'train_acc': 0.943503, 'train_avg_loss': 0.221179, 'train_f1': 0.40692, 'train_loss': 39.148657}}[0m
[38;5;39m2023-02-24 22:08:01,768 (client:306) INFO: {'Role': 'Client #2', 'Round': 7, 'Results_raw': {'train_correct': 15.0, 'train_total': 32, 'train_acc': 0.46875, 'train_avg_loss': 1.189323, 'train_f1': 0.328173, 'train_loss': 38.058338}}[0m
[38;5;39m2023-02-24 22:08:03,332 (client:306) INFO: {'Role': 'Client #9', 'Round': 7, 'Results_raw': {'train_correct': 109.0, 'train_total': 138, 'train_acc': 0.789855, 'train_avg_loss': 0.443057, 'train_f1': 0.734527, 'train_loss': 61.141872}}[0m
[38;5;39m2023-02-24 22:08:03,496 (client:306) INFO: {'Role': 'Client #8', 'Round': 7, 'Results_raw': {'train_correct': 2.0, 'train_total': 14, 'train_acc': 0.142857, 'train_avg_loss': 2.256684, 'train_f1': 0.266667, 'train_loss': 31.593577}}[0m
[38;5;39m2023-02-24 22:08:04,146 (client:306) INFO: {'Role': 'Client #10', 'Round': 7, 'Results_raw': {'train_correct': 51.0, 'train_total': 62, 'train_acc': 0.822581, 'train_avg_loss': 0.521241, 'train_f1': 0.544496, 'train_loss': 32.316914}}[0m
[38;5;39m2023-02-24 22:08:04,678 (client:306) INFO: {'Role': 'Client #7', 'Round': 7, 'Results_raw': {'train_correct': 32.0, 'train_total': 41, 'train_acc': 0.780488, 'train_avg_loss': 0.752575, 'train_f1': 0.378571, 'train_loss': 30.855565}}[0m
[38;5;39m2023-02-24 22:08:05,622 (client:306) INFO: {'Role': 'Client #6', 'Round': 7, 'Results_raw': {'train_correct': 40.0, 'train_total': 82, 'train_acc': 0.487805, 'train_avg_loss': 0.936155, 'train_f1': 0.338427, 'train_loss': 76.764682}}[0m
[38;5;39m2023-02-24 22:08:08,299 (client:306) INFO: {'Role': 'Client #1', 'Round': 7, 'Results_raw': {'train_correct': 192.0, 'train_total': 216, 'train_acc': 0.888889, 'train_avg_loss': 0.406435, 'train_f1': 0.313725, 'train_loss': 87.790034}}[0m
[38;5;39m2023-02-24 22:08:09,604 (client:306) INFO: {'Role': 'Client #5', 'Round': 7, 'Results_raw': {'train_correct': 96.0, 'train_total': 110, 'train_acc': 0.872727, 'train_avg_loss': 0.408014, 'train_f1': 0.756483, 'train_loss': 44.881514}}[0m
[38;5;39m2023-02-24 22:08:09,624 (monitor:541) INFO: {'Role': 'Server #', 'Round': 7, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:08:09,626 (server:330) INFO: Server: Starting evaluation at the end of round 7.[0m
[38;5;39m2023-02-24 22:08:09,628 (server:336) INFO: ----------- Starting a new training round (Round #8) -------------[0m
[33;20m2023-02-24 22:08:09,778 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:09,938 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:10,084 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:10,581 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:10,790 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:10,934 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:11,336 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:11,524 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:11,598 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:11,842 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:08:11,843 (server:590) INFO: {'Role': 'Server #', 'Round': 7, 'Results_weighted_avg': {'test_correct': 16.7, 'test_total': 22.0, 'test_acc': 0.759091, 'test_avg_loss': 0.701759, 'test_f1': 0.529946, 'test_loss': 19.103298}, 'Results_avg': {'test_correct': 16.7, 'test_total': 22.0, 'test_acc': 0.719616, 'test_avg_loss': 0.78031, 'test_f1': 0.500903, 'test_loss': 15.438704}, 'Results_fairness': {'test_correct': 16.7, 'test_total': 22.0, 'test_acc_std': 0.195911, 'test_acc_bottom_decile': 0.55, 'test_acc_top_decile': 0.96, 'test_acc_min': 0.277778, 'test_acc_max': 0.96, 'test_acc_bottom10%': 0.277778, 'test_acc_top10%': 0.96, 'test_acc_cos1': 0.964882, 'test_acc_entropy': 2.260411, 'test_avg_loss_std': 0.405346, 'test_avg_loss_bottom_decile': 0.440533, 'test_avg_loss_top_decile': 1.867505, 'test_avg_loss_min': 0.390306, 'test_avg_loss_max': 1.867505, 'test_avg_loss_bottom10%': 0.390306, 'test_avg_loss_top10%': 1.867505, 'test_avg_loss_cos1': 0.88741, 'test_avg_loss_entropy': 2.189239, 'test_f1_std': 0.23349, 'test_f1_bottom_decile': 0.299625, 'test_f1_top_decile': 0.958949, 'test_f1_min': 0.185185, 'test_f1_max': 0.958949, 'test_f1_bottom10%': 0.185185, 'test_f1_top10%': 0.958949, 'test_f1_cos1': 0.906366, 'test_f1_entropy': 2.19954, 'test_loss_std': 9.406614, 'test_loss_bottom_decile': 4.630849, 'test_loss_top_decile': 33.615089, 'test_loss_min': 3.431371, 'test_loss_max': 33.615089, 'test_loss_bottom10%': 3.431371, 'test_loss_top10%': 33.615089, 'test_loss_cos1': 0.853974, 'test_loss_entropy': 2.110235}}[0m
[38;5;39m2023-02-24 22:08:14,559 (client:306) INFO: {'Role': 'Client #1', 'Round': 8, 'Results_raw': {'train_correct': 188.0, 'train_total': 216, 'train_acc': 0.87037, 'train_avg_loss': 0.363734, 'train_f1': 0.310231, 'train_loss': 78.566441}}[0m
[38;5;39m2023-02-24 22:08:16,071 (client:306) INFO: {'Role': 'Client #9', 'Round': 8, 'Results_raw': {'train_correct': 113.0, 'train_total': 138, 'train_acc': 0.818841, 'train_avg_loss': 0.430798, 'train_f1': 0.77832, 'train_loss': 59.450183}}[0m
[38;5;39m2023-02-24 22:08:16,995 (client:306) INFO: {'Role': 'Client #6', 'Round': 8, 'Results_raw': {'train_correct': 44.0, 'train_total': 82, 'train_acc': 0.536585, 'train_avg_loss': 0.929551, 'train_f1': 0.374319, 'train_loss': 76.223198}}[0m
[38;5;39m2023-02-24 22:08:18,822 (client:306) INFO: {'Role': 'Client #3', 'Round': 8, 'Results_raw': {'train_correct': 165.0, 'train_total': 177, 'train_acc': 0.932203, 'train_avg_loss': 0.21821, 'train_f1': 0.321637, 'train_loss': 38.623148}}[0m
[38;5;39m2023-02-24 22:08:19,287 (client:306) INFO: {'Role': 'Client #2', 'Round': 8, 'Results_raw': {'train_correct': 19.0, 'train_total': 32, 'train_acc': 0.59375, 'train_avg_loss': 0.976874, 'train_f1': 0.445378, 'train_loss': 31.259966}}[0m
[38;5;39m2023-02-24 22:08:20,217 (client:306) INFO: {'Role': 'Client #10', 'Round': 8, 'Results_raw': {'train_correct': 51.0, 'train_total': 62, 'train_acc': 0.822581, 'train_avg_loss': 0.473828, 'train_f1': 0.546584, 'train_loss': 29.377318}}[0m
[38;5;39m2023-02-24 22:08:21,251 (client:306) INFO: {'Role': 'Client #5', 'Round': 8, 'Results_raw': {'train_correct': 94.0, 'train_total': 110, 'train_acc': 0.854545, 'train_avg_loss': 0.362498, 'train_f1': 0.707447, 'train_loss': 39.874731}}[0m
[38;5;39m2023-02-24 22:08:21,429 (client:306) INFO: {'Role': 'Client #8', 'Round': 8, 'Results_raw': {'train_correct': 2.0, 'train_total': 14, 'train_acc': 0.142857, 'train_avg_loss': 2.083375, 'train_f1': 0.266667, 'train_loss': 29.167246}}[0m
[38;5;39m2023-02-24 22:08:21,497 (client:306) INFO: {'Role': 'Client #4', 'Round': 8, 'Results_raw': {'train_correct': 2.0, 'train_total': 5, 'train_acc': 0.4, 'train_avg_loss': 1.252892, 'train_f1': 0.222222, 'train_loss': 6.264461}}[0m
[38;5;39m2023-02-24 22:08:21,844 (client:306) INFO: {'Role': 'Client #7', 'Round': 8, 'Results_raw': {'train_correct': 33.0, 'train_total': 41, 'train_acc': 0.804878, 'train_avg_loss': 0.677105, 'train_f1': 0.407407, 'train_loss': 27.761322}}[0m
[38;5;39m2023-02-24 22:08:21,852 (monitor:541) INFO: {'Role': 'Server #', 'Round': 8, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:08:21,855 (server:330) INFO: Server: Starting evaluation at the end of round 8.[0m
[38;5;39m2023-02-24 22:08:21,857 (server:336) INFO: ----------- Starting a new training round (Round #9) -------------[0m
[33;20m2023-02-24 22:08:21,998 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:22,150 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:22,298 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:22,771 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:23,080 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:23,243 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:23,650 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:23,874 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:23,935 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:24,127 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:08:24,129 (server:590) INFO: {'Role': 'Server #', 'Round': 8, 'Results_weighted_avg': {'test_correct': 15.6, 'test_total': 22.0, 'test_acc': 0.709091, 'test_avg_loss': 0.714109, 'test_f1': 0.508582, 'test_loss': 20.998282}, 'Results_avg': {'test_correct': 15.6, 'test_total': 22.0, 'test_acc': 0.672985, 'test_avg_loss': 0.761645, 'test_f1': 0.484173, 'test_loss': 15.710389}, 'Results_fairness': {'test_correct': 15.6, 'test_total': 22.0, 'test_acc_std': 0.20453, 'test_acc_bottom_decile': 0.4, 'test_acc_top_decile': 0.888889, 'test_acc_min': 0.222222, 'test_acc_max': 0.888889, 'test_acc_bottom10%': 0.222222, 'test_acc_top10%': 0.888889, 'test_acc_cos1': 0.956789, 'test_acc_entropy': 2.248224, 'test_avg_loss_std': 0.336708, 'test_avg_loss_bottom_decile': 0.531046, 'test_avg_loss_top_decile': 1.679372, 'test_avg_loss_min': 0.438441, 'test_avg_loss_max': 1.679372, 'test_avg_loss_bottom10%': 0.438441, 'test_avg_loss_top10%': 1.679372, 'test_avg_loss_cos1': 0.914612, 'test_avg_loss_entropy': 2.221029, 'test_f1_std': 0.228129, 'test_f1_bottom_decile': 0.258333, 'test_f1_top_decile': 0.883117, 'test_f1_min': 0.190476, 'test_f1_max': 0.883117, 'test_f1_bottom10%': 0.190476, 'test_f1_top10%': 0.883117, 'test_f1_cos1': 0.904615, 'test_f1_entropy': 2.195389, 'test_loss_std': 10.27012, 'test_loss_bottom_decile': 4.779415, 'test_loss_top_decile': 35.240445, 'test_loss_min': 4.661811, 'test_loss_max': 35.240445, 'test_loss_bottom10%': 4.661811, 'test_loss_top10%': 35.240445, 'test_loss_cos1': 0.83702, 'test_loss_entropy': 2.091061}}[0m
[38;5;39m2023-02-24 22:08:24,204 (client:306) INFO: {'Role': 'Client #4', 'Round': 9, 'Results_raw': {'train_correct': 2.0, 'train_total': 5, 'train_acc': 0.4, 'train_avg_loss': 1.100451, 'train_f1': 0.190476, 'train_loss': 5.502254}}[0m
[38;5;39m2023-02-24 22:08:26,815 (client:306) INFO: {'Role': 'Client #3', 'Round': 9, 'Results_raw': {'train_correct': 164.0, 'train_total': 177, 'train_acc': 0.926554, 'train_avg_loss': 0.221498, 'train_f1': 0.377055, 'train_loss': 39.205066}}[0m
[38;5;39m2023-02-24 22:08:29,272 (client:306) INFO: {'Role': 'Client #1', 'Round': 9, 'Results_raw': {'train_correct': 191.0, 'train_total': 216, 'train_acc': 0.884259, 'train_avg_loss': 0.394197, 'train_f1': 0.312858, 'train_loss': 85.146603}}[0m
[38;5;39m2023-02-24 22:08:29,409 (client:306) INFO: {'Role': 'Client #8', 'Round': 9, 'Results_raw': {'train_correct': 1.0, 'train_total': 14, 'train_acc': 0.071429, 'train_avg_loss': 2.017604, 'train_f1': 0.166667, 'train_loss': 28.246461}}[0m
[38;5;39m2023-02-24 22:08:30,099 (client:306) INFO: {'Role': 'Client #10', 'Round': 9, 'Results_raw': {'train_correct': 47.0, 'train_total': 62, 'train_acc': 0.758065, 'train_avg_loss': 0.560172, 'train_f1': 0.49888, 'train_loss': 34.73069}}[0m
[38;5;39m2023-02-24 22:08:30,490 (client:306) INFO: {'Role': 'Client #7', 'Round': 9, 'Results_raw': {'train_correct': 32.0, 'train_total': 41, 'train_acc': 0.780488, 'train_avg_loss': 0.610576, 'train_f1': 0.427451, 'train_loss': 25.033612}}[0m
[38;5;39m2023-02-24 22:08:31,805 (client:306) INFO: {'Role': 'Client #5', 'Round': 9, 'Results_raw': {'train_correct': 89.0, 'train_total': 110, 'train_acc': 0.809091, 'train_avg_loss': 0.418512, 'train_f1': 0.706667, 'train_loss': 46.036346}}[0m
[38;5;39m2023-02-24 22:08:33,870 (client:306) INFO: {'Role': 'Client #9', 'Round': 9, 'Results_raw': {'train_correct': 110.0, 'train_total': 138, 'train_acc': 0.797101, 'train_avg_loss': 0.471458, 'train_f1': 0.741504, 'train_loss': 65.061229}}[0m
[38;5;39m2023-02-24 22:08:34,420 (client:306) INFO: {'Role': 'Client #2', 'Round': 9, 'Results_raw': {'train_correct': 19.0, 'train_total': 32, 'train_acc': 0.59375, 'train_avg_loss': 0.987949, 'train_f1': 0.497425, 'train_loss': 31.614384}}[0m
[38;5;39m2023-02-24 22:08:35,603 (client:306) INFO: {'Role': 'Client #6', 'Round': 9, 'Results_raw': {'train_correct': 46.0, 'train_total': 82, 'train_acc': 0.560976, 'train_avg_loss': 0.815839, 'train_f1': 0.380471, 'train_loss': 66.898761}}[0m
[38;5;39m2023-02-24 22:08:35,626 (monitor:541) INFO: {'Role': 'Server #', 'Round': 9, 'Results_model_metric': {}}[0m
[38;5;39m2023-02-24 22:08:35,642 (server:347) INFO: Server: Training is finished! Starting evaluation.[0m
[33;20m2023-02-24 22:08:35,751 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:35,839 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:36,040 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:36,527 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:36,713 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:36,866 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:37,288 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:37,488 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:37,543 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[33;20m2023-02-24 22:08:37,754 (context:294) WARNING: No val_data or val_loader in the trainer, will skip evaluation.If this is not the case you want, please check whether there is typo for the name[0m
[38;5;39m2023-02-24 22:08:37,756 (server:590) INFO: {'Role': 'Server #', 'Round': 9, 'Results_weighted_avg': {'test_correct': 16.5, 'test_total': 22.0, 'test_acc': 0.75, 'test_avg_loss': 0.663025, 'test_f1': 0.528913, 'test_loss': 18.106686}, 'Results_avg': {'test_correct': 16.5, 'test_total': 22.0, 'test_acc': 0.700534, 'test_avg_loss': 0.733928, 'test_f1': 0.49601, 'test_loss': 14.586554}, 'Results_fairness': {'test_correct': 16.5, 'test_total': 22.0, 'test_acc_std': 0.191657, 'test_acc_bottom_decile': 0.6, 'test_acc_top_decile': 0.96, 'test_acc_min': 0.277778, 'test_acc_max': 0.96, 'test_acc_bottom10%': 0.277778, 'test_acc_top10%': 0.96, 'test_acc_cos1': 0.964553, 'test_acc_entropy': 2.26093, 'test_avg_loss_std': 0.39887, 'test_avg_loss_bottom_decile': 0.450046, 'test_avg_loss_top_decile': 1.832435, 'test_avg_loss_min': 0.353701, 'test_avg_loss_max': 1.832435, 'test_avg_loss_bottom10%': 0.353701, 'test_avg_loss_top10%': 1.832435, 'test_avg_loss_cos1': 0.878626, 'test_avg_loss_entropy': 2.182216, 'test_f1_std': 0.233898, 'test_f1_bottom_decile': 0.291188, 'test_f1_top_decile': 0.958949, 'test_f1_min': 0.196078, 'test_f1_max': 0.958949, 'test_f1_bottom10%': 0.196078, 'test_f1_top10%': 0.958949, 'test_f1_cos1': 0.90448, 'test_f1_entropy': 2.198728, 'test_loss_std': 9.178176, 'test_loss_bottom_decile': 4.050412, 'test_loss_top_decile': 32.983827, 'test_loss_min': 3.289158, 'test_loss_max': 32.983827, 'test_loss_bottom10%': 3.289158, 'test_loss_top10%': 32.983827, 'test_loss_cos1': 0.846388, 'test_loss_entropy': 2.101426}}[0m
[38;5;39m2023-02-24 22:08:37,757 (server:395) INFO: Server: Final evaluation is finished! Starting merging results.[0m
[38;5;39m2023-02-24 22:08:37,757 (server:521) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'test_loss': 3.289158, 'test_correct': 48.0, 'test_total': 5.0, 'test_acc': 0.96, 'test_avg_loss': 0.353701, 'test_f1': 0.958949}, 'client_summarized_weighted_avg': {'test_loss': 18.106686, 'test_correct': 16.5, 'test_total': 22.0, 'test_acc': 0.75, 'test_avg_loss': 0.663025, 'test_f1': 0.528913}, 'client_summarized_avg': {'test_loss': 14.586554, 'test_correct': 16.5, 'test_total': 22.0, 'test_acc': 0.700534, 'test_avg_loss': 0.733928, 'test_f1': 0.49601}, 'client_summarized_fairness': {'test_loss_entropy': 2.078116, 'test_loss_cos1': 0.834002, 'test_loss_top10%': 37.599891, 'test_loss_bottom10%': 4.048041, 'test_loss_max': 37.599891, 'test_loss_min': 4.048041, 'test_loss_top_decile': 37.599891, 'test_loss_bottom_decile': 4.708552, 'test_loss_std': 11.553613, 'test_correct': 14.7, 'test_total': 22.0, 'test_acc_std': 0.184911, 'test_acc_bottom_decile': 0.4, 'test_acc_top_decile': 0.888889, 'test_acc_min': 0.277778, 'test_acc_max': 0.888889, 'test_acc_bottom10%': 0.277778, 'test_acc_top10%': 0.888889, 'test_acc_cos1': 0.959979, 'test_acc_entropy': 2.256596, 'test_avg_loss_std': 0.442143, 'test_avg_loss_bottom_decile': 0.558686, 'test_avg_loss_top_decile': 2.088883, 'test_avg_loss_min': 0.469686, 'test_avg_loss_max': 2.088883, 'test_avg_loss_bottom10%': 0.469686, 'test_avg_loss_top10%': 2.088883, 'test_avg_loss_cos1': 0.884768, 'test_avg_loss_entropy': 2.193214, 'test_f1_std': 0.231587, 'test_f1_bottom_decile': 0.236559, 'test_f1_top_decile': 0.883117, 'test_f1_min': 0.196078, 'test_f1_max': 0.883117, 'test_f1_bottom10%': 0.196078, 'test_f1_top10%': 0.883117, 'test_f1_cos1': 0.884008, 'test_f1_entropy': 2.17626}}}[0m
[38;5;39m2023-02-24 22:08:37,758 (server:540) INFO: {'Role': 'Client #1', 'Round': 10, 'Results_raw': {'test_correct': 5.0, 'test_total': 8, 'test_acc': 0.625, 'test_avg_loss': 0.751424, 'test_f1': 0.384615, 'test_loss': 6.011389}}[0m
[38;5;39m2023-02-24 22:08:37,759 (server:540) INFO: {'Role': 'Client #2', 'Round': 10, 'Results_raw': {'test_correct': 8.0, 'test_total': 9, 'test_acc': 0.888889, 'test_avg_loss': 0.450046, 'test_f1': 0.883117, 'test_loss': 4.050412}}[0m
[38;5;39m2023-02-24 22:08:37,759 (server:540) INFO: {'Role': 'Client #3', 'Round': 10, 'Results_raw': {'test_correct': 5.0, 'test_total': 18, 'test_acc': 0.277778, 'test_avg_loss': 1.832435, 'test_f1': 0.196078, 'test_loss': 32.983827}}[0m
[38;5;39m2023-02-24 22:08:37,759 (server:540) INFO: {'Role': 'Client #4', 'Round': 10, 'Results_raw': {'test_correct': 48.0, 'test_total': 50, 'test_acc': 0.96, 'test_avg_loss': 0.353701, 'test_f1': 0.958949, 'test_loss': 17.685039}}[0m
[38;5;39m2023-02-24 22:08:37,759 (server:540) INFO: {'Role': 'Client #5', 'Round': 10, 'Results_raw': {'test_correct': 19.0, 'test_total': 21, 'test_acc': 0.904762, 'test_avg_loss': 0.455728, 'test_f1': 0.538012, 'test_loss': 9.570296}}[0m
[38;5;39m2023-02-24 22:08:37,760 (server:540) INFO: {'Role': 'Client #6', 'Round': 10, 'Results_raw': {'test_correct': 13.0, 'test_total': 17, 'test_acc': 0.764706, 'test_avg_loss': 0.707419, 'test_f1': 0.527778, 'test_loss': 12.026124}}[0m
[38;5;39m2023-02-24 22:08:37,760 (server:540) INFO: {'Role': 'Client #7', 'Round': 10, 'Results_raw': {'test_correct': 38.0, 'test_total': 49, 'test_acc': 0.77551, 'test_avg_loss': 0.515251, 'test_f1': 0.291188, 'test_loss': 25.247312}}[0m
[38;5;39m2023-02-24 22:08:37,760 (server:540) INFO: {'Role': 'Client #8', 'Round': 10, 'Results_raw': {'test_correct': 14.0, 'test_total': 23, 'test_acc': 0.608696, 'test_avg_loss': 0.897699, 'test_f1': 0.446055, 'test_loss': 20.647081}}[0m
[38;5;39m2023-02-24 22:08:37,760 (server:540) INFO: {'Role': 'Client #9', 'Round': 10, 'Results_raw': {'test_correct': 3.0, 'test_total': 5, 'test_acc': 0.6, 'test_avg_loss': 0.657832, 'test_f1': 0.375, 'test_loss': 3.289158}}[0m
[38;5;39m2023-02-24 22:08:37,760 (server:540) INFO: {'Role': 'Client #10', 'Round': 10, 'Results_raw': {'test_correct': 12.0, 'test_total': 20, 'test_acc': 0.6, 'test_avg_loss': 0.717745, 'test_f1': 0.359307, 'test_loss': 14.3549}}[0m
[38;5;39m2023-02-24 22:08:37,761 (monitor:172) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 2.235654, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 2488000, 'total_download_bytes': 1161640, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,762 (client:513) INFO: ================= client 1 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,764 (monitor:172) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 2.232751, 'total_model_size': 3260030, 'total_flops': 573346667520.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,765 (client:513) INFO: ================= client 2 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,767 (monitor:172) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 2.228921, 'total_model_size': 3260030, 'total_flops': 84940247040.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,767 (client:513) INFO: ================= client 3 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,770 (monitor:172) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 2.224798, 'total_model_size': 3260030, 'total_flops': 469825741440.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,770 (client:513) INFO: ================= client 4 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,773 (monitor:172) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 2.221782, 'total_model_size': 3260030, 'total_flops': 13271913600.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,773 (client:513) INFO: ================= client 5 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,775 (monitor:172) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 2.217868, 'total_model_size': 3260030, 'total_flops': 291982099200.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,776 (client:513) INFO: ================= client 6 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,778 (monitor:172) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 2.215104, 'total_model_size': 3260030, 'total_flops': 217659383040.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,779 (client:513) INFO: ================= client 7 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,781 (monitor:172) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 2.210958, 'total_model_size': 3260030, 'total_flops': 108829691520.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,781 (client:513) INFO: ================= client 8 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,784 (monitor:172) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 2.207836, 'total_model_size': 3260030, 'total_flops': 37161358080.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,784 (client:513) INFO: ================= client 9 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,787 (monitor:172) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 2.206277, 'total_model_size': 3260030, 'total_flops': 366304815360.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,787 (client:513) INFO: ================= client 10 received finish message =================[0m
[38;5;39m2023-02-24 22:08:37,789 (monitor:172) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 2.201848, 'total_model_size': 3260030, 'total_flops': 164571728640.0, 'total_upload_bytes': 116256, 'total_download_bytes': 0, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}[0m
[38;5;39m2023-02-24 22:08:37,790 (monitor:337) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one[0m
[38;5;39m2023-02-24 22:08:37,794 (monitor:245) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 2.218527, 'sys_avg/total_model_size': '2.83M', 'sys_avg/total_flops': '197.09G', 'sys_avg/total_upload_bytes': '324.09K', 'sys_avg/total_download_bytes': '103.13K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})[0m
[38;5;39m2023-02-24 22:08:37,794 (monitor:248) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.010726, 'sys_std/total_model_size': '915.23K', 'sys_std/total_flops': '171.11G', 'sys_std/total_upload_bytes': '665.85K', 'sys_std/total_download_bytes': '326.12K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})[0m
None
220
<generator object BaseDataTranslator.split_to_client.<locals>.<genexpr> at 0x7f85831c3350>
